{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/deepfake/blob/main/audio_deepfake_detection_enahced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **Audio Deepfake Detection, Fake Calls, Spoofing, Fraud Calls and Voice Cloning Analysis for Defensive Forensics**\n",
        "This script provides a comprehensive forensic analysis pipeline for audio files, focusing on detecting signs of deepfakes, spoofing, and manipulation. It integrates various analysis techniques including signal processing, feature extraction, traditional ML/DSP-based detection methods, SpeechBrain models (stubbed for demonstration), and state-of-the-art multimodal LLMs via vLLM and Groq.\n"
      ],
      "metadata": {
        "id": "CWnBHisbDcnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q numpy librosa soundfile matplotlib IPython webrtcvad pydub noisereduce pyAudioAnalysis speechbrain langchain openai langgraph transformers vllm requests ipywidgets audiomentations hmmlearn eyed3 langchain_community praat-parselmouth webrtcvad groq"
      ],
      "metadata": {
        "id": "ds9KD-Z5L4G8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "import time\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import numpy as np\n",
        "import torch\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import nest_asyncio\n",
        "import ipywidgets as widgets\n",
        "import webrtcvad\n",
        "import noisereduce as nr\n",
        "import parselmouth\n",
        "from pydub import AudioSegment\n",
        "from moviepy.editor import VideoFileClip\n",
        "from transformers import AutoTokenizer\n",
        "from vllm import LLM, EngineArgs, SamplingParams\n",
        "import IPython.display as ipd\n",
        "from IPython.display import display, clear_output, HTML, Image, Markdown\n",
        "\n",
        "# --- Optional Dependency Handling & Imports ---\n",
        "try:\n",
        "    import soundfile as sf\n",
        "    HAS_SOUNDFILE = True\n",
        "except ImportError:\n",
        "    print(\"[WARN] soundfile library not found (`pip install soundfile`). Some operations might be slower or fail.\")\n",
        "    HAS_SOUNDFILE = False\n",
        "\n",
        "try:\n",
        "    import pyloudnorm as pyln\n",
        "    HAS_PYLOUDNORM = True\n",
        "except ImportError:\n",
        "    print(\"[WARN] pyloudnorm library not found (`pip install pyloudnorm`). Loudness normalization disabled.\")\n",
        "    HAS_PYLOUDNORM = False\n",
        "\n",
        "try:\n",
        "    from scipy import signal\n",
        "    HAS_SCIPY = True\n",
        "except ImportError:\n",
        "    print(\"[WARN] scipy library not found (`pip install scipy`). De-humming feature disabled.\")\n",
        "    HAS_SCIPY = False\n",
        "\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    HAS_SEABORN = True\n",
        "except ImportError:\n",
        "    print(\"[WARN] seaborn library not found (`pip install seaborn`). Enhanced plots disabled.\")\n",
        "    HAS_SEABORN = False\n",
        "\n",
        "# --- SpeechBrain & LLM Integrations ---\n",
        "from speechbrain.inference.speaker import SpeakerRecognition\n",
        "try:\n",
        "    from speechbrain.augment import AddNoise\n",
        "except ImportError:\n",
        "    AddNoise = None\n",
        "try:\n",
        "    from speechbrain.pretrained import EncoderClassifier, LanguageIdentification\n",
        "except ImportError:\n",
        "    print(\"[WARN] SpeechBrain pretrained models not fully available. Some features might be limited.\")\n",
        "    EncoderClassifier, LanguageIdentification = None, None\n",
        "\n",
        "try:\n",
        "    from groq import Groq, AsyncGroq\n",
        "    HAS_GROQ = True\n",
        "except ImportError:\n",
        "    print(\"[WARN] Groq library not installed (`pip install groq`). Groq report generation disabled.\")\n",
        "    HAS_GROQ = False\n",
        "    AsyncGroq = None\n",
        "\n",
        "# --- UI/Display ---\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQQXlXZEIcGF",
        "outputId": "75e08e46-6611-4611-9af6-f9abf6e4c001"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] pyloudnorm library not found (`pip install pyloudnorm`). Loudness normalization disabled.\n",
            "[WARN] SpeechBrain pretrained models not fully available. Some features might be limited.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration & Constants ---\n",
        "GENERAL_PIPELINE_SETTINGS = {\n",
        "    \"TARGET_SR\": 16000,\n",
        "    \"VAD_AGGRESSIVENESS\": 2,\n",
        "    \"MAX_CONCURRENT_TASKS\": os.cpu_count() or 4,\n",
        "    \"PRINT_LEVEL\": \"INFO\",\n",
        "    \"LOUDNESS_TARGET_LUFS\": -23.0,\n",
        "    \"ENABLE_LOUDNESS_NORMALIZATION\": HAS_PYLOUDNORM and HAS_SOUNDFILE,\n",
        "    \"ENABLE_NOISE_REDUCTION\": True,\n",
        "    \"ENABLE_DEHUMMING\": HAS_SCIPY,\n",
        "    \"MAX_VLLM_TOKENS\": 350,\n",
        "    \"VLLM_TEMPERATURE\": 0.1,\n",
        "    \"GROQ_MODEL\": \"llama3-70b-8192\",\n",
        "    \"GROQ_TEMPERATURE\": 0.1,\n",
        "    \"VLLM_MODELS_TO_RUN\": [\"qwen2_audio\", \"ultravox\"],\n",
        "}\n",
        "\n",
        "MODEL_PATHS = {\n",
        "    \"SPKREC_MODEL_SOURCE\": \"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "    \"ANTISPOOF_MODEL_SOURCE\": \"speechbrain/anti-spoofing-ecapa-voxceleb\",\n",
        "    \"LANGID_MODEL_SOURCE\": \"speechbrain/lang-id-commonlanguage_ecapa\",\n",
        "    \"EMOTION_MODEL_SOURCE\": \"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\",\n",
        "}\n",
        "\n",
        "# --- Resource Management ---\n",
        "executor = ThreadPoolExecutor(max_workers=GENERAL_PIPELINE_SETTINGS[\"MAX_CONCURRENT_TASKS\"], thread_name_prefix='ForensicWorker')\n",
        "vllm_engines = {}\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def print_message(level, message):\n",
        "    levels = {\"DEBUG\": 0, \"INFO\": 1, \"WARN\": 2, \"ERROR\": 3}\n",
        "    if levels.get(level, 1) >= levels.get(GENERAL_PIPELINE_SETTINGS[\"PRINT_LEVEL\"], 1):\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        print(f\"{timestamp} [{level:<5}] {message}\")\n",
        "\n",
        "def get_file_extension(file_path):\n",
        "    return os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "def is_video_file(ext):\n",
        "    return ext in [\".mp4\", \".avi\", \".mov\", \".mkv\", \".webm\"]\n",
        "\n",
        "async def run_sync_in_executor(func, *args):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    return await loop.run_in_executor(executor, func, *args)\n",
        "\n",
        "def set_device_for_engine():\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- Data Model ---\n",
        "class ForensicReport:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.file_path = kwargs.get(\"file_path\")\n",
        "        self.verdict = kwargs.get(\"verdict\", \"Error: Report not generated\")\n",
        "        self.mean_risk_score = kwargs.get(\"mean_risk_score\", -1.0)\n",
        "        self.confidence = kwargs.get(\"confidence\", 0.0)\n",
        "        self.all_model_scores = kwargs.get(\"all_model_scores\", {})\n",
        "        self.all_anomalies = kwargs.get(\"all_anomalies\", [])\n",
        "        self.groq_summary = kwargs.get(\"groq_summary\", \"N/A\")\n",
        "        self.vllm_outputs = kwargs.get(\"vllm_outputs\", {})\n",
        "        self.features = kwargs.get(\"features\", {})\n",
        "        self.metrics = kwargs.get(\"metrics\", {})\n",
        "        self.speaker_info = kwargs.get(\"speaker_info\", {})\n",
        "        self.quality_info = kwargs.get(\"quality_info\", {})\n",
        "        self.loudness_info = kwargs.get(\"loudness_info\", {})\n",
        "        self.compression_info = kwargs.get(\"compression_info\", {})\n",
        "        self.reverb_info = kwargs.get(\"reverb_info\", {})\n",
        "        self.edit_detection_info = kwargs.get(\"edit_detection_info\", {})\n",
        "        self.plots = kwargs.get(\"plots\", {})\n",
        "        self.processing_times = kwargs.get(\"processing_times\", {})\n",
        "        self.timestamp = kwargs.get(\"timestamp\", datetime.utcnow().isoformat())\n",
        "\n",
        "    def json(self, indent=2):\n",
        "        serializable_data = self._make_serializable(self.__dict__)\n",
        "        return json.dumps(serializable_data, indent=indent)\n",
        "\n",
        "    def _make_serializable(self, data):\n",
        "        if isinstance(data, dict):\n",
        "            return {k: self._make_serializable(v) for k, v in data.items()}\n",
        "        elif isinstance(data, list):\n",
        "            return [self._make_serializable(item) for item in data]\n",
        "        elif isinstance(data, np.ndarray):\n",
        "            return data.tolist()\n",
        "        elif isinstance(data, (np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64)):\n",
        "            return int(data)\n",
        "        elif isinstance(data, (np.float_, np.float16, np.float32, np.float64)):\n",
        "            if np.isnan(data): return None\n",
        "            if np.isinf(data): return None\n",
        "            return float(data)\n",
        "        elif isinstance(data, (np.complex_, np.complex64, np.complex128)):\n",
        "            return {'real': data.real, 'imag': data.imag}\n",
        "        elif isinstance(data, (np.bool_)):\n",
        "            return bool(data)\n",
        "        elif isinstance(data, (np.void)):\n",
        "            return None\n",
        "        return data"
      ],
      "metadata": {
        "id": "A4UMam1_IcI0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Enhanced Preprocessing Steps ---\n",
        "async def normalize_loudness(audio_path: str, sr: int, target_lufs: float = GENERAL_PIPELINE_SETTINGS[\"LOUDNESS_TARGET_LUFS\"]) -> tuple[np.ndarray | None, dict]:\n",
        "    loudness_info = {\"status\": \"Skipped\", \"original_lufs\": None, \"target_lufs\": target_lufs}\n",
        "    if not GENERAL_PIPELINE_SETTINGS[\"ENABLE_LOUDNESS_NORMALIZATION\"]:\n",
        "        print_message(\"INFO\", \"Loudness normalization disabled.\")\n",
        "        try:\n",
        "            audio_data, _ = await run_sync_in_executor(sf.read, audio_path)\n",
        "            return audio_data, loudness_info\n",
        "        except Exception as e:\n",
        "            print_message(\"ERROR\", f\"Failed to read audio file {audio_path} even without normalization: {e}\")\n",
        "            return None, loudness_info\n",
        "\n",
        "    print_message(\"INFO\", f\"Normalizing loudness for {audio_path} to {target_lufs} LUFS...\")\n",
        "    audio_data = None\n",
        "    try:\n",
        "        audio_data, current_sr = await run_sync_in_executor(sf.read, audio_path)\n",
        "        if current_sr != sr:\n",
        "            print_message(\"WARN\", f\"Sample rate mismatch in normalize_loudness ({current_sr} != {sr}). This shouldn't happen if preprocess_audio worked correctly.\")\n",
        "            audio_data = await run_sync_in_executor(librosa.resample, audio_data.T, orig_sr=current_sr, target_sr=sr)\n",
        "            audio_data = audio_data.T\n",
        "\n",
        "        if np.max(np.abs(audio_data)) < 1e-6:\n",
        "            print_message(\"WARN\", \"Audio is silent, skipping loudness normalization.\")\n",
        "            loudness_info[\"status\"] = \"Skipped (Silent Audio)\"\n",
        "            return audio_data, loudness_info\n",
        "\n",
        "        meter = pyln.Meter(sr)\n",
        "        if audio_data.ndim > 1:\n",
        "            print_message(\"DEBUG\", \"Audio has multiple channels, converting to mono for LUFS calculation.\")\n",
        "            mono_audio = np.mean(audio_data, axis=1)\n",
        "        else:\n",
        "            mono_audio = audio_data\n",
        "\n",
        "        original_loudness = await run_sync_in_executor(meter.integrate_loudness, mono_audio)\n",
        "        loudness_info[\"original_lufs\"] = original_loudness\n",
        "\n",
        "        gain_db = target_lufs - original_loudness\n",
        "        gain_linear = 10.0**(gain_db / 20.0)\n",
        "        normalized_audio = audio_data * gain_linear\n",
        "\n",
        "        max_peak = np.max(np.abs(normalized_audio))\n",
        "        if max_peak > 0.99:\n",
        "            print_message(\"WARN\", f\"Potential clipping detected after LUFS normalization (Peak: {max_peak:.2f}). Scaling down.\")\n",
        "            normalized_audio = normalized_audio / (max_peak / 0.99)\n",
        "            loudness_info[\"status\"] = f\"Normalized (Peak Limited from {max_peak:.2f})\"\n",
        "        else:\n",
        "            loudness_info[\"status\"] = \"Normalized\"\n",
        "\n",
        "        await run_sync_in_executor(sf.write, audio_path, normalized_audio, sr)\n",
        "        print_message(\"INFO\", f\"Loudness normalized. Original: {original_loudness:.2f} LUFS -> Target: {target_lufs} LUFS.\")\n",
        "        return normalized_audio, loudness_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print_message(\"ERROR\", f\"Loudness normalization failed: {e}\")\n",
        "        loudness_info[\"status\"] = f\"Failed ({e})\"\n",
        "        if audio_data is None:\n",
        "            try:\n",
        "                audio_data, _ = await run_sync_in_executor(sf.read, audio_path)\n",
        "            except Exception as read_e:\n",
        "                print_message(\"ERROR\", f\"Failed to read audio file {audio_path} after normalization error: {read_e}\")\n",
        "                return None, loudness_info\n",
        "        return audio_data, loudness_info\n",
        "\n",
        "def apply_dehumming(audio_data: np.ndarray, sr: int, freqs_to_remove: list = [60, 120, 180, 50, 100, 150]) -> np.ndarray:\n",
        "    if not GENERAL_PIPELINE_SETTINGS[\"ENABLE_DEHUMMING\"]:\n",
        "        return audio_data\n",
        "    print_message(\"INFO\", \"Applying de-humming notch filters...\")\n",
        "    try:\n",
        "        processed_audio = audio_data.copy()\n",
        "        for freq in freqs_to_remove:\n",
        "            if freq < sr / 2:\n",
        "                Q = 30.0\n",
        "                b, a = signal.iirnotch(freq, Q, sr)\n",
        "                if processed_audio.ndim > 1:\n",
        "                    for i in range(processed_audio.shape[1]):\n",
        "                        processed_audio[:, i] = signal.filtfilt(b, a, processed_audio[:, i])\n",
        "                else:\n",
        "                    processed_audio = signal.filtfilt(b, a, processed_audio)\n",
        "        print_message(\"INFO\", f\"Applied notch filters for frequencies: {freqs_to_remove}\")\n",
        "        return processed_audio\n",
        "    except Exception as e:\n",
        "        print_message(\"ERROR\", f\"De-humming failed: {e}\")\n",
        "        return audio_data\n",
        "\n",
        "async def preprocess_audio(input_path: str, target_sr: int = GENERAL_PIPELINE_SETTINGS[\"TARGET_SR\"]) -> tuple[np.ndarray | None, int, str, dict]:\n",
        "    start_time = time.time()\n",
        "    print_message(\"INFO\", f\"Starting audio preprocessing for: {input_path}\")\n",
        "    base, _ = os.path.splitext(os.path.basename(input_path))\n",
        "    processed_wav_path = f\"processed_{base}_{int(time.time())}.wav\"\n",
        "    print_message(\"DEBUG\", f\"Processed audio will be saved to: {processed_wav_path}\")\n",
        "\n",
        "    audio_data = None\n",
        "    loudness_info = {\"status\": \"Not Attempted\"}\n",
        "\n",
        "    try:\n",
        "        ext = get_file_extension(input_path)\n",
        "\n",
        "        if is_video_file(ext):\n",
        "            print_message(\"INFO\", \"Video file detected. Extracting audio using MoviePy...\")\n",
        "            def extract_audio_sync():\n",
        "                try:\n",
        "                    clip = VideoFileClip(input_path)\n",
        "                    clip.audio.write_audiofile(processed_wav_path, fps=target_sr, codec='pcm_s16le', logger=None)\n",
        "                    clip.close()\n",
        "                    print_message(\"INFO\", f\"Audio extracted successfully to {processed_wav_path}\")\n",
        "                    return processed_wav_path\n",
        "                except Exception as e:\n",
        "                    print_message(\"ERROR\", f\"MoviePy audio extraction failed: {e}\")\n",
        "                    return None\n",
        "            processed_path = await run_sync_in_executor(extract_audio_sync)\n",
        "            if not processed_path: return None, target_sr, \"\", loudness_info\n",
        "            current_input = processed_path\n",
        "\n",
        "        elif ext != \".wav\":\n",
        "            print_message(\"INFO\", f\"Non-WAV audio file ({ext}) detected. Converting using pydub...\")\n",
        "            def convert_audio_sync():\n",
        "                try:\n",
        "                    audio = AudioSegment.from_file(input_path)\n",
        "                    audio = audio.set_channels(1).set_frame_rate(target_sr)\n",
        "                    audio.export(processed_wav_path, format=\"wav\")\n",
        "                    print_message(\"INFO\", f\"Audio converted successfully to {processed_wav_path}\")\n",
        "                    return processed_wav_path\n",
        "                except Exception as e:\n",
        "                    print_message(\"ERROR\", f\"Pydub audio conversion failed: {e}\")\n",
        "                    return None\n",
        "            processed_path = await run_sync_in_executor(convert_audio_sync)\n",
        "            if not processed_path: return None, target_sr, \"\", loudness_info\n",
        "            current_input = processed_path\n",
        "        else:\n",
        "            print_message(\"INFO\", \"Input is WAV. Ensuring target SR and mono...\")\n",
        "            def resave_wav():\n",
        "                try:\n",
        "                    audio, sr_orig = sf.read(input_path)\n",
        "                    if audio.ndim > 1:\n",
        "                        audio = np.mean(audio, axis=1)\n",
        "                    if sr_orig != target_sr:\n",
        "                        print_message(\"DEBUG\", f\"Resampling WAV from {sr_orig} Hz to {target_sr} Hz\")\n",
        "                        audio = librosa.resample(audio, orig_sr=sr_orig, target_sr=target_sr)\n",
        "                    sf.write(processed_wav_path, audio, target_sr)\n",
        "                    print_message(\"INFO\", f\"WAV standardized to {processed_wav_path} (SR={target_sr}, mono=True)\")\n",
        "                    return processed_wav_path\n",
        "                except Exception as e:\n",
        "                    print_message(\"ERROR\", f\"Failed to standardize WAV: {e}\")\n",
        "                    return None\n",
        "            processed_path = await run_sync_in_executor(resave_wav)\n",
        "            if not processed_path: return None, target_sr, \"\", loudness_info\n",
        "            current_input = processed_path\n",
        "\n",
        "        if GENERAL_PIPELINE_SETTINGS[\"ENABLE_LOUDNESS_NORMALIZATION\"]:\n",
        "            audio_data, loudness_info = await normalize_loudness(current_input, target_sr, GENERAL_PIPELINE_SETTINGS[\"LOUDNESS_TARGET_LUFS\"])\n",
        "            if audio_data is None:\n",
        "                print_message(\"WARN\", \"Proceeding without successfully normalized audio data due to error.\")\n",
        "                try:\n",
        "                    audio_data, _ = await run_sync_in_executor(sf.read, current_input)\n",
        "                except Exception as read_e:\n",
        "                    print_message(\"ERROR\", f\"Failed to load audio {current_input} after normalization error: {read_e}\")\n",
        "                    return None, target_sr, current_input, loudness_info\n",
        "        else:\n",
        "            try:\n",
        "                audio_data, _ = await run_sync_in_executor(sf.read, current_input)\n",
        "                loudness_info[\"status\"] = \"Skipped (Disabled)\"\n",
        "            except Exception as e:\n",
        "                print_message(\"ERROR\", f\"Failed to read audio file {current_input}: {e}\")\n",
        "                return None, target_sr, current_input, loudness_info\n",
        "\n",
        "        if audio_data is None:\n",
        "            print_message(\"ERROR\", \"Audio data is None after loading/normalization attempts.\")\n",
        "            return None, target_sr, current_input, loudness_info\n",
        "\n",
        "        if GENERAL_PIPELINE_SETTINGS[\"ENABLE_DEHUMMING\"]:\n",
        "            audio_data = await run_sync_in_executor(apply_dehumming, audio_data, target_sr)\n",
        "\n",
        "        peak_val = np.max(np.abs(audio_data))\n",
        "        if peak_val > 1e-6:\n",
        "            audio_data = audio_data / peak_val * 0.98\n",
        "        else:\n",
        "            print_message(\"WARN\", \"Audio signal is near silent after processing steps.\")\n",
        "\n",
        "        if GENERAL_PIPELINE_SETTINGS[\"ENABLE_NOISE_REDUCTION\"] and audio_data is not None and len(audio_data) > 0:\n",
        "            print_message(\"INFO\", \"Applying noise reduction...\")\n",
        "            def reduce_noise_sync():\n",
        "                try:\n",
        "                    reduced_audio = nr.reduce_noise(y=audio_data, sr=target_sr, prop_decrease=0.8, stationary=False)\n",
        "                    print_message(\"INFO\", \"Noise reduction applied.\")\n",
        "                    return reduced_audio\n",
        "                except Exception as e:\n",
        "                    print_message(\"WARN\", f\"Noise reduction failed: {e}\")\n",
        "                    return audio_data\n",
        "            audio_data = await run_sync_in_executor(reduce_noise_sync)\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "        print_message(\"INFO\", f\"Preprocessing complete. Time: {processing_time:.2f}s\")\n",
        "        return audio_data, target_sr, current_input, loudness_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print_message(\"ERROR\", f\"Critical error during preprocessing: {e}\")\n",
        "        processing_time = time.time() - start_time\n",
        "        print_message(\"ERROR\", f\"Preprocessing failed after {processing_time:.2f}s\")\n",
        "        return None, target_sr, processed_wav_path, {\"status\": f\"Failed ({e})\"}"
      ],
      "metadata": {
        "id": "Wh0Vt0gRIcMT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Feature Extraction (Includes VAD/Silence) ---\n",
        "async def extract_comprehensive_features(audio_data: np.ndarray, sr: int) -> dict:\n",
        "    if audio_data is None or len(audio_data) == 0:\n",
        "        print_message(\"WARN\", \"Cannot extract features from empty audio data.\")\n",
        "        return {}\n",
        "    start_time = time.time()\n",
        "    print_message(\"INFO\", \"Extracting comprehensive audio features...\")\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    def compute_vad_sync():\n",
        "        vad_ratio = 0.0\n",
        "        silence_ratio = 1.0\n",
        "        segments = []\n",
        "        try:\n",
        "            vad = webrtcvad.Vad(GENERAL_PIPELINE_SETTINGS[\"VAD_AGGRESSIVENESS\"])\n",
        "            frame_duration_ms = 20\n",
        "            frame_length = int(sr * frame_duration_ms / 1000)\n",
        "            num_frames = len(audio_data) // frame_length\n",
        "\n",
        "            if np.max(np.abs(audio_data)) > 0:\n",
        "                int16_audio = (audio_data * 32767).astype(np.int16)\n",
        "            else:\n",
        "                int16_audio = np.zeros_like(audio_data, dtype=np.int16)\n",
        "            audio_bytes = int16_audio.tobytes()\n",
        "            bytes_per_frame = frame_length * 2\n",
        "\n",
        "            speech_frames_count = 0\n",
        "            total_frames_processed = 0\n",
        "            is_speaking = False\n",
        "            segment_start_ms = 0\n",
        "\n",
        "            if bytes_per_frame == 0:\n",
        "                print_message(\"WARN\", \"Frame length is zero, cannot perform VAD.\")\n",
        "                return 0.0, 1.0, []\n",
        "\n",
        "            for i in range(num_frames):\n",
        "                start_byte = i * bytes_per_frame\n",
        "                end_byte = start_byte + bytes_per_frame\n",
        "                if end_byte > len(audio_bytes): break\n",
        "                frame_bytes = audio_bytes[start_byte:end_byte]\n",
        "\n",
        "                if len(frame_bytes) == bytes_per_frame:\n",
        "                    frame_is_speech = vad.is_speech(frame_bytes, sr)\n",
        "                    if frame_is_speech:\n",
        "                        speech_frames_count += 1\n",
        "                        if not is_speaking:\n",
        "                            segment_start_ms = i * frame_duration_ms\n",
        "                            is_speaking = True\n",
        "                    else:\n",
        "                        if is_speaking:\n",
        "                            segment_end_ms = i * frame_duration_ms\n",
        "                            segments.append([segment_start_ms, segment_end_ms])\n",
        "                            is_speaking = False\n",
        "                    total_frames_processed += 1\n",
        "                else:\n",
        "                    if is_speaking:\n",
        "                        segment_end_ms = i * frame_duration_ms\n",
        "                        segments.append([segment_start_ms, segment_end_ms])\n",
        "                        is_speaking = False\n",
        "\n",
        "            if is_speaking:\n",
        "                segments.append([segment_start_ms, num_frames * frame_duration_ms])\n",
        "\n",
        "            if total_frames_processed > 0:\n",
        "                vad_ratio = speech_frames_count / total_frames_processed\n",
        "                silence_ratio = 1.0 - vad_ratio\n",
        "            else:\n",
        "                print_message(\"WARN\", \"No frames processed for VAD analysis.\")\n",
        "                vad_ratio = 0.0\n",
        "                silence_ratio = 1.0\n",
        "\n",
        "            return vad_ratio, silence_ratio, segments\n",
        "\n",
        "        except Exception as e:\n",
        "            print_message(\"ERROR\", f\"WebRTC VAD failed: {e}\")\n",
        "            return 0.0, 1.0, []\n",
        "\n",
        "    vad_ratio, silence_ratio, speech_segments_ms = await run_sync_in_executor(compute_vad_sync)\n",
        "    features['vad_ratio'] = vad_ratio\n",
        "    features['silence_ratio'] = silence_ratio\n",
        "    features['speech_segments_ms'] = speech_segments_ms\n",
        "    features['speech_segments_s'] = [[s / 1000.0, e / 1000.0] for s, e in speech_segments_ms]\n",
        "\n",
        "    def compute_other_features_sync():\n",
        "        other_feats = {}\n",
        "        try:\n",
        "            other_feats['duration_s'] = len(audio_data) / sr\n",
        "            other_feats['energy_rms'] = np.sqrt(np.mean(audio_data ** 2))\n",
        "            other_feats['zero_crossing_rate_mean'] = np.mean(librosa.feature.zero_crossing_rate(y=audio_data))\n",
        "            other_feats['rmse_mean'] = np.mean(librosa.feature.rms(y=audio_data))\n",
        "\n",
        "            other_feats['spectral_centroid_mean'] = np.mean(librosa.feature.spectral_centroid(y=audio_data, sr=sr))\n",
        "            other_feats['spectral_bandwidth_mean'] = np.mean(librosa.feature.spectral_bandwidth(y=audio_data, sr=sr))\n",
        "            other_feats['spectral_rolloff_mean'] = np.mean(librosa.feature.spectral_rolloff(y=audio_data, sr=sr))\n",
        "            other_feats['spectral_flatness_mean'] = np.mean(librosa.feature.spectral_flatness(y=audio_data))\n",
        "            spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr, n_bands=6)\n",
        "            other_feats['spectral_contrast_mean'] = np.mean(spectral_contrast)\n",
        "            other_feats['spectral_contrast_std'] = np.std(spectral_contrast)\n",
        "\n",
        "            mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=24)\n",
        "            other_feats['mfcc_mean'] = np.mean(mfccs)\n",
        "            other_feats['mfcc_std'] = np.std(mfccs)\n",
        "            if mfccs.shape[1] > 3:\n",
        "                other_feats['mfcc_delta_mean'] = np.mean(librosa.feature.delta(mfccs))\n",
        "                other_feats['mfcc_delta2_mean'] = np.mean(librosa.feature.delta(mfccs, order=2))\n",
        "            else:\n",
        "                other_feats['mfcc_delta_mean'] = 0\n",
        "                other_feats['mfcc_delta2_mean'] = 0\n",
        "\n",
        "            chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr)\n",
        "            other_feats['chroma_mean'] = np.mean(chroma)\n",
        "            other_feats['chroma_std'] = np.std(chroma)\n",
        "\n",
        "            snd = parselmouth.Sound(audio_data, sr)\n",
        "            pitch = snd.to_pitch_ac(time_step=0.01, pitch_floor=75, pitch_ceiling=500)\n",
        "            pitch_values = pitch.selected_array['frequency']\n",
        "            pitch_values = pitch_values[pitch_values > 0]\n",
        "\n",
        "            if len(pitch_values) > 0:\n",
        "                other_feats['pitch_mean_hz'] = np.mean(pitch_values)\n",
        "                other_feats['pitch_std_hz'] = np.std(pitch_values)\n",
        "                other_feats['pitch_min_hz'] = np.min(pitch_values)\n",
        "                other_feats['pitch_max_hz'] = np.max(pitch_values)\n",
        "\n",
        "                point_process = parselmouth.praat.call(pitch, \"To PointProcess\")\n",
        "                jitter_local = parselmouth.praat.call(point_process, \"Get jitter (local)\", 0.0, 0.0, 0.0001, 0.02, 1.3)\n",
        "                intensity = snd.to_intensity(minimum_pitch=75)\n",
        "                shimmer_local = parselmouth.praat.call([snd, point_process], \"Get shimmer (local)\", 0.0, 0.0, 0.0001, 0.02, 1.3, 1.6)\n",
        "                other_feats['pitch_jitter_local_rel'] = jitter_local if not np.isnan(jitter_local) else 0\n",
        "                other_feats['intensity_shimmer_local_db'] = shimmer_local if not np.isnan(shimmer_local) else 0\n",
        "\n",
        "                harmonicity = parselmouth.praat.call(snd, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
        "                hnr = parselmouth.praat.call(harmonicity, \"Get mean\", 0, 0)\n",
        "                other_feats['hnr_mean_db'] = hnr if not np.isnan(hnr) else 0\n",
        "            else:\n",
        "                for key in ['pitch_mean_hz', 'pitch_std_hz', 'pitch_min_hz', 'pitch_max_hz',\n",
        "                            'pitch_jitter_local_rel', 'intensity_shimmer_local_db', 'hnr_mean_db']:\n",
        "                    other_feats[key] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            print_message(\"ERROR\", f\"Core feature extraction failed: {e}\")\n",
        "            return None\n",
        "\n",
        "        return other_feats\n",
        "\n",
        "    other_features = await run_sync_in_executor(compute_other_features_sync)\n",
        "    if other_features:\n",
        "        features.update(other_features)\n",
        "\n",
        "    processing_time = time.time() - start_time\n",
        "    if features:\n",
        "        print_message(\"INFO\", f\"Comprehensive feature extraction complete. Time: {processing_time:.2f}s\")\n",
        "        return features\n",
        "    else:\n",
        "        print_message(\"ERROR\", f\"Feature extraction process encountered an error after {processing_time:.2f}s\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "mmWS6aMZJ8rd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- New Vertical Analysis Agents ---\n",
        "async def analyze_compression_artifacts(audio_data: np.ndarray, sr: int) -> dict:\n",
        "    start_time = time.time()\n",
        "    print_message(\"INFO\", \"Analyzing for compression artifacts...\")\n",
        "    results = {\"score\": 0.0, \"anomaly\": False, \"reason\": \"No significant artifacts detected.\"}\n",
        "    if audio_data is None or len(audio_data) == 0:\n",
        "        results[\"reason\"] = \"Skipped (Empty Audio)\"\n",
        "        return results\n",
        "    try:\n",
        "        stft_result = np.abs(librosa.stft(audio_data))\n",
        "        freqs = librosa.fft_frequencies(sr=sr)\n",
        "        low_band_mask = freqs < 8000\n",
        "        high_band_mask = freqs > 16000\n",
        "\n",
        "        if np.any(low_band_mask) and np.any(high_band_mask):\n",
        "            low_energy = np.mean(stft_result[low_band_mask, :]**2)\n",
        "            high_energy = np.mean(stft_result[high_band_mask, :]**2)\n",
        "            if low_energy > 1e-8:\n",
        "                hf_ratio = high_energy / low_energy\n",
        "                results['hf_energy_ratio'] = hf_ratio\n",
        "                if hf_ratio < 0.005:\n",
        "                    results['score'] = 0.7\n",
        "                    results['anomaly'] = True\n",
        "                    results['reason'] = f\"Very low high-frequency energy ratio ({hf_ratio:.4f}), suggests potential compression cutoff.\"\n",
        "            else:\n",
        "                results['hf_energy_ratio'] = 0\n",
        "        else:\n",
        "            results['hf_energy_ratio'] = None\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "        print_message(\"INFO\", f\"Compression analysis complete. Score: {results['score']:.2f}. Time: {processing_time:.2f}s\")\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print_message(\"ERROR\", f\"Compression artifact analysis failed: {e}\")\n",
        "        results[\"reason\"] = f\"Error during analysis: {e}\"\n",
        "        results[\"score\"] = 0.1\n",
        "        return results\n",
        "\n",
        "async def estimate_reverb(audio_data: np.ndarray, sr: int) -> dict:\n",
        "    start_time = time.time()\n",
        "    print_message(\"INFO\", \"Estimating reverberation (basic)...\")\n",
        "    results = {\"score\": 0.0, \"description\": \"Reverb estimation inconclusive.\", \"rt60_approx_s\": None}\n",
        "    if audio_data is None or len(audio_data) < sr:\n",
        "        results[\"description\"] = \"Skipped (Audio too short or empty)\"\n",
        "        return results\n",
        "\n",
        "    try:\n",
        "        rms = librosa.feature.rms(y=audio_data)[0]\n",
        "        if len(rms) > 10:\n",
        "            from scipy.stats import kurtosis\n",
        "            rms_kurtosis = kurtosis(rms, fisher=False)\n",
        "            results['rms_envelope_kurtosis'] = rms_kurtosis\n",
        "            if rms_kurtosis < 2.5:\n",
        "                results['score'] = 0.6\n",
        "                results['description'] = f\"Low RMS kurtosis ({rms_kurtosis:.2f}) suggests possible significant reverberation.\"\n",
        "            elif rms_kurtosis > 5.0:\n",
        "                results['score'] = 0.1\n",
        "                results['description'] = f\"High RMS kurtosis ({rms_kurtosis:.2f}) suggests relatively dry signal.\"\n",
        "            else:\n",
        "                results['score'] = 0.3\n",
        "                results['description'] = f\"Moderate RMS kurtosis ({rms_kurtosis:.2f}). Reverb likely moderate.\"\n",
        "\n",
        "            results['rt60_approx_s'] = None  # Placeholder for estimated_rt60 if needed\n",
        "\n",
        "        else:\n",
        "            results['description'] = \"RMS envelope too short for kurtosis calculation.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print_message(\"ERROR\", f\"Reverb estimation failed: {e}\")\n",
        "        results[\"description\"] = f\"Error during analysis: {e}\"\n",
        "        results[\"score\"] = 0.1\n",
        "\n",
        "    processing_time = time.time() - start_time\n",
        "    print_message(\"INFO\", f\"Reverb estimation complete. Score: {results['score']:.2f}. Time: {processing_time:.2f}s\")\n",
        "    return results\n",
        "\n",
        "async def detect_potential_edits(audio_data: np.ndarray, sr: int) -> dict:\n",
        "    start_time = time.time()\n",
        "    print_message(\"INFO\", \"Detecting potential edit points (basic)...\")\n",
        "    results = {\"score\": 0.0, \"anomaly\": False, \"reason\": \"No significant inconsistencies detected.\", \"segment_feature_std\": {}}\n",
        "    if audio_data is None or len(audio_data) < sr:\n",
        "        results[\"reason\"] = \"Skipped (Audio too short or empty)\"\n",
        "        return results\n",
        "\n",
        "    try:\n",
        "        non_silent_segments = librosa.effects.split(audio_data, top_db=45, frame_length=2048, hop_length=512)\n",
        "\n",
        "        if len(non_silent_segments) <= 1:\n",
        "            results[\"reason\"] = \"Skipped (Audio contains only one non-silent segment)\"\n",
        "            return results\n",
        "\n",
        "        print_message(\"DEBUG\", f\"Found {len(non_silent_segments)} non-silent segments for edit detection.\")\n",
        "\n",
        "        segment_features = {'zcr': [], 'rms': [], 'centroid': [], 'flatness': []}\n",
        "        min_segment_len_samples = int(0.1 * sr)\n",
        "\n",
        "        for i, (start, end) in enumerate(non_silent_segments):\n",
        "            segment_audio = audio_data[start:end]\n",
        "            if len(segment_audio) < min_segment_len_samples:\n",
        "                continue\n",
        "\n",
        "            segment_features['zcr'].append(np.mean(librosa.feature.zero_crossing_rate(y=segment_audio)))\n",
        "            segment_features['rms'].append(np.mean(librosa.feature.rms(y=segment_audio)))\n",
        "            segment_features['centroid'].append(np.mean(librosa.feature.spectral_centroid(y=segment_audio, sr=sr)))\n",
        "            segment_features['flatness'].append(np.mean(librosa.feature.spectral_flatness(y=segment_audio)))\n",
        "\n",
        "        if len(segment_features['zcr']) <= 1:\n",
        "            results[\"reason\"] = \"Skipped (Not enough valid non-silent segments for comparison)\"\n",
        "            return results\n",
        "\n",
        "        max_relative_std = 0.0\n",
        "        feature_std_devs = {}\n",
        "\n",
        "        for key, values in segment_features.items():\n",
        "            if not values: continue\n",
        "            mean_val = np.mean(values)\n",
        "            std_val = np.std(values)\n",
        "            feature_std_devs[key] = std_val\n",
        "            if abs(mean_val) > 1e-6:\n",
        "                relative_std = std_val / abs(mean_val)\n",
        "                feature_std_devs[f\"{key}_relative\"] = relative_std\n",
        "                max_relative_std = max(max_relative_std, relative_std)\n",
        "            else:\n",
        "                feature_std_devs[f\"{key}_relative\"] = 0\n",
        "\n",
        "        results[\"segment_feature_std\"] = feature_std_devs\n",
        "\n",
        "        consistency_threshold = 0.5\n",
        "\n",
        "        if max_relative_std > consistency_threshold:\n",
        "            results['score'] = 0.8\n",
        "            results['anomaly'] = True\n",
        "            results['reason'] = f\"High inconsistency detected between audio segments (Max Rel Std Dev: {max_relative_std:.3f}). Potential edit point(s).\"\n",
        "        else:\n",
        "            results['score'] = 0.1\n",
        "            results['reason'] = f\"Segment features appear relatively consistent (Max Rel Std Dev: {max_relative_std:.3f}).\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print_message(\"ERROR\", f\"Edit detection failed: {e}\")\n",
        "        results[\"reason\"] = f\"Error during analysis: {e}\"\n",
        "        results[\"score\"] = 0.1\n",
        "\n",
        "    processing_time = time.time() - start_time\n",
        "    print_message(\"INFO\", f\"Edit detection complete. Score: {results['score']:.2f}. Time: {processing_time:.2f}s\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "2mvoUXx9KAg-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2zlFD0EPKBF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U0ehAAaFKBJP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}