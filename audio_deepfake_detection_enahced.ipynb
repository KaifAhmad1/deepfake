{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/deepfake/blob/main/audio_deepfake_detection_enahced.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  **Audio Deepfake Detection, Fake Calls, Spoofing, Fraud Calls and Voice Cloning Analysis for Defensive Forensics**\n",
        "This script provides a comprehensive forensic analysis pipeline for audio files, focusing on detecting signs of deepfakes, spoofing, and manipulation. It integrates various analysis techniques including signal processing, feature extraction, traditional ML/DSP-based detection methods, SpeechBrain models (stubbed for demonstration), and state-of-the-art multimodal LLMs via vLLM and Groq.\n"
      ],
      "metadata": {
        "id": "CWnBHisbDcnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q numpy librosa soundfile matplotlib IPython webrtcvad pydub noisereduce pyAudioAnalysis speechbrain langchain openai langgraph transformers vllm requests ipywidgets audiomentations hmmlearn eyed3 langchain_community praat-parselmouth webrtcvad groq"
      ],
      "metadata": {
        "id": "ds9KD-Z5L4G8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "import time\n",
        "import sys\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import numpy as np\n",
        "import torch\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import nest_asyncio\n",
        "import ipywidgets as widgets\n",
        "import webrtcvad\n",
        "import noisereduce as nr\n",
        "import parselmouth\n",
        "from pydub import AudioSegment\n",
        "from moviepy.editor import VideoFileClip\n",
        "from transformers import AutoTokenizer\n",
        "from vllm import LLM, EngineArgs, SamplingParams\n",
        "import IPython.display as ipd\n",
        "from IPython.display import display, clear_output, HTML, Image, Markdown\n",
        "\n",
        "# --- Optional Dependency Handling & Imports ---\n",
        "try:\n",
        "    import soundfile as sf\n",
        "    HAS_SOUNDFILE = True\n",
        "except ImportError:\n",
        "    print(\"[WARN] soundfile library not found (`pip install soundfile`). Some operations might be slower or fail.\")\n",
        "    HAS_SOUNDFILE = False\n",
        "\n",
        "try:\n",
        "    import pyloudnorm as pyln\n",
        "    HAS_PYLOUDNORM = True\n",
        "except ImportError:\n",
        "    print(\"[WARN] pyloudnorm library not found (`pip install pyloudnorm`). Loudness normalization disabled.\")\n",
        "    HAS_PYLOUDNORM = False\n",
        "\n",
        "try:\n",
        "    from scipy import signal\n",
        "    HAS_SCIPY = True\n",
        "except ImportError:\n",
        "    print(\"[WARN] scipy library not found (`pip install scipy`). De-humming feature disabled.\")\n",
        "    HAS_SCIPY = False\n",
        "\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    HAS_SEABORN = True\n",
        "except ImportError:\n",
        "    print(\"[WARN] seaborn library not found (`pip install seaborn`). Enhanced plots disabled.\")\n",
        "    HAS_SEABORN = False\n",
        "\n",
        "# --- SpeechBrain & LLM Integrations ---\n",
        "from speechbrain.inference.speaker import SpeakerRecognition\n",
        "try:\n",
        "    from speechbrain.augment import AddNoise\n",
        "except ImportError:\n",
        "    AddNoise = None\n",
        "try:\n",
        "    from speechbrain.pretrained import EncoderClassifier, LanguageIdentification\n",
        "except ImportError:\n",
        "    print(\"[WARN] SpeechBrain pretrained models not fully available. Some features might be limited.\")\n",
        "    EncoderClassifier, LanguageIdentification = None, None\n",
        "\n",
        "try:\n",
        "    from groq import Groq, AsyncGroq\n",
        "    HAS_GROQ = True\n",
        "except ImportError:\n",
        "    print(\"[WARN] Groq library not installed (`pip install groq`). Groq report generation disabled.\")\n",
        "    HAS_GROQ = False\n",
        "    AsyncGroq = None\n",
        "\n",
        "# --- UI/Display ---\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "0bwNxnAedYU6",
        "outputId": "cfb472fd-8efc-4770-b05c-f0eace3e6c77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 04-22 07:47:08 [__init__.py:239] Automatically detected platform cuda.\n",
            "[WARN] pyloudnorm library not found (`pip install pyloudnorm`). Loudness normalization disabled.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n",
            "DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n",
            "WARNING:py.warnings:<ipython-input-2-97f14b617ec0>:61: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
            "  from speechbrain.pretrained import EncoderClassifier, LanguageIdentification\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] SpeechBrain pretrained models not fully available. Some features might be limited.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration & Constants ---\n",
        "GENERAL_PIPELINE_SETTINGS = {\n",
        "    \"TARGET_SR\": 16000,\n",
        "    \"VAD_AGGRESSIVENESS\": 2,\n",
        "    \"MAX_CONCURRENT_TASKS\": os.cpu_count() or 4,\n",
        "    \"PRINT_LEVEL\": \"INFO\",\n",
        "    \"LOUDNESS_TARGET_LUFS\": -23.0,\n",
        "    \"ENABLE_LOUDNESS_NORMALIZATION\": HAS_PYLOUDNORM and HAS_SOUNDFILE,\n",
        "    \"ENABLE_NOISE_REDUCTION\": True,\n",
        "    \"ENABLE_DEHUMMING\": HAS_SCIPY,\n",
        "    \"MAX_VLLM_TOKENS\": 350,\n",
        "    \"VLLM_TEMPERATURE\": 0.1,\n",
        "    \"GROQ_MODEL\": \"llama3-70b-8192\",\n",
        "    \"GROQ_TEMPERATURE\": 0.1,\n",
        "    \"VLLM_MODELS_TO_RUN\": [\"qwen2_audio\", \"ultravox\"],\n",
        "}\n",
        "\n",
        "MODEL_PATHS = {\n",
        "    \"SPKREC_MODEL_SOURCE\": \"speechbrain/spkrec-ecapa-voxceleb\",\n",
        "    \"ANTISPOOF_MODEL_SOURCE\": \"speechbrain/anti-spoofing-ecapa-voxceleb\",\n",
        "    \"LANGID_MODEL_SOURCE\": \"speechbrain/lang-id-commonlanguage_ecapa\",\n",
        "    \"EMOTION_MODEL_SOURCE\": \"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\",\n",
        "}\n",
        "\n",
        "# --- Resource Management ---\n",
        "executor = ThreadPoolExecutor(max_workers=GENERAL_PIPELINE_SETTINGS[\"MAX_CONCURRENT_TASKS\"], thread_name_prefix='ForensicWorker')\n",
        "vllm_engines = {}\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def print_message(level, message):\n",
        "    levels = {\"DEBUG\": 0, \"INFO\": 1, \"WARN\": 2, \"ERROR\": 3}\n",
        "    if levels.get(level, 1) >= levels.get(GENERAL_PIPELINE_SETTINGS[\"PRINT_LEVEL\"], 1):\n",
        "        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        print(f\"{timestamp} [{level:<5}] {message}\")\n",
        "\n",
        "def get_file_extension(file_path):\n",
        "    return os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "def is_video_file(ext):\n",
        "    return ext in [\".mp4\", \".avi\", \".mov\", \".mkv\", \".webm\"]\n",
        "\n",
        "async def run_sync_in_executor(func, *args):\n",
        "    loop = asyncio.get_running_loop()\n",
        "    return await loop.run_in_executor(executor, func, *args)\n",
        "\n",
        "def set_device_for_engine():\n",
        "    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# --- Data Model ---\n",
        "class ForensicReport:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.file_path = kwargs.get(\"file_path\")\n",
        "        self.verdict = kwargs.get(\"verdict\", \"Error: Report not generated\")\n",
        "        self.mean_risk_score = kwargs.get(\"mean_risk_score\", -1.0)\n",
        "        self.confidence = kwargs.get(\"confidence\", 0.0)\n",
        "        self.all_model_scores = kwargs.get(\"all_model_scores\", {})\n",
        "        self.all_anomalies = kwargs.get(\"all_anomalies\", [])\n",
        "        self.groq_summary = kwargs.get(\"groq_summary\", \"N/A\")\n",
        "        self.vllm_outputs = kwargs.get(\"vllm_outputs\", {})\n",
        "        self.features = kwargs.get(\"features\", {})\n",
        "        self.metrics = kwargs.get(\"metrics\", {})\n",
        "        self.speaker_info = kwargs.get(\"speaker_info\", {})\n",
        "        self.quality_info = kwargs.get(\"quality_info\", {})\n",
        "        self.loudness_info = kwargs.get(\"loudness_info\", {})\n",
        "        self.compression_info = kwargs.get(\"compression_info\", {})\n",
        "        self.reverb_info = kwargs.get(\"reverb_info\", {})\n",
        "        self.edit_detection_info = kwargs.get(\"edit_detection_info\", {})\n",
        "        self.plots = kwargs.get(\"plots\", {})\n",
        "        self.processing_times = kwargs.get(\"processing_times\", {})\n",
        "        self.timestamp = kwargs.get(\"timestamp\", datetime.utcnow().isoformat())\n",
        "\n",
        "    def json(self, indent=2):\n",
        "        serializable_data = self._make_serializable(self.__dict__)\n",
        "        return json.dumps(serializable_data, indent=indent)\n",
        "\n",
        "    def _make_serializable(self, data):\n",
        "        if isinstance(data, dict):\n",
        "            return {k: self._make_serializable(v) for k, v in data.items()}\n",
        "        elif isinstance(data, list):\n",
        "            return [self._make_serializable(item) for item in data]\n",
        "        elif isinstance(data, np.ndarray):\n",
        "            return data.tolist()\n",
        "        elif isinstance(data, (np.int_, np.intc, np.intp, np.int8, np.int16, np.int32, np.int64, np.uint8, np.uint16, np.uint32, np.uint64)):\n",
        "            return int(data)\n",
        "        elif isinstance(data, (np.float_, np.float16, np.float32, np.float64)):\n",
        "            if np.isnan(data): return None\n",
        "            if np.isinf(data): return None\n",
        "            return float(data)\n",
        "        elif isinstance(data, (np.complex_, np.complex64, np.complex128)):\n",
        "            return {'real': data.real, 'imag': data.imag}\n",
        "        elif isinstance(data, (np.bool_)):\n",
        "            return bool(data)\n",
        "        elif isinstance(data, (np.void)):\n",
        "            return None\n",
        "        return data"
      ],
      "metadata": {
        "id": "KjGGtqe4dced"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Enhanced Preprocessing Steps ---\n",
        "async def normalize_loudness(audio_path: str, sr: int, target_lufs: float = GENERAL_PIPELINE_SETTINGS[\"LOUDNESS_TARGET_LUFS\"]) -> tuple[np.ndarray | None, dict]:\n",
        "    loudness_info = {\"status\": \"Skipped\", \"original_lufs\": None, \"target_lufs\": target_lufs}\n",
        "    if not GENERAL_PIPELINE_SETTINGS[\"ENABLE_LOUDNESS_NORMALIZATION\"]:\n",
        "        print_message(\"INFO\", \"Loudness normalization disabled.\")\n",
        "        try:\n",
        "            audio_data, _ = await run_sync_in_executor(sf.read, audio_path)\n",
        "            return audio_data, loudness_info\n",
        "        except Exception as e:\n",
        "            print_message(\"ERROR\", f\"Failed to read audio file {audio_path} even without normalization: {e}\")\n",
        "            return None, loudness_info\n",
        "\n",
        "    print_message(\"INFO\", f\"Normalizing loudness for {audio_path} to {target_lufs} LUFS...\")\n",
        "    audio_data = None\n",
        "    try:\n",
        "        audio_data, current_sr = await run_sync_in_executor(sf.read, audio_path)\n",
        "        if current_sr != sr:\n",
        "            print_message(\"WARN\", f\"Sample rate mismatch in normalize_loudness ({current_sr} != {sr}). This shouldn't happen if preprocess_audio worked correctly.\")\n",
        "            audio_data = await run_sync_in_executor(librosa.resample, audio_data.T, orig_sr=current_sr, target_sr=sr)\n",
        "            audio_data = audio_data.T\n",
        "\n",
        "        if np.max(np.abs(audio_data)) < 1e-6:\n",
        "            print_message(\"WARN\", \"Audio is silent, skipping loudness normalization.\")\n",
        "            loudness_info[\"status\"] = \"Skipped (Silent Audio)\"\n",
        "            return audio_data, loudness_info\n",
        "\n",
        "        meter = pyln.Meter(sr)\n",
        "        if audio_data.ndim > 1:\n",
        "            print_message(\"DEBUG\", \"Audio has multiple channels, converting to mono for LUFS calculation.\")\n",
        "            mono_audio = np.mean(audio_data, axis=1)\n",
        "        else:\n",
        "            mono_audio = audio_data\n",
        "\n",
        "        original_loudness = await run_sync_in_executor(meter.integrate_loudness, mono_audio)\n",
        "        loudness_info[\"original_lufs\"] = original_loudness\n",
        "\n",
        "        gain_db = target_lufs - original_loudness\n",
        "        gain_linear = 10.0**(gain_db / 20.0)\n",
        "        normalized_audio = audio_data * gain_linear\n",
        "\n",
        "        max_peak = np.max(np.abs(normalized_audio))\n",
        "        if max_peak > 0.99:\n",
        "            print_message(\"WARN\", f\"Potential clipping detected after LUFS normalization (Peak: {max_peak:.2f}). Scaling down.\")\n",
        "            normalized_audio = normalized_audio / (max_peak / 0.99)\n",
        "            loudness_info[\"status\"] = f\"Normalized (Peak Limited from {max_peak:.2f})\"\n",
        "        else:\n",
        "            loudness_info[\"status\"] = \"Normalized\"\n",
        "\n",
        "        await run_sync_in_executor(sf.write, audio_path, normalized_audio, sr)\n",
        "        print_message(\"INFO\", f\"Loudness normalized. Original: {original_loudness:.2f} LUFS -> Target: {target_lufs} LUFS.\")\n",
        "        return normalized_audio, loudness_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print_message(\"ERROR\", f\"Loudness normalization failed: {e}\")\n",
        "        loudness_info[\"status\"] = f\"Failed ({e})\"\n",
        "        if audio_data is None:\n",
        "            try:\n",
        "                audio_data, _ = await run_sync_in_executor(sf.read, audio_path)\n",
        "            except Exception as read_e:\n",
        "                print_message(\"ERROR\", f\"Failed to read audio file {audio_path} after normalization error: {read_e}\")\n",
        "                return None, loudness_info\n",
        "        return audio_data, loudness_info\n",
        "\n",
        "def apply_dehumming(audio_data: np.ndarray, sr: int, freqs_to_remove: list = [60, 120, 180, 50, 100, 150]) -> np.ndarray:\n",
        "    if not GENERAL_PIPELINE_SETTINGS[\"ENABLE_DEHUMMING\"]:\n",
        "        return audio_data\n",
        "    print_message(\"INFO\", \"Applying de-humming notch filters...\")\n",
        "    try:\n",
        "        processed_audio = audio_data.copy()\n",
        "        for freq in freqs_to_remove:\n",
        "            if freq < sr / 2:\n",
        "                Q = 30.0\n",
        "                b, a = signal.iirnotch(freq, Q, sr)\n",
        "                if processed_audio.ndim > 1:\n",
        "                    for i in range(processed_audio.shape[1]):\n",
        "                        processed_audio[:, i] = signal.filtfilt(b, a, processed_audio[:, i])\n",
        "                else:\n",
        "                    processed_audio = signal.filtfilt(b, a, processed_audio)\n",
        "        print_message(\"INFO\", f\"Applied notch filters for frequencies: {freqs_to_remove}\")\n",
        "        return processed_audio\n",
        "    except Exception as e:\n",
        "        print_message(\"ERROR\", f\"De-humming failed: {e}\")\n",
        "        return audio_data\n",
        "\n",
        "async def preprocess_audio(input_path: str, target_sr: int = GENERAL_PIPELINE_SETTINGS[\"TARGET_SR\"]) -> tuple[np.ndarray | None, int, str, dict]:\n",
        "    start_time = time.time()\n",
        "    print_message(\"INFO\", f\"Starting audio preprocessing for: {input_path}\")\n",
        "    base, _ = os.path.splitext(os.path.basename(input_path))\n",
        "    processed_wav_path = f\"processed_{base}_{int(time.time())}.wav\"\n",
        "    print_message(\"DEBUG\", f\"Processed audio will be saved to: {processed_wav_path}\")\n",
        "\n",
        "    audio_data = None\n",
        "    loudness_info = {\"status\": \"Not Attempted\"}\n",
        "\n",
        "    try:\n",
        "        ext = get_file_extension(input_path)\n",
        "\n",
        "        if is_video_file(ext):\n",
        "            print_message(\"INFO\", \"Video file detected. Extracting audio using MoviePy...\")\n",
        "            def extract_audio_sync():\n",
        "                try:\n",
        "                    clip = VideoFileClip(input_path)\n",
        "                    clip.audio.write_audiofile(processed_wav_path, fps=target_sr, codec='pcm_s16le', logger=None)\n",
        "                    clip.close()\n",
        "                    print_message(\"INFO\", f\"Audio extracted successfully to {processed_wav_path}\")\n",
        "                    return processed_wav_path\n",
        "                except Exception as e:\n",
        "                    print_message(\"ERROR\", f\"MoviePy audio extraction failed: {e}\")\n",
        "                    return None\n",
        "            processed_path = await run_sync_in_executor(extract_audio_sync)\n",
        "            if not processed_path: return None, target_sr, \"\", loudness_info\n",
        "            current_input = processed_path\n",
        "\n",
        "        elif ext != \".wav\":\n",
        "            print_message(\"INFO\", f\"Non-WAV audio file ({ext}) detected. Converting using pydub...\")\n",
        "            def convert_audio_sync():\n",
        "                try:\n",
        "                    audio = AudioSegment.from_file(input_path)\n",
        "                    audio = audio.set_channels(1).set_frame_rate(target_sr)\n",
        "                    audio.export(processed_wav_path, format=\"wav\")\n",
        "                    print_message(\"INFO\", f\"Audio converted successfully to {processed_wav_path}\")\n",
        "                    return processed_wav_path\n",
        "                except Exception as e:\n",
        "                    print_message(\"ERROR\", f\"Pydub audio conversion failed: {e}\")\n",
        "                    return None\n",
        "            processed_path = await run_sync_in_executor(convert_audio_sync)\n",
        "            if not processed_path: return None, target_sr, \"\", loudness_info\n",
        "            current_input = processed_path\n",
        "        else:\n",
        "            print_message(\"INFO\", \"Input is WAV. Ensuring target SR and mono...\")\n",
        "            def resave_wav():\n",
        "                try:\n",
        "                    audio, sr_orig = sf.read(input_path)\n",
        "                    if audio.ndim > 1:\n",
        "                        audio = np.mean(audio, axis=1)\n",
        "                    if sr_orig != target_sr:\n",
        "                        print_message(\"DEBUG\", f\"Resampling WAV from {sr_orig} Hz to {target_sr} Hz\")\n",
        "                        audio = librosa.resample(audio, orig_sr=sr_orig, target_sr=target_sr)\n",
        "                    sf.write(processed_wav_path, audio, target_sr)\n",
        "                    print_message(\"INFO\", f\"WAV standardized to {processed_wav_path} (SR={target_sr}, mono=True)\")\n",
        "                    return processed_wav_path\n",
        "                except Exception as e:\n",
        "                    print_message(\"ERROR\", f\"Failed to standardize WAV: {e}\")\n",
        "                    return None\n",
        "            processed_path = await run_sync_in_executor(resave_wav)\n",
        "            if not processed_path: return None, target_sr, \"\", loudness_info\n",
        "            current_input = processed_path\n",
        "\n",
        "        if GENERAL_PIPELINE_SETTINGS[\"ENABLE_LOUDNESS_NORMALIZATION\"]:\n",
        "            audio_data, loudness_info = await normalize_loudness(current_input, target_sr, GENERAL_PIPELINE_SETTINGS[\"LOUDNESS_TARGET_LUFS\"])\n",
        "            if audio_data is None:\n",
        "                print_message(\"WARN\", \"Proceeding without successfully normalized audio data due to error.\")\n",
        "                try:\n",
        "                    audio_data, _ = await run_sync_in_executor(sf.read, current_input)\n",
        "                except Exception as read_e:\n",
        "                    print_message(\"ERROR\", f\"Failed to load audio {current_input} after normalization error: {read_e}\")\n",
        "                    return None, target_sr, current_input, loudness_info\n",
        "        else:\n",
        "            try:\n",
        "                audio_data, _ = await run_sync_in_executor(sf.read, current_input)\n",
        "                loudness_info[\"status\"] = \"Skipped (Disabled)\"\n",
        "            except Exception as e:\n",
        "                print_message(\"ERROR\", f\"Failed to read audio file {current_input}: {e}\")\n",
        "                return None, target_sr, current_input, loudness_info\n",
        "\n",
        "        if audio_data is None:\n",
        "            print_message(\"ERROR\", \"Audio data is None after loading/normalization attempts.\")\n",
        "            return None, target_sr, current_input, loudness_info\n",
        "\n",
        "        if GENERAL_PIPELINE_SETTINGS[\"ENABLE_DEHUMMING\"]:\n",
        "            audio_data = await run_sync_in_executor(apply_dehumming, audio_data, target_sr)\n",
        "\n",
        "        peak_val = np.max(np.abs(audio_data))\n",
        "        if peak_val > 1e-6:\n",
        "            audio_data = audio_data / peak_val * 0.98\n",
        "        else:\n",
        "            print_message(\"WARN\", \"Audio signal is near silent after processing steps.\")\n",
        "\n",
        "        if GENERAL_PIPELINE_SETTINGS[\"ENABLE_NOISE_REDUCTION\"] and audio_data is not None and len(audio_data) > 0:\n",
        "            print_message(\"INFO\", \"Applying noise reduction...\")\n",
        "            def reduce_noise_sync():\n",
        "                try:\n",
        "                    reduced_audio = nr.reduce_noise(y=audio_data, sr=target_sr, prop_decrease=0.8, stationary=False)\n",
        "                    print_message(\"INFO\", \"Noise reduction applied.\")\n",
        "                    return reduced_audio\n",
        "                except Exception as e:\n",
        "                    print_message(\"WARN\", f\"Noise reduction failed: {e}\")\n",
        "                    return audio_data\n",
        "            audio_data = await run_sync_in_executor(reduce_noise_sync)\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "        print_message(\"INFO\", f\"Preprocessing complete. Time: {processing_time:.2f}s\")\n",
        "        return audio_data, target_sr, current_input, loudness_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print_message(\"ERROR\", f\"Critical error during preprocessing: {e}\")\n",
        "        processing_time = time.time() - start_time\n",
        "        print_message(\"ERROR\", f\"Preprocessing failed after {processing_time:.2f}s\")\n",
        "        return None, target_sr, processed_wav_path, {\"status\": f\"Failed ({e})\"}"
      ],
      "metadata": {
        "id": "FioFWT-8dgsi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Feature Extraction (Includes VAD/Silence) ---\n",
        "async def extract_comprehensive_features(audio_data: np.ndarray, sr: int) -> dict:\n",
        "    if audio_data is None or len(audio_data) == 0:\n",
        "        print_message(\"WARN\", \"Cannot extract features from empty audio data.\")\n",
        "        return {}\n",
        "    start_time = time.time()\n",
        "    print_message(\"INFO\", \"Extracting comprehensive audio features...\")\n",
        "\n",
        "    features = {}\n",
        "\n",
        "    def compute_vad_sync():\n",
        "        vad_ratio = 0.0\n",
        "        silence_ratio = 1.0\n",
        "        segments = []\n",
        "        try:\n",
        "            vad = webrtcvad.Vad(GENERAL_PIPELINE_SETTINGS[\"VAD_AGGRESSIVENESS\"])\n",
        "            frame_duration_ms = 20\n",
        "            frame_length = int(sr * frame_duration_ms / 1000)\n",
        "            num_frames = len(audio_data) // frame_length\n",
        "\n",
        "            if np.max(np.abs(audio_data)) > 0:\n",
        "                int16_audio = (audio_data * 32767).astype(np.int16)\n",
        "            else:\n",
        "                int16_audio = np.zeros_like(audio_data, dtype=np.int16)\n",
        "            audio_bytes = int16_audio.tobytes()\n",
        "            bytes_per_frame = frame_length * 2\n",
        "\n",
        "            speech_frames_count = 0\n",
        "            total_frames_processed = 0\n",
        "            is_speaking = False\n",
        "            segment_start_ms = 0\n",
        "\n",
        "            if bytes_per_frame == 0:\n",
        "                print_message(\"WARN\", \"Frame length is zero, cannot perform VAD.\")\n",
        "                return 0.0, 1.0, []\n",
        "\n",
        "            for i in range(num_frames):\n",
        "                start_byte = i * bytes_per_frame\n",
        "                end_byte = start_byte + bytes_per_frame\n",
        "                if end_byte > len(audio_bytes): break\n",
        "                frame_bytes = audio_bytes[start_byte:end_byte]\n",
        "\n",
        "                if len(frame_bytes) == bytes_per_frame:\n",
        "                    frame_is_speech = vad.is_speech(frame_bytes, sr)\n",
        "                    if frame_is_speech:\n",
        "                        speech_frames_count += 1\n",
        "                        if not is_speaking:\n",
        "                            segment_start_ms = i * frame_duration_ms\n",
        "                            is_speaking = True\n",
        "                    else:\n",
        "                        if is_speaking:\n",
        "                            segment_end_ms = i * frame_duration_ms\n",
        "                            segments.append([segment_start_ms, segment_end_ms])\n",
        "                            is_speaking = False\n",
        "                    total_frames_processed += 1\n",
        "                else:\n",
        "                    if is_speaking:\n",
        "                        segment_end_ms = i * frame_duration_ms\n",
        "                        segments.append([segment_start_ms, segment_end_ms])\n",
        "                        is_speaking = False\n",
        "\n",
        "            if is_speaking:\n",
        "                segments.append([segment_start_ms, num_frames * frame_duration_ms])\n",
        "\n",
        "            if total_frames_processed > 0:\n",
        "                vad_ratio = speech_frames_count / total_frames_processed\n",
        "                silence_ratio = 1.0 - vad_ratio\n",
        "            else:\n",
        "                print_message(\"WARN\", \"No frames processed for VAD analysis.\")\n",
        "                vad_ratio = 0.0\n",
        "                silence_ratio = 1.0\n",
        "\n",
        "            return vad_ratio, silence_ratio, segments\n",
        "\n",
        "        except Exception as e:\n",
        "            print_message(\"ERROR\", f\"WebRTC VAD failed: {e}\")\n",
        "            return 0.0, 1.0, []\n",
        "\n",
        "    vad_ratio, silence_ratio, speech_segments_ms = await run_sync_in_executor(compute_vad_sync)\n",
        "    features['vad_ratio'] = vad_ratio\n",
        "    features['silence_ratio'] = silence_ratio\n",
        "    features['speech_segments_ms'] = speech_segments_ms\n",
        "    features['speech_segments_s'] = [[s / 1000.0, e / 1000.0] for s, e in speech_segments_ms]\n",
        "\n",
        "    def compute_other_features_sync():\n",
        "        other_feats = {}\n",
        "        try:\n",
        "            other_feats['duration_s'] = len(audio_data) / sr\n",
        "            other_feats['energy_rms'] = np.sqrt(np.mean(audio_data ** 2))\n",
        "            other_feats['zero_crossing_rate_mean'] = np.mean(librosa.feature.zero_crossing_rate(y=audio_data))\n",
        "            other_feats['rmse_mean'] = np.mean(librosa.feature.rms(y=audio_data))\n",
        "\n",
        "            other_feats['spectral_centroid_mean'] = np.mean(librosa.feature.spectral_centroid(y=audio_data, sr=sr))\n",
        "            other_feats['spectral_bandwidth_mean'] = np.mean(librosa.feature.spectral_bandwidth(y=audio_data, sr=sr))\n",
        "            other_feats['spectral_rolloff_mean'] = np.mean(librosa.feature.spectral_rolloff(y=audio_data, sr=sr))\n",
        "            other_feats['spectral_flatness_mean'] = np.mean(librosa.feature.spectral_flatness(y=audio_data))\n",
        "            spectral_contrast = librosa.feature.spectral_contrast(y=audio_data, sr=sr, n_bands=6)\n",
        "            other_feats['spectral_contrast_mean'] = np.mean(spectral_contrast)\n",
        "            other_feats['spectral_contrast_std'] = np.std(spectral_contrast)\n",
        "\n",
        "            mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=24)\n",
        "            other_feats['mfcc_mean'] = np.mean(mfccs)\n",
        "            other_feats['mfcc_std'] = np.std(mfccs)\n",
        "            if mfccs.shape[1] > 3:\n",
        "                other_feats['mfcc_delta_mean'] = np.mean(librosa.feature.delta(mfccs))\n",
        "                other_feats['mfcc_delta2_mean'] = np.mean(librosa.feature.delta(mfccs, order=2))\n",
        "            else:\n",
        "                other_feats['mfcc_delta_mean'] = 0\n",
        "                other_feats['mfcc_delta2_mean'] = 0\n",
        "\n",
        "            chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr)\n",
        "            other_feats['chroma_mean'] = np.mean(chroma)\n",
        "            other_feats['chroma_std'] = np.std(chroma)\n",
        "\n",
        "            snd = parselmouth.Sound(audio_data, sr)\n",
        "            pitch = snd.to_pitch_ac(time_step=0.01, pitch_floor=75, pitch_ceiling=500)\n",
        "            pitch_values = pitch.selected_array['frequency']\n",
        "            pitch_values = pitch_values[pitch_values > 0]\n",
        "\n",
        "            if len(pitch_values) > 0:\n",
        "                other_feats['pitch_mean_hz'] = np.mean(pitch_values)\n",
        "                other_feats['pitch_std_hz'] = np.std(pitch_values)\n",
        "                other_feats['pitch_min_hz'] = np.min(pitch_values)\n",
        "                other_feats['pitch_max_hz'] = np.max(pitch_values)\n",
        "\n",
        "                point_process = parselmouth.praat.call(pitch, \"To PointProcess\")\n",
        "                jitter_local = parselmouth.praat.call(point_process, \"Get jitter (local)\", 0.0, 0.0, 0.0001, 0.02, 1.3)\n",
        "                intensity = snd.to_intensity(minimum_pitch=75)\n",
        "                shimmer_local = parselmouth.praat.call([snd, point_process], \"Get shimmer (local)\", 0.0, 0.0, 0.0001, 0.02, 1.3, 1.6)\n",
        "                other_feats['pitch_jitter_local_rel'] = jitter_local if not np.isnan(jitter_local) else 0\n",
        "                other_feats['intensity_shimmer_local_db'] = shimmer_local if not np.isnan(shimmer_local) else 0\n",
        "\n",
        "                harmonicity = parselmouth.praat.call(snd, \"To Harmonicity (cc)\", 0.01, 75, 0.1, 1.0)\n",
        "                hnr = parselmouth.praat.call(harmonicity, \"Get mean\", 0, 0)\n",
        "                other_feats['hnr_mean_db'] = hnr if not np.isnan(hnr) else 0\n",
        "            else:\n",
        "                for key in ['pitch_mean_hz', 'pitch_std_hz', 'pitch_min_hz', 'pitch_max_hz',\n",
        "                            'pitch_jitter_local_rel', 'intensity_shimmer_local_db', 'hnr_mean_db']:\n",
        "                    other_feats[key] = 0\n",
        "\n",
        "        except Exception as e:\n",
        "            print_message(\"ERROR\", f\"Core feature extraction failed: {e}\")\n",
        "            return None\n",
        "\n",
        "        return other_feats\n",
        "\n",
        "    other_features = await run_sync_in_executor(compute_other_features_sync)\n",
        "    if other_features:\n",
        "        features.update(other_features)\n",
        "\n",
        "    processing_time = time.time() - start_time\n",
        "    if features:\n",
        "        print_message(\"INFO\", f\"Comprehensive feature extraction complete. Time: {processing_time:.2f}s\")\n",
        "        return features\n",
        "    else:\n",
        "        print_message(\"ERROR\", f\"Feature extraction process encountered an error after {processing_time:.2f}s\")\n",
        "        return {}"
      ],
      "metadata": {
        "id": "nD_grEBeej_V"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- New Vertical Analysis Agents ---\n",
        "async def analyze_compression_artifacts(audio_data: np.ndarray, sr: int) -> dict:\n",
        "    start_time = time.time()\n",
        "    print_message(\"INFO\", \"Analyzing for compression artifacts...\")\n",
        "    results = {\"score\": 0.0, \"anomaly\": False, \"reason\": \"No significant artifacts detected.\"}\n",
        "    if audio_data is None or len(audio_data) == 0:\n",
        "        results[\"reason\"] = \"Skipped (Empty Audio)\"\n",
        "        return results\n",
        "    try:\n",
        "        stft_result = np.abs(librosa.stft(audio_data))\n",
        "        freqs = librosa.fft_frequencies(sr=sr)\n",
        "        low_band_mask = freqs < 8000\n",
        "        high_band_mask = freqs > 16000\n",
        "\n",
        "        if np.any(low_band_mask) and np.any(high_band_mask):\n",
        "            low_energy = np.mean(stft_result[low_band_mask, :]**2)\n",
        "            high_energy = np.mean(stft_result[high_band_mask, :]**2)\n",
        "            if low_energy > 1e-8:\n",
        "                hf_ratio = high_energy / low_energy\n",
        "                results['hf_energy_ratio'] = hf_ratio\n",
        "                if hf_ratio < 0.005:\n",
        "                    results['score'] = 0.7\n",
        "                    results['anomaly'] = True\n",
        "                    results['reason'] = f\"Very low high-frequency energy ratio ({hf_ratio:.4f}), suggests potential compression cutoff.\"\n",
        "            else:\n",
        "                results['hf_energy_ratio'] = 0\n",
        "        else:\n",
        "            results['hf_energy_ratio'] = None\n",
        "\n",
        "        processing_time = time.time() - start_time\n",
        "        print_message(\"INFO\", f\"Compression analysis complete. Score: {results['score']:.2f}. Time: {processing_time:.2f}s\")\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print_message(\"ERROR\", f\"Compression artifact analysis failed: {e}\")\n",
        "        results[\"reason\"] = f\"Error during analysis: {e}\"\n",
        "        results[\"score\"] = 0.1\n",
        "        return results\n",
        "\n",
        "async def estimate_reverb(audio_data: np.ndarray, sr: int) -> dict:\n",
        "    start_time = time.time()\n",
        "    print_message(\"INFO\", \"Estimating reverberation (basic)...\")\n",
        "    results = {\"score\": 0.0, \"description\": \"Reverb estimation inconclusive.\", \"rt60_approx_s\": None}\n",
        "    if audio_data is None or len(audio_data) < sr:\n",
        "        results[\"description\"] = \"Skipped (Audio too short or empty)\"\n",
        "        return results\n",
        "\n",
        "    try:\n",
        "        rms = librosa.feature.rms(y=audio_data)[0]\n",
        "        if len(rms) > 10:\n",
        "            from scipy.stats import kurtosis\n",
        "            rms_kurtosis = kurtosis(rms, fisher=False)\n",
        "            results['rms_envelope_kurtosis'] = rms_kurtosis\n",
        "            if rms_kurtosis < 2.5:\n",
        "                results['score'] = 0.6\n",
        "                results['description'] = f\"Low RMS kurtosis ({rms_kurtosis:.2f}) suggests possible significant reverberation.\"\n",
        "            elif rms_kurtosis > 5.0:\n",
        "                results['score'] = 0.1\n",
        "                results['description'] = f\"High RMS kurtosis ({rms_kurtosis:.2f}) suggests relatively dry signal.\"\n",
        "            else:\n",
        "                results['score'] = 0.3\n",
        "                results['description'] = f\"Moderate RMS kurtosis ({rms_kurtosis:.2f}). Reverb likely moderate.\"\n",
        "\n",
        "            results['rt60_approx_s'] = None  # Placeholder for estimated_rt60 if needed\n",
        "\n",
        "        else:\n",
        "            results['description'] = \"RMS envelope too short for kurtosis calculation.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print_message(\"ERROR\", f\"Reverb estimation failed: {e}\")\n",
        "        results[\"description\"] = f\"Error during analysis: {e}\"\n",
        "        results[\"score\"] = 0.1\n",
        "\n",
        "    processing_time = time.time() - start_time\n",
        "    print_message(\"INFO\", f\"Reverb estimation complete. Score: {results['score']:.2f}. Time: {processing_time:.2f}s\")\n",
        "    return results\n",
        "\n",
        "async def detect_potential_edits(audio_data: np.ndarray, sr: int) -> dict:\n",
        "    start_time = time.time()\n",
        "    print_message(\"INFO\", \"Detecting potential edit points (basic)...\")\n",
        "    results = {\"score\": 0.0, \"anomaly\": False, \"reason\": \"No significant inconsistencies detected.\", \"segment_feature_std\": {}}\n",
        "    if audio_data is None or len(audio_data) < sr:\n",
        "        results[\"reason\"] = \"Skipped (Audio too short or empty)\"\n",
        "        return results\n",
        "\n",
        "    try:\n",
        "        non_silent_segments = librosa.effects.split(audio_data, top_db=45, frame_length=2048, hop_length=512)\n",
        "\n",
        "        if len(non_silent_segments) <= 1:\n",
        "            results[\"reason\"] = \"Skipped (Audio contains only one non-silent segment)\"\n",
        "            return results\n",
        "\n",
        "        print_message(\"DEBUG\", f\"Found {len(non_silent_segments)} non-silent segments for edit detection.\")\n",
        "\n",
        "        segment_features = {'zcr': [], 'rms': [], 'centroid': [], 'flatness': []}\n",
        "        min_segment_len_samples = int(0.1 * sr)\n",
        "\n",
        "        for i, (start, end) in enumerate(non_silent_segments):\n",
        "            segment_audio = audio_data[start:end]\n",
        "            if len(segment_audio) < min_segment_len_samples:\n",
        "                continue\n",
        "\n",
        "            segment_features['zcr'].append(np.mean(librosa.feature.zero_crossing_rate(y=segment_audio)))\n",
        "            segment_features['rms'].append(np.mean(librosa.feature.rms(y=segment_audio)))\n",
        "            segment_features['centroid'].append(np.mean(librosa.feature.spectral_centroid(y=segment_audio, sr=sr)))\n",
        "            segment_features['flatness'].append(np.mean(librosa.feature.spectral_flatness(y=segment_audio)))\n",
        "\n",
        "        if len(segment_features['zcr']) <= 1:\n",
        "            results[\"reason\"] = \"Skipped (Not enough valid non-silent segments for comparison)\"\n",
        "            return results\n",
        "\n",
        "        max_relative_std = 0.0\n",
        "        feature_std_devs = {}\n",
        "\n",
        "        for key, values in segment_features.items():\n",
        "            if not values: continue\n",
        "            mean_val = np.mean(values)\n",
        "            std_val = np.std(values)\n",
        "            feature_std_devs[key] = std_val\n",
        "            if abs(mean_val) > 1e-6:\n",
        "                relative_std = std_val / abs(mean_val)\n",
        "                feature_std_devs[f\"{key}_relative\"] = relative_std\n",
        "                max_relative_std = max(max_relative_std, relative_std)\n",
        "            else:\n",
        "                feature_std_devs[f\"{key}_relative\"] = 0\n",
        "\n",
        "        results[\"segment_feature_std\"] = feature_std_devs\n",
        "\n",
        "        consistency_threshold = 0.5\n",
        "\n",
        "        if max_relative_std > consistency_threshold:\n",
        "            results['score'] = 0.8\n",
        "            results['anomaly'] = True\n",
        "            results['reason'] = f\"High inconsistency detected between audio segments (Max Rel Std Dev: {max_relative_std:.3f}). Potential edit point(s).\"\n",
        "        else:\n",
        "            results['score'] = 0.1\n",
        "            results['reason'] = f\"Segment features appear relatively consistent (Max Rel Std Dev: {max_relative_std:.3f}).\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print_message(\"ERROR\", f\"Edit detection failed: {e}\")\n",
        "        results[\"reason\"] = f\"Error during analysis: {e}\"\n",
        "        results[\"score\"] = 0.1\n",
        "\n",
        "    processing_time = time.time() - start_time\n",
        "    print_message(\"INFO\", f\"Edit detection complete. Score: {results['score']:.2f}. Time: {processing_time:.2f}s\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "f9dIDDxweo0I"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Visualization (Enhanced) ---\n",
        "async def generate_plots(audio_data: np.ndarray, sr: int, features: dict) -> dict:\n",
        "    if audio_data is None or len(audio_data) == 0:\n",
        "        print_message(\"WARN\", \"Cannot generate plots from empty audio data.\")\n",
        "        return {}\n",
        "\n",
        "    start_time = time.time()\n",
        "    print_message(\"INFO\", \"Generating plots...\")\n",
        "    plot_paths = {}\n",
        "    plot_ts = int(time.time())\n",
        "\n",
        "    def save_spectrogram():\n",
        "        try:\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            S = librosa.feature.melspectrogram(y=audio_data, sr=sr, n_mels=128, fmax=8000)\n",
        "            S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "            librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel', fmax=8000)\n",
        "            plt.colorbar(format='%+2.0f dB')\n",
        "            plt.title('Mel-Spectrogram')\n",
        "            plt.tight_layout()\n",
        "            spec_path = f\"spectrogram_{plot_ts}.png\"\n",
        "            plt.savefig(spec_path)\n",
        "            plt.close()\n",
        "            print_message(\"DEBUG\", f\"Spectrogram saved to {spec_path}\")\n",
        "            return spec_path\n",
        "        except Exception as e:\n",
        "            print_message(\"ERROR\", f\"Failed to generate spectrogram: {e}\")\n",
        "            plt.close()\n",
        "            return None\n",
        "\n",
        "    def save_feature_chart():\n",
        "        key_numeric_features = {\n",
        "            k: features.get(k) for k in [\n",
        "                'energy_rms', 'zero_crossing_rate_mean', 'spectral_centroid_mean',\n",
        "                'spectral_bandwidth_mean', 'spectral_rolloff_mean', 'spectral_flatness_mean', 'pitch_mean_hz', 'pitch_std_hz',\n",
        "                'pitch_jitter_local_rel', 'intensity_shimmer_local_db', 'hnr_mean_db',\n",
        "                'vad_ratio', 'silence_ratio'\n",
        "            ] if isinstance(features.get(k), (int, float, np.number)) and np.isfinite(features.get(k))\n",
        "        }\n",
        "        if not key_numeric_features:\n",
        "            print_message(\"WARN\", \"No key numeric features available for bar chart.\")\n",
        "            return None\n",
        "        try:\n",
        "            labels = list(key_numeric_features.keys())\n",
        "            values = list(key_numeric_features.values())\n",
        "            plt.figure(figsize=(12, 6))\n",
        "            bars = plt.bar(labels, values, color='cornflowerblue')\n",
        "            plt.ylabel('Value')\n",
        "            plt.title('Key Audio Feature Metrics')\n",
        "            plt.xticks(rotation=45, ha=\"right\", fontsize=9)\n",
        "            plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
        "            plt.tight_layout()\n",
        "            chart_path = f\"feature_metrics_{plot_ts}.png\"\n",
        "            plt.savefig(chart_path)\n",
        "            plt.close()\n",
        "            print_message(\"DEBUG\", f\"Feature chart saved to {chart_path}\")\n",
        "            return chart_path\n",
        "        except Exception as e:\n",
        "            print_message(\"ERROR\", f\"Failed to generate feature chart: {e}\")\n",
        "            plt.close()\n",
        "            return None\n",
        "\n",
        "    def plot_feature_timeseries(feature_key='energy_rms', title='Energy (RMS) Contour'):\n",
        "        try:\n",
        "            rms_frames = librosa.feature.rms(y=audio_data)[0]\n",
        "            times = librosa.times_like(rms_frames, sr=sr)\n",
        "            if len(times) > 1:\n",
        "                plt.figure(figsize=(12, 3))\n",
        "                plt.plot(times, rms_frames, label=title, alpha=0.8)\n",
        "                plt.xlabel(\"Time (s)\")\n",
        "                plt.ylabel(\"Amplitude (RMS)\")\n",
        "                plt.title(title)\n",
        "                plt.grid(linestyle='--', alpha=0.6)\n",
        "                plt.legend()\n",
        "                plt.tight_layout()\n",
        "                ts_path = f\"energy_timeseries_{plot_ts}.png\"\n",
        "                plt.savefig(ts_path)\n",
        "                plt.close()\n",
        "                print_message(\"DEBUG\", f\"Energy timeseries plot saved to {ts_path}\")\n",
        "                return ts_path\n",
        "            else:\n",
        "                print_message(\"WARN\", \"Not enough data for energy timeseries plot.\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print_message(\"ERROR\", f\"Failed to generate energy timeseries plot: {e}\")\n",
        "            plt.close()\n",
        "            return None\n",
        "\n",
        "    plot_tasks = [\n",
        "        run_sync_in_executor(save_spectrogram),\n",
        "        run_sync_in_executor(save_feature_chart),\n",
        "        run_sync_in_executor(plot_feature_timeseries),\n",
        "    ]\n",
        "    plot_results = await asyncio.gather(*plot_tasks, return_exceptions=True)\n",
        "\n",
        "    plot_keys = [\"spectrogram\", \"feature_chart\", \"energy_timeseries\"]\n",
        "    for key, path in zip(plot_keys, plot_results):\n",
        "        if isinstance(path, str) and os.path.exists(path):\n",
        "            plot_paths[key] = path\n",
        "        elif isinstance(path, Exception):\n",
        "            print_message(\"ERROR\", f\"Plotting task '{key}' failed: {path}\")\n",
        "\n",
        "    processing_time = time.time() - start_time\n",
        "    print_message(\"INFO\", f\"Plot generation complete. Found {len(plot_paths)} plots. Time: {processing_time:.2f}s\")\n",
        "    return plot_paths"
      ],
      "metadata": {
        "id": "iKX0YASoetdA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Existing Forensic Agents (SpeechBrain, etc.) - Assume functions are defined as before ---\n",
        "async def run_speechbrain_speaker_diarization(audio_path: str) -> tuple[list, list, dict]:\n",
        "    print_message(\"INFO\", f\"Running stub diarization for {audio_path}...\")\n",
        "    await asyncio.sleep(0.1)\n",
        "    return [\"Speaker_0\", \"Speaker_1\"], [[0.5, 2.1], [2.5, 4.0]], {\"Speaker_0\": 0.6, \"Speaker_1\": 0.4}\n",
        "\n",
        "async def run_speechbrain_spoof_detection(audio_path: str) -> tuple[float, list]:\n",
        "    print_message(\"INFO\", f\"Running stub spoof detection for {audio_path}...\")\n",
        "    await asyncio.sleep(0.1)\n",
        "    score = np.random.rand() * 0.8\n",
        "    anomalies = [f\"Possible spoofing indicated ({score:.2f})\"] if score > 0.5 else []\n",
        "    return score, anomalies\n",
        "\n",
        "async def run_language_id(audio_path: str) -> tuple[str, float]:\n",
        "    print_message(\"INFO\", f\"Running stub language ID for {audio_path}...\")\n",
        "    await asyncio.sleep(0.1)\n",
        "    langs = [\"en-US\", \"es-ES\", \"fr-FR\"]\n",
        "    lang = np.random.choice(langs)\n",
        "    conf = np.random.rand() * 0.5 + 0.5\n",
        "    return lang, conf\n",
        "\n",
        "async def run_emotion_analysis(audio_data: np.ndarray, sr: int) -> tuple[float, list, str, str]:\n",
        "    print_message(\"INFO\", \"Running stub emotion analysis...\")\n",
        "    await asyncio.sleep(0.1)\n",
        "    if audio_data is None: return 0.1, [], \"error\", \"No audio data\"\n",
        "    emotions = [\"neutral\", \"happy\", \"sad\", \"angry\"]\n",
        "    emo = np.random.choice(emotions)\n",
        "    conf = np.random.rand() * 0.4 + 0.5\n",
        "    score = {\"neutral\": 0.1, \"happy\": 0.2, \"sad\": 0.7, \"angry\": 0.8}.get(emo, 0.1) * conf\n",
        "    anomalies = [f\"Negative emotion detected: {emo}\"] if emo in [\"sad\", \"angry\"] and conf > 0.6 else []\n",
        "    desc = f\"Predominant emotion: {emo} (Conf: {conf:.2f})\"\n",
        "    return score, anomalies, emo, desc\n",
        "\n",
        "async def estimate_noise_and_quality(audio_data: np.ndarray, sr: int) -> tuple[float, float]:\n",
        "    print_message(\"INFO\", \"Running stub noise/quality estimation...\")\n",
        "    await asyncio.sleep(0.1)\n",
        "    if audio_data is None: return 1.0, -1.0\n",
        "    noise_score = np.random.rand() * 0.5\n",
        "    snr_db = np.random.rand() * 20 + 5\n",
        "    return noise_score, snr_db"
      ],
      "metadata": {
        "id": "93qiZN3RexoW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- vLLM Integration (Enhanced Prompts) ---\n",
        "def get_vllm_qwen2_audio_config(audio_count: int) -> tuple[str, dict, list | None, str | None]:\n",
        "    return \"Qwen/Qwen2-Audio-7B-Instruct\", {\"model\": \"Qwen/Qwen2-Audio-7B-Instruct\", \"dtype\":\"float16\", \"max_model_len\":4096, \"limit_mm_per_prompt\":{\"audio\": audio_count}}, [\"<|im_end|>\"], None\n",
        "\n",
        "def get_vllm_ultravox_config(audio_count: int) -> tuple[str, dict, list | None, str | None]:\n",
        "    return \"fixie-ai/ultravox-v0_5-llama-3_2-1b\", {\"model\": \"fixie-ai/ultravox-v0_5-llama-3_2-1b\", \"dtype\":\"float16\", \"max_model_len\":4096, \"limit_mm_per_prompt\":{\"audio\": audio_count}, \"trust_remote_code\":True}, [\"<|end_of_text|>\"], None\n",
        "\n",
        "async def run_vllm_inference_single_model(\n",
        "    model_type: str,\n",
        "    audio_path: str,\n",
        "    question: str\n",
        ") -> tuple[str, str]:\n",
        "    start_time = time.time()\n",
        "    print_message(\"INFO\", f\"Starting vLLM inference for model: {model_type}...\")\n",
        "\n",
        "    config_func_map = {\n",
        "        \"qwen2_audio\": get_vllm_qwen2_audio_config,\n",
        "        \"ultravox\": get_vllm_ultravox_config,\n",
        "    }\n",
        "    if model_type not in config_func_map:\n",
        "        print_message(\"ERROR\", f\"Unknown vLLM model type: {model_type}\")\n",
        "        return model_type, \"[ERROR: Unknown vLLM model type]\"\n",
        "\n",
        "    config_func = config_func_map[model_type]\n",
        "    model_name, engine_args_dict, stop_token_ids_or_strs, chat_template = config_func(audio_count=1)\n",
        "\n",
        "    engine_args_dict['device'] = set_device_for_engine()\n",
        "\n",
        "    engine_key = tuple(sorted(engine_args_dict.items()))\n",
        "    llm_engine = vllm_engines.get(engine_key)\n",
        "    if llm_engine is None:\n",
        "        print_message(\"INFO\", f\"Initializing new vLLM engine for {model_name}...\")\n",
        "        try:\n",
        "            engine_args_obj = EngineArgs(**engine_args_dict)\n",
        "            llm_engine = LLM(engine_args=engine_args_obj)\n",
        "            vllm_engines[engine_key] = llm_engine\n",
        "            print_message(\"INFO\", f\"vLLM engine for {model_name} initialized.\")\n",
        "        except Exception as e:\n",
        "            print_message(\"ERROR\", f\"Failed to initialize vLLM engine for {model_name}: {e}\")\n",
        "            return model_type, f\"[ERROR: Engine initialization failed: {e}]\"\n",
        "    else:\n",
        "        print_message(\"DEBUG\", f\"Using cached vLLM engine for {model_name}.\")\n",
        "\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    except Exception as e:\n",
        "        print_message(\"ERROR\", f\"Failed to load tokenizer for {model_name}: {e}\")\n",
        "        return model_type, f\"[ERROR: Tokenizer load failed: {e}]\"\n",
        "\n",
        "    if model_type == \"qwen2_audio\":\n",
        "        audio_in_prompt = f\"Audio 1: <|audio_bos|><|AUDIO|><|audio_eos|>\\n\"\n",
        "        system_prompt = (\n",
        "            \"You are a meticulous forensic audio analyst AI. Analyze the provided audio file \"\n",
        "            \"based on the user's query. Focus on detecting signs of deepfakes, spoofing, or manipulation. \"\n",
        "            \"Provide objective findings backed by reasoning.\"\n",
        "        )\n",
        "        user_prompt_structured = (\n",
        "            f\"{audio_in_prompt}{question}\\n\\n\"\n",
        "            \"**Please structure your response with the following sections:**\\n\"\n",
        "            \"- **Voice Quality:** (Describe clarity, naturalness, intonation, pace, presence of robotic artifacts, etc.)\\n\"\n",
        "            \"- **Background Analysis:** (Describe noise type, level, consistency, any unusual sounds.)\\n\"\n",
        "            \"- **Potential Anomalies:** (List specific observations like spectral gaps, phase issues, unnatural pauses, pitch shifts, jitter/shimmer perception, edit points, etc.)\\n\"\n",
        "            \"- **Risk Assessment:** (Your overall assessment: Low/Medium/High risk of manipulation, with brief justification.)\"\n",
        "        )\n",
        "        prompt = (f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n\"\n",
        "                  f\"<|im_start|>user\\n{user_prompt_structured}<|im_end|>\\n\"\n",
        "                  f\"<|im_start|>assistant\\n\")\n",
        "\n",
        "    elif model_type == \"ultravox\":\n",
        "        system_prompt = \"You are an expert audio analyst specializing in detecting synthetic speech and audio manipulation.\"\n",
        "        user_prompt_structured = (\n",
        "            f\"<|audio|>\\n{question}\\n\\n\"\n",
        "            \"**Provide a detailed forensic analysis covering:**\\n\"\n",
        "            \"1.  **Speech Naturalness:** (Rate intonation, rhythm, pauses. Mention any perceived robotic or synthetic qualities.)\\n\"\n",
        "            \"2.  **Acoustic Environment:** (Assess background noise, reverb, consistency.)\\n\"\n",
        "            \"3.  **Manipulation Indicators:** (Explicitly look for and describe artifacts like phase inconsistencies, spectral discontinuities, clicks/pops at potential edit points, unusual harmonic structure.)\\n\"\n",
        "            \"4.  **Conclusion:** (Summarize findings and state your confidence level regarding authenticity.)\"\n",
        "        )\n",
        "        messages = [\n",
        "            {'role': 'system', 'content': system_prompt},\n",
        "            {'role': 'user', 'content': user_prompt_structured}\n",
        "        ]\n",
        "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    else:\n",
        "        prompt = f\"User: {question}\"\n",
        "\n",
        "    try:\n",
        "        response = await run_sync_in_executor(llm_engine.generate, [prompt])\n",
        "        output = response[0].outputs[0].text\n",
        "        print_message(\"INFO\", f\"vLLM inference complete for {model_type}. Time: {time.time() - start_time:.2f}s\")\n",
        "        return model_type, output\n",
        "    except Exception as e:\n",
        "        print_message(\"ERROR\", f\"vLLM inference failed for {model_type}: {e}\")\n",
        "        return model_type, f\"[ERROR: Inference failed: {e}]\"\n",
        "\n",
        "async def run_vllm_multimodal_inference(audio_path: str, question: str) -> dict:\n",
        "    results = {}\n",
        "    tasks = [run_vllm_inference_single_model(model, audio_path, question) for model in GENERAL_PIPELINE_SETTINGS[\"VLLM_MODELS_TO_RUN\"]]\n",
        "    vllm_outputs = await asyncio.gather(*tasks)\n",
        "    for model_type, output in vllm_outputs:\n",
        "        results[model_type] = output\n",
        "    return results"
      ],
      "metadata": {
        "id": "aowLcK5Je1OM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LLM Forensic Reporting (Groq - Enhanced Prompt) ---\n",
        "async def run_groq_report(report_data: dict) -> str:\n",
        "    if not HAS_GROQ:\n",
        "        return \"Groq reporting disabled (library not installed).\"\n",
        "    groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
        "    if not groq_api_key:\n",
        "        return \"Groq reporting disabled (GROQ_API_KEY environment variable not set).\"\n",
        "\n",
        "    start_time = time.time()\n",
        "    print_message(\"INFO\", \"Generating enhanced forensic report summary via Groq...\")\n",
        "\n",
        "    client = AsyncGroq(api_key=groq_api_key)\n",
        "\n",
        "    context = f\"\"\"\n",
        "    **Forensic Audio Analysis Report Context:**\n",
        "\n",
        "    *   **File:** {report_data.get('file_path', 'N/A')}\n",
        "    *   **Overall Verdict:** {report_data.get('verdict', 'N/A')}\n",
        "    *   **Mean Risk Score:** {report_data.get('mean_risk_score', -1.0):.3f} (0=low risk, 1=high risk)\n",
        "    *   **Confidence:** {report_data.get('confidence', -1.0):.3f} (0=low confidence, 1=high confidence in score)\n",
        "    *   **Key Anomalies Detected:** {'; '.join(report_data.get('all_anomalies', ['None'])) or 'None'}\n",
        "\n",
        "    **Component Scores & Findings:**\n",
        "    {json.dumps(report_data.get('all_model_scores', {}), indent=2)}\n",
        "\n",
        "    **Audio Quality & Environment:**\n",
        "    *   Noise Residual Score: {report_data.get('quality_info', {}).get('noise_residual_score', 'N/A'):.3f} (Lower=Better)\n",
        "    *   Estimated SNR (HPSS): {report_data.get('quality_info', {}).get('snr_hpss_db', 'N/A'):.2f} dB (Higher=Better)\n",
        "    *   VAD Ratio (Speech): {report_data.get('features', {}).get('vad_ratio', 'N/A'):.3f}\n",
        "    *   Loudness: {report_data.get('loudness_info', {}).get('status', 'N/A')} (Original: {report_data.get('loudness_info', {}).get('original_lufs'):.2f} LUFS if measured)\n",
        "    *   Compression Analysis: {report_data.get('compression_info', {}).get('reason', 'N/A')} (Score: {report_data.get('compression_info', {}).get('score', 'N/A'):.2f})\n",
        "    *   Reverb Estimation: {report_data.get('reverb_info', {}).get('description', 'N/A')} (Score: {report_data.get('reverb_info', {}).get('score', 'N/A'):.2f})\n",
        "    *   Edit Detection: {report_data.get('edit_detection_info', {}).get('reason', 'N/A')} (Score: {report_data.get('edit_detection_info', {}).get('score', 'N/A'):.2f})\n",
        "\n",
        "    **Speaker & Language Information:**\n",
        "    *   Detected Speakers: {', '.join(report_data.get('speaker_info', {}).get('identities', ['N/A'])) or 'N/A'}\n",
        "    *   Detected Language: {report_data.get('speaker_info', {}).get('language', 'N/A')} (Confidence: {report_data.get('speaker_info', {}).get('language_confidence', 'N/A'):.2f})\n",
        "\n",
        "    **Key Voice Characteristics (Features):**\n",
        "    *   Duration: {report_data.get('features', {}).get('duration_s', 'N/A'):.2f}s\n",
        "    *   Pitch Mean (F0): {report_data.get('features', {}).get('pitch_mean_hz', 'N/A'):.2f} Hz\n",
        "    *   Pitch StdDev: {report_data.get('features', {}).get('pitch_std_hz', 'N/A'):.2f} Hz\n",
        "    *   Pitch Min: {report_data.get('features', {}).get('pitch_min_hz', 'N/A'):.2f} Hz\n",
        "    *   Pitch Max: {report_data.get('features', {}).get('pitch_max_hz', 'N/A'):.2f} Hz\n",
        "    *   Jitter (Local Rel): {report_data.get('features', {}).get('pitch_jitter_local_rel', 'N/A'):.5f} (Typical < ~0.01)\n",
        "    *   Shimmer (Local dB): {report_data.get('features', {}).get('intensity_shimmer_local_db', 'N/A'):.3f} (Typical < ~0.5 dB)\n",
        "    *   Harmonicity (HNR): {report_data.get('features', {}).get('hnr_mean_db', 'N/A'):.2f} dB (Higher is more harmonic/tonal)\n",
        "\n",
        "    **Multimodal LLM Analysis Summaries:**\n",
        "    {json.dumps(report_data.get('vllm_outputs', {'Info': 'No vLLM analysis performed.'}), indent=2)}\n",
        "    \"\"\"\n",
        "\n",
        "    system_prompt = \"\"\"\n",
        "    You are a senior forensic audio analyst AI assistant. Your task is to synthesize the provided forensic findings into a comprehensive, clear, and objective report using Markdown.\n",
        "\n",
        "    **Instructions:**\n",
        "    1.  **Structure the Report:** Use the following Markdown sections: `## Executive Summary`, `## Detailed Findings`, `## Multimodal LLM Insights`, `## Conclusion and Recommendations`, `## Limitations`.\n",
        "    2.  **Executive Summary:** Start with the `Overall Verdict`. Briefly state the key evidence (top 2-3 findings) supporting the verdict, mentioning specific scores or anomalies (e.g., \"High spoof score (0.92)\", \"Inconsistent segment features\"). State the overall `Mean Risk Score` and `Confidence`.\n",
        "    3.  **Detailed Findings:**\n",
        "        *   **Create a Markdown Table:** Summarize key scores and assessments. Include columns for 'Analysis Type', 'Metric/Finding', 'Value/Score', 'Assessment/Anomaly'. Include findings for Quality, Spoofing, Compression, Reverb, Edits, Emotion, Speaker/Lang ID.\n",
        "        *   **Explain Key Metrics:** Briefly define what metrics like Jitter, Shimmer, and HNR mean in the context of voice naturalness. Mention typical ranges if provided in context (e.g., Jitter < 0.01).\n",
        "        *   **Elaborate on Anomalies:** Discuss the 'Key Anomalies Detected', linking them back to specific metrics or observations from the context. Explain *why* they are considered anomalous.\n",
        "    4.  **Multimodal LLM Insights:** Briefly summarize the key points from *each* vLLM model's output provided in the context. Note any confirmations or contradictions with other analyses. Do *not* just copy the raw output.\n",
        "    5.  **Conclusion and Recommendations:** Reiterate the main conclusion. Provide specific recommendations based on the findings (e.g., \"Manual inspection of segments X-Y advised due to high inconsistency score\", \"Compare with known voice sample if available\", \"No significant manipulation indicators found\").\n",
        "    6.  **Limitations:** Mention potential limitations (e.g., \"Analysis might be affected by poor initial audio quality (SNR: Z)\", \"Models may not detect novel spoofing techniques\", \"Edit detection is basic and may miss subtle edits\").\n",
        "    7.  **Formatting & Tone:** Use clear, objective language. Use Markdown for formatting (bolding, lists, tables). Be concise but thorough. Ensure the report flows logically.\n",
        "    \"\"\"\n",
        "\n",
        "    user_prompt = f\"Generate a detailed forensic audio report in Markdown based on the following context:\\n\\n{context}\\n\\n**Forensic Report:**\"\n",
        "\n",
        "    try:\n",
        "        chat_completion = await client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt},\n",
        "            ],\n",
        "            model=GENERAL_PIPELINE_SETTINGS[\"GROQ_MODEL\"],\n",
        "            temperature=GENERAL_PIPELINE_SETTINGS[\"GROQ_TEMPERATURE\"],\n",
        "            max_tokens=2048,\n",
        "        )\n",
        "        summary = chat_completion.choices[0].message.content\n",
        "        print_message(\"INFO\", \"Groq report generated successfully.\")\n",
        "    except Exception as e:\n",
        "        print_message(\"ERROR\", f\"Groq API call failed: {e}\")\n",
        "        summary = f\"### Groq Report Generation Failed\\n\\nAn error occurred while generating the report summary via Groq:\\n\\n```\\n{e}\\n```\"\n",
        "\n",
        "    processing_time = time.time() - start_time\n",
        "    print_message(\"INFO\", f\"Groq report generation complete. Time: {processing_time:.2f}s\")\n",
        "    return summary\n",
        "\n",
        "# --- Aggregation and Final Report Generation (Enhanced) ---\n",
        "async def aggregate_and_finalize_report(\n",
        "    processed_audio_path: str,\n",
        "    preprocess_time: float,\n",
        "    analysis_results: dict,\n",
        "    vllm_outputs: dict,\n",
        "    plots: dict,\n",
        "    loudness_info: dict\n",
        ") -> ForensicReport:\n",
        "    start_time = time.time()\n",
        "    print_message(\"INFO\", \"Aggregating results and finalizing report...\")\n",
        "\n",
        "    all_scores = {}\n",
        "    all_anomalies = []\n",
        "    detailed_results = analysis_results\n",
        "\n",
        "    features = analysis_results.get(\"features\", {})\n",
        "\n",
        "    spoof_score, spoof_anomalies = analysis_results.get(\"spoof_detection\", (0.0, []))\n",
        "    all_scores['spoof_detection'] = spoof_score\n",
        "    all_anomalies.extend(spoof_anomalies)\n",
        "\n",
        "    emo_score, emo_anomalies, emo_label, emo_desc = analysis_results.get(\"emotion\", (0.1, [], \"neutral\", \"\"))\n",
        "    all_scores['emotion_risk'] = emo_score\n",
        "    all_anomalies.extend(emo_anomalies)\n",
        "\n",
        "    comp_info = analysis_results.get(\"compression\", {\"score\": 0.0, \"anomaly\": False, \"reason\": \"N/A\"})\n",
        "    all_scores['compression_artifact'] = comp_info.get('score', 0.0)\n",
        "    if comp_info.get('anomaly'): all_anomalies.append(f\"Compression: {comp_info.get('reason', 'Anomaly detected')}\")\n",
        "\n",
        "    reverb_info = analysis_results.get(\"reverb\", {\"score\": 0.0, \"description\": \"N/A\"})\n",
        "    all_scores['reverb_estimation'] = reverb_info.get('score', 0.0)\n",
        "    if reverb_info.get('score', 0.0) > 0.5: all_anomalies.append(f\"Reverb: {reverb_info.get('description', 'Significant reverberation')}\")\n",
        "\n",
        "    edit_info = analysis_results.get(\"edit_detection\", {\"score\": 0.0, \"anomaly\": False, \"reason\": \"N/A\"})\n",
        "    all_scores['edit_inconsistency'] = edit_info.get('score', 0.0)\n",
        "    if edit_info.get('anomaly'): all_anomalies.append(f\"Edit Detection: {edit_info.get('reason', 'Anomaly detected')}\")\n",
        "\n",
        "    speaker_identities, diar_segments, gender_dist = analysis_results.get(\"diarization\", ([], [], {}))\n",
        "    lang_label, lang_conf = analysis_results.get(\"language_id\", (\"unknown\", 0.0))\n",
        "    speaker_info = {\n",
        "        \"identities\": speaker_identities,\n",
        "        \"gender_distribution\": gender_dist,\n",
        "        \"language\": lang_label,\n",
        "        \"language_confidence\": lang_conf,\n",
        "    }\n",
        "\n",
        "    noise_score, snr_db = analysis_results.get(\"quality\", (1.0, -1.0))\n",
        "    quality_info = {\n",
        "        \"noise_residual_score\": noise_score,\n",
        "        \"snr_hpss_db\": snr_db\n",
        "    }\n",
        "\n",
        "    final_scores = list(all_scores.values())\n",
        "    if not final_scores:\n",
        "        mean_score = 0.0\n",
        "        confidence = 0.0\n",
        "    else:\n",
        "        mean_score = float(np.mean(final_scores))\n",
        "        base_confidence = max(0.0, 1.0 - float(np.std(final_scores)))\n",
        "        quality_bonus = max(0, (1.0 - noise_score - 0.5)) * 0.2\n",
        "        confidence = min(1.0, base_confidence + quality_bonus)\n",
        "\n",
        "    if mean_score > 0.65 and confidence > 0.4:\n",
        "        verdict = \" High Risk: Likely Deepfake/Spoof Detected\"\n",
        "    elif mean_score > 0.45 or (mean_score > 0.35 and len(all_anomalies) > 1):\n",
        "        verdict = \" Medium Risk: Suspicious - Manual Review Recommended\"\n",
        "        if len(all_anomalies) > 0: confidence = min(1.0, confidence + 0.05)\n",
        "    elif mean_score < 0.25 and confidence > 0.5:\n",
        "        verdict = \" Low Risk: Likely Authentic\"\n",
        "    else:\n",
        "        verdict = \" Inconclusive - Review Details\"\n",
        "\n",
        "    metrics = {\n",
        "        \"duration_s\": features.get('duration_s'), \"vad_ratio\": features.get('vad_ratio'),\n",
        "        \"silence_ratio\": features.get('silence_ratio'), \"snr_hpss_db\": snr_db,\n",
        "        \"noise_residual_score\": noise_score, \"pitch_mean_hz\": features.get('pitch_mean_hz'),\n",
        "        \"pitch_std_hz\": features.get('pitch_std_hz'),\n",
        "        \"pitch_jitter_local_rel\": features.get('pitch_jitter_local_rel'),\n",
        "        \"intensity_shimmer_local_db\": features.get('intensity_shimmer_local_db'),\n",
        "        \"hnr_mean_db\": features.get('hnr_mean_db'),\n",
        "        \"rms_envelope_kurtosis\": reverb_info.get('rms_envelope_kurtosis'),\n",
        "        \"hf_energy_ratio\": comp_info.get('hf_energy_ratio'),\n",
        "        \"edit_max_relative_std\": edit_info.get(\"segment_feature_std\", {}).get(\"max_relative_std\", 0.0)\n",
        "    }\n",
        "    metrics = {k: v for k, v in metrics.items() if v is not None and np.isfinite(v) and isinstance(v, (int, float, np.number))}\n",
        "\n",
        "    report_dict = {\n",
        "        \"file_path\": processed_audio_path,\n",
        "        \"verdict\": verdict,\n",
        "        \"mean_risk_score\": mean_score,\n",
        "        \"confidence\": confidence,\n",
        "        \"all_model_scores\": all_scores,\n",
        "        \"all_anomalies\": list(set(all_anomalies)),\n",
        "        \"vllm_outputs\": vllm_outputs,\n",
        "        \"features\": features,\n",
        "        \"metrics\": metrics,\n",
        "        \"speaker_info\": speaker_info,\n",
        "        \"quality_info\": quality_info,\n",
        "        \"loudness_info\": loudness_info,\n",
        "        \"compression_info\": comp_info,\n",
        "        \"reverb_info\": reverb_info,\n",
        "        \"edit_detection_info\": edit_info,\n",
        "        \"plots\": plots,\n",
        "        \"detailed_results\": detailed_results,\n",
        "        \"processing_times\": analysis_results.get(\"processing_times\", {}),\n",
        "        \"timestamp\": datetime.utcnow().isoformat()\n",
        "    }\n",
        "    report_dict[\"processing_times\"][\"preprocessing\"] = preprocess_time\n",
        "\n",
        "    groq_summary = await run_groq_report(report_dict)\n",
        "    report_dict[\"groq_summary\"] = groq_summary\n",
        "\n",
        "    final_report = ForensicReport(**report_dict)\n",
        "\n",
        "    aggregation_time = time.time() - start_time\n",
        "    final_report.processing_times[\"aggregation_and_summary\"] = aggregation_time\n",
        "    print_message(\"INFO\", f\"Report aggregation and Groq summary complete. Time: {aggregation_time:.2f}s\")\n",
        "    return final_report"
      ],
      "metadata": {
        "id": "1eeH_Y56e6jp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9EbNDFVAfCtL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}