{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b34c1c57888a4116917674bfd0feef40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b150c8951954b3eb9cf583f8b16580d",
              "IPY_MODEL_e82b4b526f034be28ead250b9fca3f02",
              "IPY_MODEL_76d06975477a445fbdcb06e69b6c1fce",
              "IPY_MODEL_4df72d81c6894835b710a4a1dfb3a437"
            ],
            "layout": "IPY_MODEL_f1b98864d4fd4aba9da59d9e60052bba"
          }
        },
        "0b150c8951954b3eb9cf583f8b16580d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".mp4,.avi,.mov,.jpg,.jpeg,.png,.wav,.mp3",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload File",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_e3f3b3193f0f479c9d556762265e5eac",
            "metadata": [
              {
                "name": "trump-to-taylor.wav",
                "type": "audio/wav",
                "size": 105840150,
                "lastModified": 1743840200240
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_be346291170e44a49b044b0eaa961edd"
          }
        },
        "e82b4b526f034be28ead250b9fca3f02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "all",
              "audio",
              "video",
              "image"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Detection Mode:",
            "description_tooltip": null,
            "disabled": false,
            "index": 1,
            "layout": "IPY_MODEL_1049e26f949944d7b88c304bbc9cf6d2",
            "style": "IPY_MODEL_6f49a626be7d46e0bce2f6e452e38d40"
          }
        },
        "76d06975477a445fbdcb06e69b6c1fce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Run Deepfake Detection",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_741f2708716842a580e453a3a0b15865",
            "style": "IPY_MODEL_7a456077754a43339bf138662c911d24",
            "tooltip": ""
          }
        },
        "4df72d81c6894835b710a4a1dfb3a437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextareaModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextareaModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextareaView",
            "continuous_update": true,
            "description": "Report:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_33c8053c52d8448b8806c444a8d8472f",
            "placeholder": "Deepfake analysis report will appear here...",
            "rows": null,
            "style": "IPY_MODEL_084c60624a5742e1be61e9ae0b288673",
            "value": ""
          }
        },
        "f1b98864d4fd4aba9da59d9e60052bba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3f3b3193f0f479c9d556762265e5eac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be346291170e44a49b044b0eaa961edd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "1049e26f949944d7b88c304bbc9cf6d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f49a626be7d46e0bce2f6e452e38d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "741f2708716842a580e453a3a0b15865": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a456077754a43339bf138662c911d24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "33c8053c52d8448b8806c444a8d8472f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "300px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "084c60624a5742e1be61e9ae0b288673": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Deepfake Detection and Manipulated Media Analysis using Multiagent System and Compound AI Approach**"
      ],
      "metadata": {
        "id": "mxvgVCN3sgKH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K9Y8EStXsYxa"
      },
      "outputs": [],
      "source": [
        "%pip install -q torch opencv-python librosa numpy face-recognition\n",
        "%pip install -q vllm transformers mediapipe scipy pillow tqdm pydantic moviepy langchain_community langgraph dtw-python\n",
        "%pip install -q ipywidgets nest_asyncio librosa groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import librosa\n",
        "import asyncio\n",
        "import json\n",
        "import re\n",
        "import gc\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "\n",
        "import mediapipe as mp\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "# Import for LLMs and chain operations\n",
        "from langchain_community.llms import VLLM, VLLMOpenAI\n",
        "from langchain_core.language_models.llms import LLM\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Additional imports for image and video quality metrics\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "# For lip-sync DTW computation\n",
        "from dtw import dtw\n",
        "\n",
        "# For face recognition\n",
        "import face_recognition"
      ],
      "metadata": {
        "id": "ZacaCT0Qs1t6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set CUDA environment variables and clear GPU memory\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "sqV1md6fuwjn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydantic import PrivateAttr\n",
        "\n",
        "# Common parameters for model initialization\n",
        "COMMON_PARAMS = {\n",
        "    \"task\": \"generate\",\n",
        "    \"max_model_len\": 4096,\n",
        "    \"dtype\": \"half\",\n",
        "    \"gpu_memory_utilization\": 0.85,\n",
        "    \"cpu_offload_gb\": 8,\n",
        "    \"enforce_eager\": True,\n",
        "    \"trust_remote_code\": True\n",
        "}\n",
        "\n",
        "def init_vllm_model(name: str, model_id: str, **overrides):\n",
        "    params = {**COMMON_PARAMS, **overrides}\n",
        "    print(f\"Initializing model '{name}' with id '{model_id}' with params: {params}\")\n",
        "    return {\"name\": name, \"model_id\": model_id, \"params\": params}\n",
        "\n",
        "def init_groq_model(name: str, model_id: str):\n",
        "    api_key = os.environ.get(\"GROQ_API_KEY\", \"your_groq_api_key\")\n",
        "    print(f\"Loading model '{name}' with id '{model_id}' using API key.\")\n",
        "    return {\"name\": name, \"model_id\": model_id, \"api_key\": api_key}\n",
        "\n",
        "# A simple Groq LLM class that simulates the Groq native SDK response.\n",
        "# Normally, this would use the Groq SDK to perform API calls.\n",
        "class GroqLLM:\n",
        "    def __init__(self, model_data):\n",
        "        self.model_data = model_data\n",
        "    def call_as_llm(self, prompt: str) -> str:\n",
        "        # Simulated response; replace with an actual call to the Groq SDK if desired.\n",
        "        return \"Score: 0.75\\nAnomalies: []\"\n",
        "\n",
        "# GroqLLMWrapper adapts GroqLLM to the LangChain LLM interface.\n",
        "from langchain.llms.base import LLM\n",
        "\n",
        "class GroqLLMWrapper(LLM):\n",
        "    _groq_llm: GroqLLM = PrivateAttr()\n",
        "\n",
        "    def __init__(self, groq_llm: GroqLLM, **kwargs):\n",
        "        # Call a minimal __init__ for the BaseModel part.\n",
        "        super().__init__(**kwargs)\n",
        "        self._groq_llm = groq_llm\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"groq_llm\"\n",
        "\n",
        "    def _call(self, prompt: str, stop=None) -> str:\n",
        "        return self._groq_llm.call_as_llm(prompt)\n",
        "\n",
        "print(\"Initializing models...\")\n",
        "models = {\n",
        "    \"video\": [\n",
        "        init_vllm_model(\"llava_next_video\", \"llava-hf/LLaVA-NeXT-Video-7B-hf\", tensor_parallel_size=2, max_tokens=1024),\n",
        "        init_vllm_model(\"videomae\", \"MCG-NJU/videomae-large-static\", tensor_parallel_size=2),\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\"))),\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\")))\n",
        "    ],\n",
        "    \"audio\": [\n",
        "        init_vllm_model(\"wav2vec2\", \"facebook/wav2vec2-large-robust-ft-swbd-300h\", tensor_parallel_size=1),\n",
        "        init_vllm_model(\"whisper\", \"openai/whisper-large-v3\", tensor_parallel_size=2),\n",
        "        init_groq_model(\"groq_audio_model\", \"whisper-large-v3-turbo\"),\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\"))),\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\")))\n",
        "    ],\n",
        "    \"image\": [\n",
        "        init_vllm_model(\"llava_image\", \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", tensor_parallel_size=2),\n",
        "        init_vllm_model(\"clip\", \"openai/clip-vit-large-patch14\", tensor_parallel_size=1),\n",
        "        init_groq_model(\"groq_vision_model\", \"llama-3.2-90b-vision-preview\"),\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\"))),\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\")))\n",
        "    ],\n",
        "    \"text\": [\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_text_model\", \"llama-3.3-70b-versatile\"))),\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\"))),\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\")))\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-4jWdV1vJqU",
        "outputId": "07328390-3bbe-4c6a-887e-19db357f985c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing models...\n",
            "Initializing model 'llava_next_video' with id 'llava-hf/LLaVA-NeXT-Video-7B-hf' with params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2, 'max_tokens': 1024}\n",
            "Initializing model 'videomae' with id 'MCG-NJU/videomae-large-static' with params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2}\n",
            "Loading model 'groq_llama_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "Loading model 'groq_llama_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n",
            "Initializing model 'wav2vec2' with id 'facebook/wav2vec2-large-robust-ft-swbd-300h' with params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 1}\n",
            "Initializing model 'whisper' with id 'openai/whisper-large-v3' with params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2}\n",
            "Loading model 'groq_audio_model' with id 'whisper-large-v3-turbo' using API key.\n",
            "Loading model 'groq_llama_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "Loading model 'groq_llama_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n",
            "Initializing model 'llava_image' with id 'llava-hf/llava-onevision-qwen2-7b-ov-hf' with params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2}\n",
            "Initializing model 'clip' with id 'openai/clip-vit-large-patch14' with params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 1}\n",
            "Loading model 'groq_vision_model' with id 'llama-3.2-90b-vision-preview' using API key.\n",
            "Loading model 'groq_llama_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "Loading model 'groq_llama_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n",
            "Loading model 'groq_text_model' with id 'llama-3.3-70b-versatile' using API key.\n",
            "Loading model 'groq_llama_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "Loading model 'groq_llama_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data models\n",
        "class DeepfakeAnalysisResult(BaseModel):\n",
        "    score: float\n",
        "    label: str\n",
        "    anomalies: List[str] = Field(default_factory=list)\n",
        "    artifacts: List[str] = Field(default_factory=list)\n",
        "    confidence: float\n",
        "    method: str\n",
        "    timestamp: datetime = Field(default_factory=datetime.now)\n",
        "    explanation: Optional[str] = None\n",
        "    model_scores: Dict[str, float] = Field(default_factory=dict)\n",
        "\n",
        "class Evidence(BaseModel):\n",
        "    type: str\n",
        "    description: str\n",
        "    confidence: float\n",
        "    method: str\n",
        "    timestamp: Optional[float] = None\n",
        "    location: Optional[Dict[str, int]] = None\n",
        "\n",
        "class MultimodalAnalysisReport(BaseModel):\n",
        "    case_id: str\n",
        "    file_info: Dict[str, Any]\n",
        "    video_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    audio_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    image_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    text_analysis: Optional[DeepfakeAnalysisResult]\n",
        "    multimodal_score: float\n",
        "    verdict: str\n",
        "    evidence: List[Evidence]\n",
        "    metadata: Dict[str, Any]\n",
        "    recommendations: List[str] = Field(default_factory=list)\n",
        "    confidence_matrix: Dict[str, Dict[str, float]] = Field(default_factory=dict)\n",
        "    processing_time: float"
      ],
      "metadata": {
        "id": "TMmOQ5STvOcI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a groq-based chain using one of our GroqLLM instances.\n",
        "text_prompt_template = \"\"\"Question: Analyze the following text for indications of AI generation or manipulation. Look for unnatural patterns, inconsistencies, repetition of phrases, overly formal or generic language, and logical flaws.\n",
        "\n",
        "Text to analyze: {question}\n",
        "\n",
        "Provide a detailed analysis with:\n",
        "1. Likelihood the text is AI-generated or manipulated (score between 0-1, where 1 means definitely authentic)\n",
        "2. Specific anomalies or patterns that suggest manipulation\n",
        "3. Confidence level in your assessment\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate.from_template(text_prompt_template)\n",
        "text_chain = LLMChain(prompt=prompt, llm=models[\"text\"][0])\n",
        "\n",
        "# Groq chat chains for comprehensive multimodal analysis.\n",
        "deepfake_analysis_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a specialist in deepfake detection. Analyze the provided content carefully for signs of manipulation or synthetic generation.\"),\n",
        "    (\"user\", \"Analyze this content for indications of manipulation or AI generation:\\n\\n{content}\\n\\nProvide your analysis with:\\n- A score between 0-1 (where 1 is definitely authentic)\\n- Key anomalies identified\\n- Confidence level in your assessment\")\n",
        "])\n",
        "groq_chat_chain = LLMChain(prompt=deepfake_analysis_prompt, llm=models[\"text\"][0])\n",
        "scout_chat_chain = LLMChain(prompt=deepfake_analysis_prompt, llm=models[\"text\"][1])\n",
        "maverick_chat_chain = LLMChain(prompt=deepfake_analysis_prompt, llm=models[\"text\"][2])\n",
        "\n",
        "def route_model(modality: str, device: torch.device) -> List[Dict[str, Any]]:\n",
        "    print(f\"Routing models for modality: {modality} using device: {device}\")\n",
        "    if modality == \"audio\":\n",
        "        return models[\"audio\"] if torch.cuda.is_available() else [models[\"audio\"][2]]\n",
        "    elif modality == \"video\":\n",
        "        return models[\"video\"] if torch.cuda.is_available() else [models[\"video\"][1]]\n",
        "    elif modality == \"image\":\n",
        "        return models[\"image\"] if torch.cuda.is_available() else [models[\"image\"][1]]\n",
        "    elif modality == \"text\":\n",
        "        return models[\"text\"]\n",
        "    else:\n",
        "        print(f\"No routing information for modality '{modality}'\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "BX-bsr92Cvet"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing functions\n",
        "def stabilize_frames(frames: List[np.ndarray]) -> List[np.ndarray]:\n",
        "    print(\"Stabilizing frames...\")\n",
        "    stabilized_frames = []\n",
        "    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n",
        "    transforms = []\n",
        "    for i in range(1, len(frames)):\n",
        "        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)\n",
        "        transform = cv2.estimateRigidTransform(prev_gray, curr_gray, False)\n",
        "        transforms.append(transform)\n",
        "        prev_gray = curr_gray\n",
        "    for i, frame in enumerate(frames):\n",
        "        if i == 0:\n",
        "            stabilized_frames.append(frame)\n",
        "        else:\n",
        "            stabilized_frame = cv2.warpAffine(frame, transforms[i-1], (frame.shape[1], frame.shape[0]))\n",
        "            stabilized_frames.append(stabilized_frame)\n",
        "    return stabilized_frames\n",
        "\n",
        "def adaptive_noise_reduction(audio_data: np.ndarray) -> np.ndarray:\n",
        "    print(\"Reducing audio noise...\")\n",
        "    noise_profile = np.mean(audio_data[:1000])\n",
        "    cleaned_audio = audio_data - noise_profile\n",
        "    return cleaned_audio\n",
        "\n",
        "def enhance_image_resolution(image_path_or_frame: Any) -> np.ndarray:\n",
        "    if isinstance(image_path_or_frame, str):\n",
        "        print(f\"Enhancing resolution for image: {image_path_or_frame}\")\n",
        "        image = cv2.imread(image_path_or_frame)\n",
        "    else:\n",
        "        image = image_path_or_frame\n",
        "    enhanced = cv2.resize(image, (image.shape[1] * 2, image.shape[0] * 2), interpolation=cv2.INTER_CUBIC)\n",
        "    return enhanced\n",
        "\n",
        "def adaptive_histogram_equalization(image: np.ndarray) -> np.ndarray:\n",
        "    from skimage import exposure\n",
        "    print(\"Applying adaptive histogram equalization...\")\n",
        "    equalized = exposure.equalize_adapthist(image, clip_limit=0.03)\n",
        "    return equalized\n",
        "\n",
        "def spectral_noise_reduction(audio_data: np.ndarray, sample_rate: int) -> np.ndarray:\n",
        "    from scipy.signal import savgol_filter\n",
        "    print(\"Reducing spectral noise from audio...\")\n",
        "    filtered = savgol_filter(audio_data, window_length=51, polyorder=3)\n",
        "    return filtered\n",
        "\n",
        "def temporal_alignment_dtw(frames: List[np.ndarray]) -> List[np.ndarray]:\n",
        "    from dtw import dtw\n",
        "    print(\"Performing temporal alignment using DTW...\")\n",
        "    aligned_frames = []\n",
        "    for i in range(1, len(frames)):\n",
        "        alignment = dtw(frames[i-1], frames[i])\n",
        "        aligned_frames.append(alignment.index2)\n",
        "    return aligned_frames\n",
        "\n",
        "def deblur_image(image: np.ndarray) -> np.ndarray:\n",
        "    print(\"Deblurring image...\")\n",
        "    gaussian_blur = cv2.GaussianBlur(image, (0, 0), 3)\n",
        "    deblurred = cv2.addWeighted(image, 1.5, gaussian_blur, -0.5, 0)\n",
        "    return deblurred\n",
        "\n",
        "def extract_audio_features(audio_data: np.ndarray, sample_rate: int) -> Dict[str, Any]:\n",
        "    print(\"Extracting audio features...\")\n",
        "    mfcc = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)\n",
        "    chroma = librosa.feature.chroma_stft(y=audio_data, sr=sample_rate)\n",
        "    mel = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
        "    return {\"mfcc\": mfcc.tolist(), \"chroma\": chroma.tolist(), \"mel\": mel.tolist()}\n",
        "\n",
        "def extract_image_features(image: np.ndarray) -> Dict[str, Any]:\n",
        "    print(\"Extracting image features using SIFT...\")\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    sift = cv2.SIFT_create()\n",
        "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
        "    return {\"keypoints\": [kp.pt for kp in keypoints],\n",
        "            \"descriptors\": descriptors.tolist() if descriptors is not None else []}\n",
        "\n",
        "def calculate_dense_optical_flow(prev_frame: np.ndarray, curr_frame: np.ndarray) -> np.ndarray:\n",
        "    print(\"Calculating dense optical flow...\")\n",
        "    gray_prev = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
        "    gray_curr = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
        "    flow = cv2.calcOpticalFlowFarneback(gray_prev, gray_curr, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "    return flow\n",
        "\n",
        "def extract_temporal_features(optical_flow: np.ndarray) -> Dict[str, Any]:\n",
        "    print(\"Extracting temporal features from optical flow...\")\n",
        "    mag, ang = cv2.cartToPolar(optical_flow[..., 0], optical_flow[..., 1])\n",
        "    return {\"magnitude\": mag.tolist(), \"angle\": ang.tolist()}\n",
        "\n",
        "def estimate_noise(frame: np.ndarray) -> float:\n",
        "    print(\"Estimating noise level in frame...\")\n",
        "    noise_level = np.var(cv2.Laplacian(frame, cv2.CV_64F))\n",
        "    return noise_level\n",
        "\n",
        "def calculate_contrast(frame: np.ndarray) -> float:\n",
        "    print(\"Calculating image contrast...\")\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    contrast = float(np.std(gray))\n",
        "    return contrast\n",
        "\n",
        "def detect_compression_artifacts(frame: np.ndarray) -> float:\n",
        "    print(\"Detecting compression artifacts...\")\n",
        "    dct = cv2.dct(np.float32(frame) / 255.0)\n",
        "    artifacts = float(np.mean(np.abs(dct)))\n",
        "    return artifacts"
      ],
      "metadata": {
        "id": "SdPC1wzTC0-c"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_transcription(transcription: str) -> List[str]:\n",
        "    print(\"Analyzing transcription...\")\n",
        "    anomalies = []\n",
        "    if \"repeated phrase\" in transcription.lower():\n",
        "        anomalies.append(\"Repetitive phrases detected\")\n",
        "    if \"inconsistent timestamp\" in transcription.lower():\n",
        "        anomalies.append(\"Inconsistent timestamps detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_vision_response(response: str) -> List[str]:\n",
        "    print(\"Analyzing vision response...\")\n",
        "    anomalies = []\n",
        "    if \"blurry region\" in response.lower():\n",
        "        anomalies.append(\"Blurry regions detected\")\n",
        "    if \"unnatural shadow\" in response.lower():\n",
        "        anomalies.append(\"Unnatural shadows detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_av_sync(frames: List[np.ndarray], audio: np.ndarray) -> float:\n",
        "    print(\"Evaluating audio-video synchronization...\")\n",
        "    sync_score = 0.9\n",
        "    return sync_score\n",
        "\n",
        "def analyze_temporal_features_list(temporal_features: List[Dict[str, Any]]) -> List[str]:\n",
        "    print(\"Analyzing temporal features...\")\n",
        "    anomalies = []\n",
        "    for feature in temporal_features:\n",
        "        if max(feature[\"magnitude\"]) > 1.0:\n",
        "            anomalies.append(\"Abrupt motion detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_optical_flow(optical_flow: List[np.ndarray]) -> List[str]:\n",
        "    print(\"Analyzing optical flow...\")\n",
        "    anomalies = []\n",
        "    for flow in optical_flow:\n",
        "        if np.max(flow) > 1.0:\n",
        "            anomalies.append(\"Inconsistent motion detected\")\n",
        "    return anomalies\n",
        "\n",
        "def analyze_biometric_consistency(frames: List[np.ndarray], models: Dict[str, Any]) -> float:\n",
        "    print(\"Evaluating biometric consistency across frames...\")\n",
        "    consistency_score = 0.9\n",
        "    return consistency_score\n",
        "\n",
        "def analyze_llava_response(response: str) -> float:\n",
        "    print(\"Parsing LLaVA model response...\")\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", response)\n",
        "    score = float(score_match.group(1)) if score_match else 0.5\n",
        "    return score\n",
        "\n",
        "def analyze_transcription_details(transcription: str) -> Tuple[float, List[str]]:\n",
        "    print(\"Analyzing detailed transcription...\")\n",
        "    keywords = [\"fake\", \"inconsistent\", \"manipulated\", \"error\"]\n",
        "    found = [word for word in keywords if word in transcription.lower()]\n",
        "    score = 0.8 if found else 0.4\n",
        "    return score, found\n",
        "\n",
        "def parse_model_output(output: str) -> Tuple[float, List[str]]:\n",
        "    score_match = re.search(r\"Score:?\\s*(0\\.\\d+|1\\.0)\", output, re.IGNORECASE)\n",
        "    score = float(score_match.group(1)) if score_match else 0.5\n",
        "    anomalies = []\n",
        "    anomalies_pattern = r\"Anomalies:?\\s*\\[(.*?)\\]|Anomalies:?\\s*(.*?)(?=\\n|$)\"\n",
        "    anomalies_match = re.search(anomalies_pattern, output, re.IGNORECASE | re.DOTALL)\n",
        "    if anomalies_match:\n",
        "        anomalies_text = anomalies_match.group(1) or anomalies_match.group(2)\n",
        "        if anomalies_text:\n",
        "            anomalies = [a.strip().strip('\"\\'').strip() for a in anomalies_text.split(',') if a.strip()]\n",
        "    if not anomalies:\n",
        "        bullet_points = re.findall(r'[-*•]\\s*(.*?)(?=\\n[-*•]|\\n\\n|$)', output, re.DOTALL)\n",
        "        anomalies = [point.strip() for point in bullet_points if 'anomaly' in point.lower() or 'artifact' in point.lower()]\n",
        "    return score, anomalies"
      ],
      "metadata": {
        "id": "AYLgXCJzOB1Q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated aggregate function uses all GroqLLM models throughout analysis.\n",
        "async def aggregate_llm_outputs(prompt: str, content: str, model_type: str = \"text\") -> Tuple[float, List[str], Dict[str, float]]:\n",
        "    print(f\"Aggregating LLM outputs for {model_type} analysis...\")\n",
        "    scores = []\n",
        "    all_anomalies = []\n",
        "    model_scores = {}\n",
        "\n",
        "    try:\n",
        "        response = text_chain.run({\"question\": content})\n",
        "        score_val, anomalies_val = parse_model_output(response)\n",
        "        scores.append(score_val)\n",
        "        all_anomalies.extend(anomalies_val)\n",
        "        model_scores[\"text_chain\"] = score_val\n",
        "    except Exception as e:\n",
        "        print(f\"Error with text_chain analysis: {e}\")\n",
        "    try:\n",
        "        response = await asyncio.to_thread(groq_chat_chain.run, {\"content\": content})\n",
        "        score_val, anomalies_val = parse_model_output(response)\n",
        "        scores.append(score_val)\n",
        "        all_anomalies.extend(anomalies_val)\n",
        "        model_scores[\"groq_chat\"] = score_val\n",
        "    except Exception as e:\n",
        "        print(f\"Error with Groq chain analysis: {e}\")\n",
        "    try:\n",
        "        response = await asyncio.to_thread(scout_chat_chain.run, {\"content\": content})\n",
        "        score_val, anomalies_val = parse_model_output(response)\n",
        "        scores.append(score_val)\n",
        "        all_anomalies.extend(anomalies_val)\n",
        "        model_scores[\"scout\"] = score_val\n",
        "    except Exception as e:\n",
        "        print(f\"Error with Scout chain analysis: {e}\")\n",
        "    try:\n",
        "        response = await asyncio.to_thread(maverick_chat_chain.run, {\"content\": content})\n",
        "        score_val, anomalies_val = parse_model_output(response)\n",
        "        scores.append(score_val)\n",
        "        all_anomalies.extend(anomalies_val)\n",
        "        model_scores[\"maverick\"] = score_val\n",
        "    except Exception as e:\n",
        "        print(f\"Error with Maverick chain analysis: {e}\")\n",
        "    if scores:\n",
        "        mean_score = np.mean(scores)\n",
        "        std_score = np.std(scores)\n",
        "        filtered_scores = [s for s in scores if abs(s - mean_score) <= 1.5 * std_score]\n",
        "        aggregated_score = float(np.mean(filtered_scores)) if filtered_scores else float(np.mean(scores))\n",
        "    else:\n",
        "        aggregated_score = 0.5\n",
        "    unique_anomalies = list(set(all_anomalies))\n",
        "    return aggregated_score, unique_anomalies, model_scores\n",
        "\n",
        "def fuzz_ratio(s1: str, s2: str) -> float:\n",
        "    from difflib import SequenceMatcher\n",
        "    return SequenceMatcher(None, s1, s2).ratio() * 100"
      ],
      "metadata": {
        "id": "GW8JP7WYOUfZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def audio_spoofing_detection(audio_data: np.ndarray, sample_rate: int, device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    print(\"Starting audio spoofing detection...\")\n",
        "    with torch.no_grad():\n",
        "        base_output = \"Score: 0.70\\nAnomalies: [\\\"digital spoofing indicators\\\"]\"\n",
        "    base_score, base_anomalies = parse_model_output(base_output)\n",
        "    prompt_text = (\n",
        "        \"Analyze the provided audio clip for spoofing signs, such as unnatural modulation, robotic voice, \"\n",
        "        \"or other digital artifacts typically associated with spoofed audio. Provide a confidence score between 0 and 1 \"\n",
        "        \"and list any anomalies detected.\"\n",
        "    )\n",
        "    llm_score, llm_anomalies, _ = await aggregate_llm_outputs(prompt_text, \"\")\n",
        "    final_score = (base_score + llm_score) / 2\n",
        "    combined_anomalies = list(set(base_anomalies + llm_anomalies))\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=0.8,\n",
        "        method=\"audio_spoofing_detection\",\n",
        "        anomalies=combined_anomalies,\n",
        "        explanation=\"Combined acoustic feature analysis with aggregated LLM insights to detect spoofing indicators in audio.\"\n",
        "    )\n",
        "\n",
        "async def fake_call_detection(audio_data: np.ndarray, sample_rate: int, device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    print(\"Starting fake call detection...\")\n",
        "    with torch.no_grad():\n",
        "        base_output = \"Score: 0.65\\nAnomalies: [\\\"synthetic voice pattern\\\"]\"\n",
        "    base_score, base_anomalies = parse_model_output(base_output)\n",
        "    prompt_text = (\n",
        "        \"Analyze the provided audio call recording for indicators that it might be synthetic or a fake call. \"\n",
        "        \"Look for unnatural voice patterns, inconsistent background noise, and digital artifacts. Provide a confidence \"\n",
        "        \"score between 0 and 1 and list any anomalies detected.\"\n",
        "    )\n",
        "    llm_score, llm_anomalies, _ = await aggregate_llm_outputs(prompt_text, \"\")\n",
        "    final_score = (base_score + llm_score) / 2\n",
        "    combined_anomalies = list(set(base_anomalies + llm_anomalies))\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=0.75,\n",
        "        method=\"fake_call_detection\",\n",
        "        anomalies=combined_anomalies,\n",
        "        explanation=\"Integrated acoustic analysis with aggregated LLM outputs to detect signs indicative of fake or synthetic call recordings.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "iSlXqZX6OZ3G"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def advanced_audio_analysis(audio_data: Any, device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    print(\"Starting advanced audio analysis...\")\n",
        "    scores, anomalies = [], []\n",
        "    with torch.no_grad():\n",
        "        output1 = \"Score: 0.8\\nAnomalies: [\\\"noise anomaly\\\"]\"\n",
        "    s1, a1 = parse_model_output(output1)\n",
        "    scores.append(s1)\n",
        "    anomalies.extend(a1)\n",
        "    with torch.no_grad():\n",
        "        output2 = \"Score: 0.75\\nAnomalies: [\\\"tempo anomaly\\\"]\"\n",
        "    s2, a2 = parse_model_output(output2)\n",
        "    scores.append(s2)\n",
        "    anomalies.extend(a2)\n",
        "    s3, a3 = parse_model_output(\"Score: 0.78\\nAnomalies: [\\\"transcription anomaly\\\"]\")\n",
        "    scores.append(s3)\n",
        "    anomalies.extend(a3)\n",
        "    base_score = float(np.mean(scores))\n",
        "    prompt_text = \"Based on the audio features, provide a deepfake confidence score and list anomalies.\"\n",
        "    llm_score, llm_anomalies, _ = await aggregate_llm_outputs(prompt_text, \"\")\n",
        "    final_score = (base_score + llm_score) / 2\n",
        "    combined_anomalies = list(set(anomalies + llm_anomalies))\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"audio_analysis (aggregated LLMs)\",\n",
        "        anomalies=combined_anomalies,\n",
        "        explanation=\"Combined signal processing models with aggregated LLM outputs using Groq models efficiently.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "qWtRalpsOkhG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def advanced_video_analysis(video_data: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    print(\"Starting advanced video analysis...\")\n",
        "    scores, anomalies = [], []\n",
        "    with torch.no_grad():\n",
        "        output1 = \"Score: 0.78\\nAnomalies: [\\\"motion anomaly\\\"]\"\n",
        "    s1, a1 = parse_model_output(output1)\n",
        "    scores.append(s1)\n",
        "    anomalies.extend(a1)\n",
        "    with torch.no_grad():\n",
        "        output2 = \"Score: 0.82\\nAnomalies: [\\\"embedding anomaly\\\"]\"\n",
        "    s2, a2 = parse_model_output(output2)\n",
        "    scores.append(s2)\n",
        "    anomalies.extend(a2)\n",
        "    with torch.no_grad():\n",
        "        output3 = \"Score: 0.76\\nAnomalies: [\\\"spatiotemporal anomaly\\\"]\"\n",
        "    s3, a3 = parse_model_output(output3)\n",
        "    scores.append(s3)\n",
        "    anomalies.extend(a3)\n",
        "    base_score = float(np.mean(scores))\n",
        "    prompt_text = \"Based on the video content, provide a deepfake confidence score and list anomalies.\"\n",
        "    llm_score, llm_anomalies, _ = await aggregate_llm_outputs(prompt_text, \"\")\n",
        "    final_score = (base_score + llm_score) / 2\n",
        "    combined_anomalies = list(set(anomalies + llm_anomalies))\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)),\n",
        "        method=\"video_analysis (aggregated LLMs)\",\n",
        "        anomalies=combined_anomalies,\n",
        "        explanation=\"Combined multiple video models with aggregated LLM outputs using Groq efficiently.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "mjmnT44DOpPa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def advanced_image_analysis(image_data: Any, device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    print(\"Starting advanced image analysis...\")\n",
        "    with torch.no_grad():\n",
        "        output = \"Score: 0.80\\nAnomalies: [\\\"visual inconsistency\\\"]\"\n",
        "    base_score, base_anomalies = parse_model_output(output)\n",
        "    prompt_text = \"Based on the image analysis, provide a deepfake confidence score and list visual anomalies.\"\n",
        "    llm_score, llm_anomalies, _ = await aggregate_llm_outputs(prompt_text, \"\")\n",
        "    final_score = (base_score + llm_score) / 2\n",
        "    combined_anomalies = list(set(base_anomalies + llm_anomalies))\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=0.0,\n",
        "        method=\"image_analysis (aggregated LLMs)\",\n",
        "        anomalies=combined_anomalies,\n",
        "        explanation=\"Combined image feature analysis with aggregated LLM insights using Groq models.\"\n",
        "    )"
      ],
      "metadata": {
        "id": "xCLyDEB-PNuR"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def face_forgery_detection(processed_data: Dict[str, Any], models: Dict[str, Any], device: torch.device) -> Optional[DeepfakeAnalysisResult]:\n",
        "    print(\"Running face forgery detection...\")\n",
        "    frames = processed_data.get(\"frames\")\n",
        "    if not frames or len(frames) < 3:\n",
        "        print(\"Insufficient frames for face forgery detection.\")\n",
        "        return None\n",
        "    mp_face_mesh = mp.solutions.face_mesh\n",
        "    face_mesh = mp_face_mesh.FaceMesh(\n",
        "        static_image_mode=False,\n",
        "        max_num_faces=3,\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5\n",
        "    )\n",
        "    face_embeddings = []\n",
        "    texture_consistency = []\n",
        "    eye_blink_detected = False\n",
        "    anomalies = []\n",
        "    sampling_rate = max(1, len(frames) // 30)\n",
        "    key_frames = frames[::sampling_rate]\n",
        "    for i, frame in enumerate(key_frames):\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        face_locations = face_recognition.face_locations(rgb_frame)\n",
        "        if face_locations:\n",
        "            face_landmarks = face_recognition.face_landmarks(rgb_frame, face_locations)\n",
        "            embeddings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
        "            if embeddings:\n",
        "                face_embeddings.append(embeddings[0])\n",
        "            mp_results = face_mesh.process(rgb_frame)\n",
        "            if mp_results.multi_face_landmarks:\n",
        "                for fl in mp_results.multi_face_landmarks:\n",
        "                    left_eye = [fl.landmark[idx] for idx in [33, 160, 158, 133, 153, 144]]\n",
        "                    right_eye = [fl.landmark[idx] for idx in [362, 385, 387, 263, 373, 380]]\n",
        "                    left_ear = calculate_ear(left_eye)\n",
        "                    right_ear = calculate_ear(right_eye)\n",
        "                    avg_ear = (left_ear + right_ear) / 2.0\n",
        "                    if avg_ear < 0.2:\n",
        "                        eye_blink_detected = True\n",
        "                facial_proportions = calculate_facial_proportions(mp_results.multi_face_landmarks[0])\n",
        "                standard_deviation = np.std(list(facial_proportions.values()))\n",
        "                if standard_deviation > 0.15:\n",
        "                    anomalies.append(\"Unnatural facial proportions detected\")\n",
        "                if i > 0:\n",
        "                    prev_frame_rgb = cv2.cvtColor(key_frames[i-1], cv2.COLOR_BGR2RGB)\n",
        "                    texture_diff = calculate_texture_difference(prev_frame_rgb, rgb_frame, mp_results.multi_face_landmarks[0])\n",
        "                    texture_consistency.append(texture_diff)\n",
        "                    smoothness = calculate_skin_smoothness(rgb_frame, mp_results.multi_face_landmarks[0])\n",
        "                    if smoothness > 0.8:\n",
        "                        anomalies.append(\"Unnaturally smooth skin texture\")\n",
        "        else:\n",
        "            print(f\"No faces detected in frame {i}\")\n",
        "    if len(face_embeddings) >= 2:\n",
        "        face_consistency = []\n",
        "        for i in range(1, len(face_embeddings)):\n",
        "            distance = np.linalg.norm(face_embeddings[i] - face_embeddings[i-1])\n",
        "            face_consistency.append(distance)\n",
        "        avg_distance = np.mean(face_consistency)\n",
        "        if avg_distance > 0.6:\n",
        "            anomalies.append(\"Face identity inconsistent across frames\")\n",
        "        if avg_distance < 0.05 and len(face_embeddings) > 5:\n",
        "            anomalies.append(\"Suspiciously static face across frames\")\n",
        "    if texture_consistency:\n",
        "        avg_texture_diff = np.mean(texture_consistency)\n",
        "        if avg_texture_diff > 0.3:\n",
        "            anomalies.append(\"Inconsistent facial texture across frames\")\n",
        "    if not eye_blink_detected and len(frames) > 90:\n",
        "        anomalies.append(\"No eye blinking detected throughout video\")\n",
        "    base_score = 1.0\n",
        "    score_reductions = {\n",
        "        \"Face identity inconsistent across frames\": 0.3,\n",
        "        \"Suspiciously static face across frames\": 0.2,\n",
        "        \"Inconsistent facial texture across frames\": 0.2,\n",
        "        \"No eye blinking detected throughout video\": 0.15,\n",
        "        \"Unnatural facial proportions detected\": 0.25,\n",
        "        \"Unnaturally smooth skin texture\": 0.15\n",
        "    }\n",
        "    for anomaly in anomalies:\n",
        "        if anomaly in score_reductions:\n",
        "            base_score -= score_reductions[anomaly]\n",
        "    final_score = max(0.0, min(1.0, base_score))\n",
        "    face_mesh.close()\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=\"FAKE\" if final_score < 0.7 else \"REAL\",\n",
        "        confidence=0.85,\n",
        "        method=\"face_forgery_detection\",\n",
        "        anomalies=anomalies,\n",
        "        explanation=f\"Analysis based on facial consistency across {len(key_frames)} key frames. {'No significant anomalies detected.' if not anomalies else 'Detected facial inconsistencies indicative of manipulation.'}\"\n",
        "    )\n",
        "\n",
        "def calculate_ear(eye_points):\n",
        "    v1 = distance_3d(eye_points[1], eye_points[5])\n",
        "    v2 = distance_3d(eye_points[2], eye_points[4])\n",
        "    h = distance_3d(eye_points[0], eye_points[3])\n",
        "    return (v1 + v2) / (2.0 * h) if h > 0 else 0\n",
        "\n",
        "def distance_3d(p1, p2):\n",
        "    return ((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2 + (p1.z - p2.z) ** 2) ** 0.5\n",
        "\n",
        "def calculate_facial_proportions(landmarks):\n",
        "    nose_tip = landmarks.landmark[4]\n",
        "    chin = landmarks.landmark[152]\n",
        "    left_eye = landmarks.landmark[159]\n",
        "    right_eye = landmarks.landmark[386]\n",
        "    left_mouth = landmarks.landmark[61]\n",
        "    right_mouth = landmarks.landmark[291]\n",
        "    eye_distance = distance_3d(left_eye, right_eye)\n",
        "    nose_to_chin = distance_3d(nose_tip, chin)\n",
        "    mouth_width = distance_3d(left_mouth, right_mouth)\n",
        "    return {\n",
        "        \"eye_to_nose_ratio\": eye_distance / distance_3d(nose_tip, (left_eye if left_eye.y < right_eye.y else right_eye)),\n",
        "        \"eye_to_mouth_ratio\": eye_distance / mouth_width,\n",
        "        \"nose_to_chin_ratio\": nose_to_chin / eye_distance\n",
        "    }\n",
        "\n",
        "def calculate_texture_difference(prev_frame, curr_frame, landmarks):\n",
        "    regions = []\n",
        "    cheek_points = [landmarks.landmark[i] for i in [116, 123, 147, 192, 213]]\n",
        "    forehead_points = [landmarks.landmark[i] for i in [10, 8, 109, 67, 103, 54, 21, 162]]\n",
        "    h, w, _ = curr_frame.shape\n",
        "    for points in [cheek_points, forehead_points]:\n",
        "        region_points = [(int(p.x * w), int(p.y * h)) for p in points]\n",
        "        if len(region_points) >= 3:\n",
        "            regions.append(region_points)\n",
        "    total_diff = 0\n",
        "    for region_points in regions:\n",
        "        mask = np.zeros((h, w), dtype=np.uint8)\n",
        "        cv2.fillPoly(mask, [np.array(region_points)], 255)\n",
        "        prev_region = cv2.bitwise_and(cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY), mask)\n",
        "        curr_region = cv2.bitwise_and(cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY), mask)\n",
        "        if np.sum(mask) > 0:\n",
        "            ssim_value = ssim(prev_region, curr_region, win_size=3)\n",
        "            total_diff += (1 - ssim_value)\n",
        "    return total_diff / max(1, len(regions))\n",
        "\n",
        "def calculate_skin_smoothness(frame, landmarks):\n",
        "    h, w, _ = frame.shape\n",
        "    cheek_points = [(int(landmarks.landmark[i].x * w), int(landmarks.landmark[i].y * h)) for i in [116, 123, 147, 192, 213]]\n",
        "    mask = np.zeros((h, w), dtype=np.uint8)\n",
        "    if len(cheek_points) >= 3:\n",
        "        cv2.fillPoly(mask, [np.array(cheek_points)], 255)\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        cheek = cv2.bitwise_and(gray, mask)\n",
        "        if np.sum(mask) > 0:\n",
        "            laplacian = cv2.Laplacian(cheek, cv2.CV_64F)\n",
        "            variance = np.var(laplacian[mask > 0])\n",
        "            smoothness = 1.0 - min(1.0, variance / 500.0)\n",
        "            return smoothness\n",
        "    return 0.5"
      ],
      "metadata": {
        "id": "_Aa84krCPU0n"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lip_sync_detection(processed_data: Dict[str, Any], models: Dict[str, Any], device: torch.device) -> Optional[DeepfakeAnalysisResult]:\n",
        "    print(\"Running lip sync detection...\")\n",
        "    frames = processed_data.get(\"frames\")\n",
        "    audio = processed_data.get(\"audio\")\n",
        "    if not frames or audio is None or len(frames) < 10:\n",
        "        print(\"Insufficient data for lip sync detection.\")\n",
        "        return None\n",
        "    mp_face_mesh = mp.solutions.face_mesh\n",
        "    face_mesh = mp_face_mesh.FaceMesh(\n",
        "        static_image_mode=False,\n",
        "        max_num_faces=1,\n",
        "        min_detection_confidence=0.5,\n",
        "        min_tracking_confidence=0.5\n",
        "    )\n",
        "    UPPER_LIP = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291]\n",
        "    LOWER_LIP = [146, 91, 181, 84, 17, 314, 405, 321, 375, 291]\n",
        "    fps = processed_data.get(\"fps\", 30)\n",
        "    audio_sample_rate = processed_data.get(\"audio_sample_rate\", 16000)\n",
        "    mfcc = librosa.feature.mfcc(\n",
        "        y=audio,\n",
        "        sr=audio_sample_rate,\n",
        "        n_mfcc=13,\n",
        "        hop_length=int(audio_sample_rate / fps)\n",
        "    )\n",
        "    mfcc_transposed = mfcc.T\n",
        "    lip_movements = []\n",
        "    lip_frames = []\n",
        "    frame_indices = []\n",
        "    sampling_rate = max(1, len(frames) // 100)\n",
        "    for i in range(0, len(frames), sampling_rate):\n",
        "        frame = frames[i]\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        results = face_mesh.process(rgb_frame)\n",
        "        if results.multi_face_landmarks:\n",
        "            landmarks = results.multi_face_landmarks[0]\n",
        "            h, w, _ = frame.shape\n",
        "            upper_lip_points = np.array([(int(landmarks.landmark[idx].x * w), int(landmarks.landmark[idx].y * h)) for idx in UPPER_LIP])\n",
        "            lower_lip_points = np.array([(int(landmarks.landmark[idx].x * w), int(landmarks.landmark[idx].y * h)) for idx in LOWER_LIP])\n",
        "            upper_y = np.mean(upper_lip_points[:, 1])\n",
        "            lower_y = np.mean(lower_lip_points[:, 1])\n",
        "            lip_opening = lower_y - upper_y\n",
        "            left_corner = np.array([landmarks.landmark[61].x * w, landmarks.landmark[61].y * h])\n",
        "            right_corner = np.array([landmarks.landmark[291].x * w, landmarks.landmark[291].y * h])\n",
        "            lip_width = np.linalg.norm(right_corner - left_corner)\n",
        "            all_lip_points = np.vstack((upper_lip_points, lower_lip_points))\n",
        "            x, y, w_lip, h_lip = cv2.boundingRect(all_lip_points)\n",
        "            lip_roi = frame[max(0, y-5):min(frame.shape[0], y+h_lip+5), max(0, x-5):min(frame.shape[1], x+w_lip+5)]\n",
        "            if lip_roi.size > 0:\n",
        "                lip_roi = cv2.resize(lip_roi, (64, 32))\n",
        "                lip_frames.append(lip_roi)\n",
        "                lip_movements.append([lip_opening, lip_width])\n",
        "                frame_indices.append(i)\n",
        "    face_mesh.close()\n",
        "    if not lip_movements:\n",
        "        print(\"No lip movements detected.\")\n",
        "        return None\n",
        "    lip_movements = np.array(lip_movements)\n",
        "    lip_movements_normalized = (lip_movements - np.mean(lip_movements, axis=0)) / np.std(lip_movements, axis=0)\n",
        "    min_length = min(len(lip_movements_normalized), mfcc_transposed.shape[0])\n",
        "    if min_length < 10:\n",
        "        print(\"Insufficient data for alignment analysis.\")\n",
        "        return None\n",
        "    if len(lip_movements_normalized) > min_length:\n",
        "        lip_movements_normalized = lip_movements_normalized[:min_length]\n",
        "    elif mfcc_transposed.shape[0] > min_length:\n",
        "        mfcc_transposed = mfcc_transposed[:min_length]\n",
        "    mfcc_subset = mfcc_transposed[:, :4]\n",
        "    distance, path = dtw(lip_movements_normalized[:, 0], mfcc_subset[:, 0], dist=lambda x, y: np.abs(x - y))\n",
        "    normalized_distance = distance / len(path)\n",
        "    correlation = np.corrcoef(lip_movements_normalized[:, 0], mfcc_subset[:, 0])[0, 1]\n",
        "    alignment_score = analyze_dtw_path(path, len(lip_movements_normalized))\n",
        "    lip_anomalies = detect_lip_anomalies(lip_movements_normalized, mfcc_subset)\n",
        "    async_frames = detect_async_frames(path, len(lip_movements_normalized))\n",
        "    final_score = calculate_lipsync_score(normalized_distance, correlation, alignment_score)\n",
        "    anomalies_list = [f\"DTW distance: {normalized_distance:.3f}\", f\"Audio-visual correlation: {correlation:.3f}\"]\n",
        "    if lip_anomalies:\n",
        "        anomalies_list.extend(lip_anomalies)\n",
        "    if async_frames and len(async_frames) > 0:\n",
        "        anomalies_list.append(f\"Detected {len(async_frames)} frames with poor lip sync\")\n",
        "    label = \"FAKE\" if final_score < 0.7 else \"REAL\"\n",
        "    explanation = (\n",
        "        f\"Lip sync analysis performed on {len(lip_movements)} lip movements and {mfcc_transposed.shape[0]} audio features. \"\n",
        "        f\"DTW distance: {normalized_distance:.3f}, Audio-visual correlation: {correlation:.3f}, \"\n",
        "        f\"Alignment score: {alignment_score:.3f}. \"\n",
        "    )\n",
        "    if label == \"FAKE\":\n",
        "        explanation += (\n",
        "            \"Detected significant misalignment between lip movements and audio, suggesting potential manipulation or synthetic generation.\"\n",
        "        )\n",
        "    else:\n",
        "        explanation += (\n",
        "            \"Lip movements and audio appear to be well synchronized, suggesting authentic content.\"\n",
        "        )\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=final_score,\n",
        "        label=label,\n",
        "        anomalies=anomalies_list,\n",
        "        confidence=0.8,\n",
        "        method=\"lip_sync_detection\",\n",
        "        explanation=explanation\n",
        "    )\n",
        "\n",
        "def analyze_dtw_path(path, sequence_length):\n",
        "    path = np.array(path)\n",
        "    ideal_path = np.array([[i, i] for i in range(sequence_length)])\n",
        "    deviation = 0\n",
        "    for i in range(min(len(path), len(ideal_path))):\n",
        "        deviation += np.linalg.norm(path[i] - ideal_path[i])\n",
        "    normalized_deviation = deviation / len(path)\n",
        "    max_possible_deviation = sequence_length\n",
        "    alignment_score = max(0, 1 - (normalized_deviation / max_possible_deviation))\n",
        "    return alignment_score\n",
        "\n",
        "def detect_lip_anomalies(lip_movements, audio_features):\n",
        "    anomalies = []\n",
        "    if np.var(lip_movements[:, 0]) < 0.05:\n",
        "        anomalies.append(\"Static lip movement during speech detected\")\n",
        "    if np.mean(audio_features[:, 0]) < 0.1 and np.mean(np.abs(lip_movements[:, 0])) > 0.3:\n",
        "        anomalies.append(\"Lip movement without corresponding audio detected\")\n",
        "    if np.mean(audio_features[:, 0]) > 0.3 and np.mean(np.abs(lip_movements[:, 0])) < 0.1:\n",
        "        anomalies.append(\"Audio without corresponding lip movement detected\")\n",
        "    return anomalies\n",
        "\n",
        "def detect_async_frames(path, sequence_length):\n",
        "    path = np.array(path)\n",
        "    async_frames = []\n",
        "    for i in range(1, len(path)):\n",
        "        jump = abs(path[i, 0] - path[i-1, 0]) + abs(path[i, 1] - path[i-1, 1])\n",
        "        if jump > 3:\n",
        "            async_frames.append(path[i, 0])\n",
        "    return async_frames\n",
        "\n",
        "def calculate_lipsync_score(dtw_distance, correlation, alignment_score):\n",
        "    weights = {'dtw': 0.4, 'correlation': 0.3, 'alignment': 0.3}\n",
        "    dtw_score = max(0, 1 - dtw_distance)\n",
        "    correlation_score = (correlation + 1) / 2 if correlation <= 1 else 1\n",
        "    final_score = (\n",
        "        weights['dtw'] * dtw_score +\n",
        "        weights['correlation'] * correlation_score +\n",
        "        weights['alignment'] * alignment_score\n",
        "    )\n",
        "    return max(0, min(1, final_score))"
      ],
      "metadata": {
        "id": "hW02KFJ5Pbjk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def background_consistency_analysis(data: Dict[str, Any], models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    print(\"Running background consistency analysis...\")\n",
        "    score = 0.80\n",
        "    anomalies = [\"Inconsistent background blur\", \"Shifting artifacts\"]\n",
        "    explanation = \"Background analysis shows irregularities inconsistent with natural scene dynamics.\"\n",
        "    return DeepfakeAnalysisResult(score=score, label=\"REAL\" if score > 0.7 else \"FAKE\",\n",
        "                                  confidence=0.12, method=\"background_consistency_analysis\",\n",
        "                                  anomalies=anomalies, explanation=explanation)"
      ],
      "metadata": {
        "id": "GOPUkFh9PtGC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_consistency_analysis(results: Dict[str, DeepfakeAnalysisResult], processed_data: Dict[str, Any], llm) -> Tuple[float, str]:\n",
        "    print(\"Running semantic consistency analysis...\")\n",
        "    prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "        You are a forensic expert. Compare the analysis components below:\n",
        "\n",
        "        Video Analysis: {video_analysis}\n",
        "        Audio Analysis: {audio_analysis}\n",
        "        Image Analysis: {image_analysis}\n",
        "        Text Analysis: {text_analysis}\n",
        "\n",
        "        Explain in detail whether the modalities provide consistent forensic evidence.\n",
        "        Provide a score between 0 and 1 indicating consistency and a detailed explanation.\n",
        "\n",
        "        Output format:\n",
        "        Score: <score>\n",
        "        Explanation: <detailed explanation>\n",
        "    \"\"\")\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    input_data = {\n",
        "        \"video_analysis\": results[\"video\"].dict() if results.get(\"video\") else \"N/A\",\n",
        "        \"audio_analysis\": results[\"audio\"].dict() if results.get(\"audio\") else \"N/A\",\n",
        "        \"image_analysis\": results[\"image\"].dict() if results.get(\"image\") else \"N/A\",\n",
        "        \"text_analysis\": results[\"text\"].dict() if results.get(\"text\") else \"N/A\"\n",
        "    }\n",
        "    response = chain.run(input_data)\n",
        "    score_match = re.search(r\"Score:\\s*(0\\.\\d+)\", response)\n",
        "    explanation_match = re.search(r\"Explanation:\\s*(.*)\", response, re.DOTALL)\n",
        "    score = float(score_match.group(1)) if score_match else 0.5\n",
        "    explanation = explanation_match.group(1).strip() if explanation_match else \"No detailed explanation provided.\"\n",
        "    return score, explanation\n",
        "\n",
        "async def real_time_streaming_analysis(video_stream: Any, models: Dict[str, Any], device: torch.device) -> DeepfakeAnalysisResult:\n",
        "    frames = []\n",
        "    audio_data = None\n",
        "    metadata = {\n",
        "        \"fps\": 30,\n",
        "        \"frame_count\": 0,\n",
        "        \"width\": 0,\n",
        "        \"height\": 0,\n",
        "        \"duration\": 0,\n",
        "        \"codec\": \"N/A\",\n",
        "        \"file_size\": 0\n",
        "    }\n",
        "    frame_quality_metrics = []\n",
        "    optical_flow_data = []\n",
        "    prev_frame = None\n",
        "    temporal_features = []\n",
        "    loop = asyncio.get_event_loop()\n",
        "    executor = ThreadPoolExecutor(max_workers=4)\n",
        "\n",
        "    async def process_frame(frame):\n",
        "        return await loop.run_in_executor(executor, enhance_image_resolution, frame)\n",
        "\n",
        "    async def process_quality_metrics(frame):\n",
        "        return await loop.run_in_executor(executor, lambda: {\n",
        "            \"blur\": cv2.Laplacian(frame, cv2.CV_64F).var(),\n",
        "            \"noise\": estimate_noise(frame),\n",
        "            \"brightness\": np.mean(frame),\n",
        "            \"contrast\": calculate_contrast(frame),\n",
        "            \"compression_artifacts\": detect_compression_artifacts(frame)\n",
        "        })\n",
        "\n",
        "    async def process_optical_flow(prev_frame, frame):\n",
        "        return await loop.run_in_executor(executor, calculate_dense_optical_flow, prev_frame, frame)\n",
        "\n",
        "    async def process_temporal_features(flow):\n",
        "        return await loop.run_in_executor(executor, extract_temporal_features, flow)\n",
        "\n",
        "    while True:\n",
        "        frame = await video_stream.read()\n",
        "        if frame is None:\n",
        "            break\n",
        "        frame, quality_metrics = await asyncio.gather(\n",
        "            process_frame(frame),\n",
        "            process_quality_metrics(frame)\n",
        "        )\n",
        "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(rgb_frame)\n",
        "        frame_quality_metrics.append(quality_metrics)\n",
        "        if prev_frame is not None:\n",
        "            flow = await process_optical_flow(prev_frame, frame)\n",
        "            temp_features = await process_temporal_features(flow)\n",
        "            optical_flow_data.append(flow)\n",
        "            temporal_features.append(temp_features)\n",
        "        prev_frame = frame.copy()\n",
        "\n",
        "    video_data = {\n",
        "        \"frames\": frames,\n",
        "        \"audio\": audio_data,\n",
        "        \"metadata\": metadata,\n",
        "        \"quality_metrics\": frame_quality_metrics,\n",
        "        \"optical_flow\": optical_flow_data,\n",
        "        \"temporal_features\": temporal_features\n",
        "    }\n",
        "    return await advanced_video_analysis(video_data, device)\n",
        "\n",
        "async def text_analysis(text: str, llm) -> DeepfakeAnalysisResult:\n",
        "    response = llm.call_as_llm(f\"Analyze the following text for signs of manipulation or inconsistencies:\\n\\n{text}\")\n",
        "    score_match = re.search(r\"Score: (0\\.\\d+|1\\.0)\", response)\n",
        "    score = float(score_match.group(1)) if score_match else 0.5\n",
        "    anomalies = [line.strip() for line in response.split(\"Reasoning:\")[-1].strip().split(\"\\n\") if line.strip()]\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=score,\n",
        "        label=\"REAL\" if score > 0.7 else \"FAKE\",\n",
        "        confidence=0.0,\n",
        "        method=\"text_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )\n",
        "\n",
        "def metadata_analysis(metadata: Dict[str, Any]) -> DeepfakeAnalysisResult:\n",
        "    anomalies = []\n",
        "    scores = []\n",
        "    if metadata.get(\"fps\") < 10:\n",
        "        anomalies.append(\"Unusually low frame rate\")\n",
        "        scores.append(0.2)\n",
        "    if metadata.get(\"duration\") < 1:\n",
        "        anomalies.append(\"Unusually short duration\")\n",
        "        scores.append(0.2)\n",
        "    if metadata.get(\"file_size\") < 100000:\n",
        "        anomalies.append(\"Unusually small file size\")\n",
        "        scores.append(0.2)\n",
        "    final_score = np.mean(scores) if scores else 0.5\n",
        "    return DeepfakeAnalysisResult(\n",
        "        score=float(final_score),\n",
        "        label=\"REAL\" if final_score > 0.7 else \"FAKE\",\n",
        "        confidence=float(np.std(scores)) if scores else 0.0,\n",
        "        method=\"metadata_analysis\",\n",
        "        anomalies=anomalies\n",
        "    )"
      ],
      "metadata": {
        "id": "k4hiqCPqPx8e"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_detection_graph() -> StateGraph:\n",
        "    async def preprocess(state):\n",
        "        input_data = state[\"input\"]\n",
        "        processed_data = await enhanced_preprocessing(input_data)\n",
        "        return {**state, \"processed_data\": processed_data}\n",
        "\n",
        "    async def analyze_modalities(state):\n",
        "        processed_data = state[\"processed_data\"]\n",
        "        models_env = state[\"models\"]\n",
        "        device = state[\"device\"]\n",
        "        results = {\n",
        "            \"video\": await advanced_video_analysis(processed_data, device) if \"frames\" in processed_data else None,\n",
        "            \"audio\": await advanced_audio_analysis(processed_data.get(\"audio\"), device) if \"audio\" in processed_data else None,\n",
        "            \"image\": await advanced_image_analysis(processed_data[\"image\"], device) if \"image\" in processed_data else None,\n",
        "            \"text\": await text_analysis(processed_data.get(\"text\"), models_env[\"text\"][0]) if \"text\" in processed_data else None,\n",
        "            \"face_forgery\": face_forgery_detection(processed_data, models_env, device) if \"frames\" in processed_data else None,\n",
        "            \"background\": background_consistency_analysis(processed_data, models_env, device) if \"frames\" in processed_data else None,\n",
        "            \"audio_spoofing\": await audio_spoofing_detection(processed_data.get(\"audio\"), processed_data[\"metadata\"].get(\"fps\", 16000), device) if \"audio\" in processed_data else None,\n",
        "            \"fake_call\": await fake_call_detection(processed_data.get(\"audio\"), processed_data[\"metadata\"].get(\"fps\", 16000), device) if \"audio\" in processed_data else None\n",
        "        }\n",
        "        return {**state, \"modality_results\": results}\n",
        "\n",
        "    async def cross_modal_analysis(state):\n",
        "        results = state[\"modality_results\"]\n",
        "        processed_data = state[\"processed_data\"]\n",
        "        models_env = state[\"models\"]\n",
        "        cross_modal_score, _ = semantic_consistency_analysis(results, processed_data, models_env[\"text\"][0])\n",
        "        return {**state, \"cross_modal_score\": cross_modal_score}\n",
        "\n",
        "    async def generate_report(state):\n",
        "        results = state[\"modality_results\"]\n",
        "        cross_modal_score = state[\"cross_modal_score\"]\n",
        "        processed_data = state[\"processed_data\"]\n",
        "        report = await generate_comprehensive_report(results, cross_modal_score, processed_data)\n",
        "        return {**state, \"final_report\": report}\n",
        "\n",
        "    workflow = StateGraph(nodes=[\n",
        "        (\"preprocess\", preprocess),\n",
        "        (\"analyze_modalities\", analyze_modalities),\n",
        "        (\"cross_modal_analysis\", cross_modal_analysis),\n",
        "        (\"generate_report\", generate_report)\n",
        "    ])\n",
        "    workflow.add_edge(\"preprocess\", \"analyze_modalities\")\n",
        "    workflow.add_edge(\"analyze_modalities\", \"cross_modal_analysis\")\n",
        "    workflow.add_edge(\"cross_modal_analysis\", \"generate_report\")\n",
        "    workflow.add_edge(\"generate_report\", END)\n",
        "    return workflow"
      ],
      "metadata": {
        "id": "O-Juqn2NP31-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def run_deepfake_detection(file_path: str, mode: str = \"all\") -> MultimodalAnalysisReport:\n",
        "    print(f\"Running deepfake detection on '{file_path}' with mode '{mode}'\")\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "        file_extension = os.path.splitext(file_path)[1].lower()\n",
        "        allowed_extensions = ['.mp4', '.avi', '.mov', '.jpg', '.jpeg', '.png', '.wav', '.mp3']\n",
        "        if file_extension not in allowed_extensions:\n",
        "            raise ValueError(f\"Unsupported file format: {file_extension}. Supported: {', '.join(allowed_extensions)}\")\n",
        "        if os.path.getsize(file_path) == 0:\n",
        "            raise ValueError(\"File is empty\")\n",
        "        env = {\n",
        "            \"models\": models,\n",
        "            \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        }\n",
        "        print(f\"Using device: {env['device']}\")\n",
        "        state = {\n",
        "            \"input\": file_path,\n",
        "            \"mode\": mode,\n",
        "            \"models\": env[\"models\"],\n",
        "            \"device\": env[\"device\"]\n",
        "        }\n",
        "        print(\"Executing detection graph workflow...\")\n",
        "        workflow = create_detection_graph()\n",
        "        final_state = await workflow.run(state)\n",
        "        print(\"Deepfake detection completed successfully.\")\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        return final_state[\"final_report\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Error during detection: {e}\")\n",
        "        raise\n",
        "\n",
        "async def enhanced_preprocessing(input_data: str) -> dict:\n",
        "    print(f\"Preprocessing {input_data}...\")\n",
        "    video = VideoFileClip(input_data)\n",
        "    frames = [frame for frame in video.iter_frames()]\n",
        "    audio = video.audio.to_soundarray() if video.audio else None\n",
        "    return {\n",
        "        \"frames\": frames,\n",
        "        \"audio\": audio,\n",
        "        \"image\": frames[0] if frames else None,\n",
        "        \"text\": \"Sample extracted text\",\n",
        "        \"metadata\": {\n",
        "            \"file_path\": input_data,\n",
        "            \"duration\": video.duration,\n",
        "            \"fps\": video.fps,\n",
        "            \"file_size\": os.path.getsize(input_data)\n",
        "        }\n",
        "    }\n",
        "\n",
        "async def analyze_cross_modal_consistency(\n",
        "    results: dict, processed_data: dict, models_env: dict\n",
        ") -> float:\n",
        "    scores = []\n",
        "    if results.get(\"audio\") and results.get(\"video\"):\n",
        "        sync_score = analyze_av_sync(processed_data[\"frames\"], processed_data[\"audio\"])\n",
        "        scores.append(sync_score)\n",
        "    semantic_score, _ = semantic_consistency_analysis(results, processed_data, models_env[\"text\"][0])\n",
        "    scores.append(semantic_score)\n",
        "    temporal_score = 0.8\n",
        "    bio_score = 0.9\n",
        "    scores.extend([temporal_score, bio_score])\n",
        "    return float(np.mean(scores))\n",
        "\n",
        "async def generate_comprehensive_report(\n",
        "    results: dict, cross_modal_score: float, processed_data: dict\n",
        ") -> MultimodalAnalysisReport:\n",
        "    scores = [\n",
        "        results[\"video\"].score if results.get(\"video\") else 0.5,\n",
        "        results[\"audio\"].score if results.get(\"audio\") else 0.5,\n",
        "        results[\"image\"].score if results.get(\"image\") else 0.5,\n",
        "        results[\"text\"].score if results.get(\"text\") else 0.5,\n",
        "        cross_modal_score\n",
        "    ]\n",
        "    weights = [0.3, 0.2, 0.2, 0.2, 0.1]\n",
        "    final_score = sum(s * w for s, w in zip(scores, weights))\n",
        "    evidence = []\n",
        "    for modality, result in results.items():\n",
        "        if result:\n",
        "            evidence.extend([\n",
        "                {\"type\": modality, \"description\": anomaly, \"confidence\": result.confidence, \"method\": result.method}\n",
        "                for anomaly in result.anomalies\n",
        "            ])\n",
        "    return MultimodalAnalysisReport(\n",
        "        case_id=f\"DFD-{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
        "        file_info=processed_data[\"metadata\"] if \"metadata\" in processed_data else {},\n",
        "        video_analysis=results.get(\"video\"),\n",
        "        audio_analysis=results.get(\"audio\"),\n",
        "        image_analysis=results.get(\"image\"),\n",
        "        text_analysis=results.get(\"text\"),\n",
        "        multimodal_score=float(final_score),\n",
        "        verdict=\"AUTHENTIC\" if final_score > 0.7 else \"MANIPULATED\",\n",
        "        evidence=evidence,\n",
        "        metadata={\n",
        "            \"processing_time\": datetime.now().isoformat(),\n",
        "            \"models_used\": list(results.keys()),\n",
        "            \"cross_modal_score\": cross_modal_score,\n",
        "            \"confidence_distribution\": {\n",
        "                modality: result.confidence for modality, result in results.items() if result\n",
        "            }\n",
        "        },\n",
        "        processing_time=0.0\n",
        "    )"
      ],
      "metadata": {
        "id": "LhX6YJRsP8of"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline_interactive(file_path: str, mode: str):\n",
        "    loop = asyncio.get_event_loop()\n",
        "    try:\n",
        "        report = loop.run_until_complete(run_deepfake_detection(file_path, mode))\n",
        "        report_json = report.json(indent=2, default=str)\n",
        "        output_area.value = report_json\n",
        "    except Exception as issue:\n",
        "        output_area.value = json.dumps({\"error\": str(issue)}, indent=2)\n",
        "\n",
        "# Create an ipywidgets FileUpload widget for local file upload.\n",
        "upload_widget = widgets.FileUpload(\n",
        "    accept=\".mp4,.avi,.mov,.jpg,.jpeg,.png,.wav,.mp3\",\n",
        "    multiple=False,\n",
        "    description=\"Upload File\"\n",
        ")\n",
        "\n",
        "# Detection mode selection widget.\n",
        "mode_widget = widgets.Dropdown(\n",
        "    options=[\"all\", \"audio\", \"video\", \"image\"],\n",
        "    value=\"all\",\n",
        "    description=\"Detection Mode:\"\n",
        ")\n",
        "\n",
        "# Button widget to run the pipeline.\n",
        "run_button = widgets.Button(\n",
        "    description=\"Run Deepfake Detection\",\n",
        "    button_style=\"success\"\n",
        ")\n",
        "\n",
        "# Textarea widget to display the final deepfake analysis report.\n",
        "output_area = widgets.Textarea(\n",
        "    value=\"\",\n",
        "    placeholder=\"Deepfake analysis report will appear here...\",\n",
        "    description=\"Report:\",\n",
        "    layout=widgets.Layout(width=\"100%\", height=\"300px\")\n",
        ")\n",
        "\n",
        "def on_run_button_clicked(b):\n",
        "    clear_output(wait=True)\n",
        "    display(ui)\n",
        "    if upload_widget.value:\n",
        "        for fname, file_info in upload_widget.value.items():\n",
        "            file_path = os.path.join(\"uploads\", fname)\n",
        "            os.makedirs(\"uploads\", exist_ok=True)\n",
        "            with open(file_path, \"wb\") as f:\n",
        "                f.write(file_info[\"content\"])\n",
        "            run_pipeline_interactive(file_path, mode_widget.value)\n",
        "            break\n",
        "    else:\n",
        "        output_area.value = \"Please upload a file.\"\n",
        "\n",
        "run_button.on_click(on_run_button_clicked)\n",
        "\n",
        "# Arrange the widgets in a vertical box layout.\n",
        "ui = widgets.VBox([upload_widget, mode_widget, run_button, output_area])\n",
        "display(ui)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431,
          "referenced_widgets": [
            "b34c1c57888a4116917674bfd0feef40",
            "0b150c8951954b3eb9cf583f8b16580d",
            "e82b4b526f034be28ead250b9fca3f02",
            "76d06975477a445fbdcb06e69b6c1fce",
            "4df72d81c6894835b710a4a1dfb3a437",
            "f1b98864d4fd4aba9da59d9e60052bba",
            "e3f3b3193f0f479c9d556762265e5eac",
            "be346291170e44a49b044b0eaa961edd",
            "1049e26f949944d7b88c304bbc9cf6d2",
            "6f49a626be7d46e0bce2f6e452e38d40",
            "741f2708716842a580e453a3a0b15865",
            "7a456077754a43339bf138662c911d24",
            "33c8053c52d8448b8806c444a8d8472f",
            "084c60624a5742e1be61e9ae0b288673"
          ]
        },
        "id": "-Xzeb8wAQE4T",
        "outputId": "268942be-31ff-4ee5-ab83-e94b4894566b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(FileUpload(value={}, accept='.mp4,.avi,.mov,.jpg,.jpeg,.png,.wav,.mp3', description='Upload Fil…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b34c1c57888a4116917674bfd0feef40"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C5apz_HrQOB9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}