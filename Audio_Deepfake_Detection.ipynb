{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "41ddeb180c9d4cb59918348017471193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_943dfc2d890741f0a931cf518c5a1667",
              "IPY_MODEL_d90cf447a1d44ab6ab0de05a324bff0a",
              "IPY_MODEL_be3b01019da749a6a780f68cf6197a00",
              "IPY_MODEL_f73e510193a34c9991b6bd1b8facd744",
              "IPY_MODEL_6fd4e970927c4f4f862a3fcae2810bc9",
              "IPY_MODEL_310ae85a5b124e97a3b6a6caf72e3936"
            ],
            "layout": "IPY_MODEL_a170a0ebb5e5467ca62022928c163378"
          }
        },
        "943dfc2d890741f0a931cf518c5a1667": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_195480593d664d01aaf07ad1a944d8c5",
              "IPY_MODEL_0b421cb2609847caadf69a018cf6e63c",
              "IPY_MODEL_46e3b6c0ed694647a2f7b81c891cb534"
            ],
            "layout": "IPY_MODEL_96b87e31f365423bbaf5154dd0d66efd"
          }
        },
        "d90cf447a1d44ab6ab0de05a324bff0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3ba8e7f37b240929347f6cfc2fde589",
            "placeholder": "​",
            "style": "IPY_MODEL_a867a68869f0423191959ab02684c1f2",
            "value": "Status: Waiting for file upload..."
          }
        },
        "be3b01019da749a6a780f68cf6197a00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Run Deepfake Detection",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_636f9d14597847c08bb039659a610adc",
            "style": "IPY_MODEL_d335bf11605f45ae964d97d912072e1c",
            "tooltip": ""
          }
        },
        "f73e510193a34c9991b6bd1b8facd744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TabModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TabModel",
            "_titles": {
              "0": "Summary",
              "1": "Logs",
              "2": "Raw JSON"
            },
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TabView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33b2dcf026fb4856b914e077878c633a",
              "IPY_MODEL_132ff040874c4def9e4a5bd56b607fc2",
              "IPY_MODEL_9184c45b0b78450da1beb3e6bb198452"
            ],
            "layout": "IPY_MODEL_b76639c08e4b4833a065b217c86d78d0",
            "selected_index": 1
          }
        },
        "6fd4e970927c4f4f862a3fcae2810bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2da31718c7e2488fa2d404df22afc248",
            "placeholder": "​",
            "style": "IPY_MODEL_6558621b719642b6a1df24c0cfaa254b",
            "value": "Pipeline Logs:"
          }
        },
        "310ae85a5b124e97a3b6a6caf72e3936": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_dd92ef3ec72649e29b302082c303b380",
            "msg_id": "",
            "outputs": []
          }
        },
        "a170a0ebb5e5467ca62022928c163378": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "195480593d664d01aaf07ad1a944d8c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".wav,.mp3,.flac,.m4a",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_7016580fba0741c4a6e3eeb4f6f1c6e8",
            "metadata": [
              {
                "name": "trump-to-taylor.wav",
                "type": "audio/wav",
                "size": 105840150,
                "lastModified": 1743840200240
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_97cb97624496420e945501176fc78ada"
          }
        },
        "0b421cb2609847caadf69a018cf6e63c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5445c20eba2d44919e33affe3de82e63",
            "placeholder": "​",
            "style": "IPY_MODEL_4a56e9406f134758ae422c08bce5dde4",
            "value": "File Size: Not Uploaded"
          }
        },
        "46e3b6c0ed694647a2f7b81c891cb534": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "Upload:",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c87e7d90e95e4594b7ff5878b87d26aa",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65a7b0e4c8ca41e9b3b3218543dcf112",
            "value": 0
          }
        },
        "96b87e31f365423bbaf5154dd0d66efd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3ba8e7f37b240929347f6cfc2fde589": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a867a68869f0423191959ab02684c1f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "636f9d14597847c08bb039659a610adc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d335bf11605f45ae964d97d912072e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "33b2dcf026fb4856b914e077878c633a": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_896728ac5a8245a4a65519ecff94cef3",
            "msg_id": "",
            "outputs": []
          }
        },
        "132ff040874c4def9e4a5bd56b607fc2": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_ccca3934c6d744c787f2fe531f81b589",
            "msg_id": "",
            "outputs": []
          }
        },
        "9184c45b0b78450da1beb3e6bb198452": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_fe2056b38d044274a070d1358cb9a743",
            "msg_id": "",
            "outputs": []
          }
        },
        "b76639c08e4b4833a065b217c86d78d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2da31718c7e2488fa2d404df22afc248": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6558621b719642b6a1df24c0cfaa254b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7016580fba0741c4a6e3eeb4f6f1c6e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97cb97624496420e945501176fc78ada": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "5445c20eba2d44919e33affe3de82e63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a56e9406f134758ae422c08bce5dde4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c87e7d90e95e4594b7ff5878b87d26aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65a7b0e4c8ca41e9b3b3218543dcf112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd92ef3ec72649e29b302082c303b380": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "896728ac5a8245a4a65519ecff94cef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccca3934c6d744c787f2fe531f81b589": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe2056b38d044274a070d1358cb9a743": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/deepfake/blob/main/Audio_Deepfake_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Audio Deepfake Detection, Fake Calls, Spoofing, Fraud Calls and Voice Cloning Analysis for Defensice Forensics**"
      ],
      "metadata": {
        "id": "P4GCDUZqAlnl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "71zcprttAcJh"
      },
      "outputs": [],
      "source": [
        "!pip install -q numpy librosa soundfile matplotlib IPython webrtcvad pydub noisereduce pyAudioAnalysis speechbrain langchain openai langgraph transformers vllm requests ipywidgets\n",
        "!pip install -q audiomentations hmmlearn eyed3 langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "import concurrent.futures\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile as sf\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "\n",
        "import webrtcvad\n",
        "from pydub import AudioSegment\n",
        "from audiomentations import Compose, AddGaussianNoise\n",
        "import noisereduce as nr\n",
        "\n",
        "from pyAudioAnalysis import audioSegmentation as aS\n",
        "import speechbrain as sb\n",
        "from speechbrain.inference.speaker import SpeakerRecognition\n",
        "\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "import openai\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from vllm import LLM, EngineArgs, SamplingParams\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output, HTML"
      ],
      "metadata": {
        "id": "WJ1SfiSpCKgS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Data Model for Forensic Report\n",
        "#############################################\n",
        "class ForensicReport:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.file = kwargs.get(\"file\")\n",
        "        self.verdict = kwargs.get(\"verdict\")\n",
        "        self.mean_score = kwargs.get(\"mean_score\")\n",
        "        self.confidence = kwargs.get(\"confidence\")\n",
        "        self.all_model_scores = kwargs.get(\"all_model_scores\")\n",
        "        self.all_anomalies = kwargs.get(\"all_anomalies\")\n",
        "        self.natural_summary = kwargs.get(\"natural_summary\")\n",
        "        self.asr_transcript = kwargs.get(\"asr_transcript\")\n",
        "        self.asr_lang = kwargs.get(\"asr_lang\")\n",
        "        self.speaker_identities = kwargs.get(\"speaker_identities\")\n",
        "        self.speaker_spoof_score = kwargs.get(\"speaker_spoof_score\")\n",
        "        self.noise_quality_score = kwargs.get(\"noise_quality_score\")\n",
        "        self.gender_distribution = kwargs.get(\"gender_distribution\")\n",
        "        self.detailed_results = kwargs.get(\"detailed_results\")\n",
        "        self.timestamp = kwargs.get(\"timestamp\")\n",
        "        self.extra_info = kwargs.get(\"extra_info\", {})\n",
        "        self.vllm_model_outputs = kwargs.get(\"vllm_model_outputs\", {})\n",
        "\n",
        "    def json(self, indent=2):\n",
        "        return json.dumps(self.__dict__, indent=indent)"
      ],
      "metadata": {
        "id": "lGmrhtdJqV5s"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Audio Preprocessing and Feature Extraction\n",
        "#############################################\n",
        "def preprocess_audio(audio_path: str, out_sr: int = 16000, mono: bool = True, reduce_noise: bool = True) -> Tuple[np.ndarray, int]:\n",
        "    print(\"[Step 1] Loading and preprocessing audio...\")\n",
        "    ext = os.path.splitext(audio_path)[1].lower()\n",
        "    if ext == \".wav\":\n",
        "        try:\n",
        "            audio, sr = sf.read(audio_path, dtype='float32', always_2d=False, mmap=True)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error reading WAV file {audio_path}: {e}\")\n",
        "        if mono and audio.ndim > 1:\n",
        "            audio = np.mean(audio, axis=1)\n",
        "        if sr != out_sr:\n",
        "            print(\"[Step 1] Resampling from {} to {} Hz\".format(sr, out_sr))\n",
        "            audio = librosa.resample(audio, orig_sr=sr, target_sr=out_sr)\n",
        "            sr = out_sr\n",
        "    else:\n",
        "        print(\"[Step 1] Converting non-WAV file to WAV format...\")\n",
        "        audio_seg = AudioSegment.from_file(audio_path)\n",
        "        audio_seg = audio_seg.set_frame_rate(out_sr).set_channels(1 if mono else 2)\n",
        "        temp_wav = \"temp_input.wav\"\n",
        "        audio_seg.export(temp_wav, format=\"wav\")\n",
        "        audio, sr = sf.read(temp_wav, dtype='float32', always_2d=False, mmap=True)\n",
        "        if mono and audio.ndim > 1:\n",
        "            audio = np.mean(audio, axis=1)\n",
        "        os.remove(temp_wav)\n",
        "    audio = audio / (np.max(np.abs(audio)) + 1e-8)\n",
        "    if reduce_noise:\n",
        "        print(\"[Step 1] Applying noise reduction...\")\n",
        "        try:\n",
        "            audio = nr.reduce_noise(y=audio, sr=sr)\n",
        "        except Exception:\n",
        "            print(\"[Step 1] Noise reduction failed; proceeding without.\")\n",
        "    print(\"[Step 1] Audio loaded successfully with {} samples at {} Hz\".format(len(audio), sr))\n",
        "    return audio, sr\n",
        "\n",
        "def extract_features(audio: np.ndarray, sr: int) -> Dict[str, float]:\n",
        "    print(\"[Step 2] Extracting audio features...\")\n",
        "    feat = {}\n",
        "    feat['duration'] = len(audio) / sr\n",
        "    feat['energy'] = np.sqrt(np.mean(audio ** 2))\n",
        "    feat['zcr'] = np.mean(librosa.feature.zero_crossing_rate(y=audio))\n",
        "    feat['rmse'] = np.mean(librosa.feature.rms(y=audio))\n",
        "    feat['spec_centroid'] = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sr))\n",
        "    feat['spec_bandwidth'] = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=sr))\n",
        "    feat['spec_rolloff'] = np.mean(librosa.feature.spectral_rolloff(y=audio, sr=sr))\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
        "    feat['mfcc_mean'] = np.mean(mfccs)\n",
        "    feat['mfcc_std'] = np.std(mfccs)\n",
        "    chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
        "    feat['chroma_mean'] = np.mean(chroma)\n",
        "    feat['chroma_std'] = np.std(chroma)\n",
        "    st_feats, _ = aS.feature_extraction(audio, sr, int(0.050 * sr), int(0.025 * sr))\n",
        "    feat['st_energy_std'] = np.std(st_feats[1, :])\n",
        "    feat['spectral_flatness'] = np.mean(librosa.feature.spectral_flatness(y=audio))\n",
        "    try:\n",
        "        signal_power = np.mean(audio ** 2)\n",
        "        noise_est = audio - librosa.effects.hpss(audio)[1]\n",
        "        noise_power = np.mean(noise_est ** 2)\n",
        "        feat['snr_est'] = 10 * np.log10((signal_power + 1e-6) / (noise_power + 1e-6))\n",
        "    except Exception:\n",
        "        feat['snr_est'] = 0\n",
        "    print(\"[Step 2] Features extracted: Duration {:.2f} sec, Energy {:.3f}, ZCR {:.3f}\".format(feat['duration'], feat['energy'], feat['zcr']))\n",
        "    return feat\n",
        "\n",
        "def extract_vad_ratio(audio: np.ndarray, sr: int) -> float:\n",
        "    print(\"[Step 3] Running VAD (Voice Activity Detection)...\")\n",
        "    try:\n",
        "        vad = webrtcvad.Vad(2)\n",
        "        audio_bytes = (audio * 32768).astype(np.int16).tobytes()\n",
        "        speech_frames = 0\n",
        "        total_frames = 0\n",
        "        frame_length = 320\n",
        "        for i in range(0, len(audio_bytes), frame_length):\n",
        "            if i+frame_length > len(audio_bytes):\n",
        "                break\n",
        "            total_frames += 1\n",
        "            if vad.is_speech(audio_bytes[i:i+frame_length], sr):\n",
        "                speech_frames += 1\n",
        "        ratio = speech_frames / (total_frames + 1e-8)\n",
        "    except Exception:\n",
        "        ratio = 0\n",
        "    print(f\"[Step 3] VAD ratio: {ratio:.3f}\")\n",
        "    return ratio\n",
        "\n",
        "def extract_gender_distribution(audio_path: str) -> Tuple[int, int, Dict[str, float]]:\n",
        "    print(\"[Step 4] Estimating gender distribution via speaker diarization...\")\n",
        "    try:\n",
        "        segs, _ = py_audio_segmentation(audio_path)\n",
        "        male_segs = sum(1 for s in segs if \"male\" in s.lower())\n",
        "        female_segs = sum(1 for s in segs if \"female\" in s.lower())\n",
        "        total = male_segs + female_segs\n",
        "        distribution = {\"male\": male_segs/total if total>0 else 0,\n",
        "                        \"female\": female_segs/total if total>0 else 0}\n",
        "        print(\"[Step 4] Gender distribution: {}\".format(distribution))\n",
        "        return (len(segs), male_segs, distribution)\n",
        "    except Exception:\n",
        "        return (0, 0, {})\n",
        "\n",
        "def py_audio_segmentation(audio_path: str) -> Tuple[List[str], List[Any]]:\n",
        "    try:\n",
        "        segs, classes, _ = aS.speaker_diarization(audio_path, 2, plot_res=False)\n",
        "        seg_labels = [\"male\" if c==0 else \"female\" for c in classes]\n",
        "        return seg_labels, classes\n",
        "    except Exception:\n",
        "        return [], []"
      ],
      "metadata": {
        "id": "1T5j-gOysSNC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Forensic Agent Functions using SpeechBrain\n",
        "#############################################\n",
        "def run_speechbrain_speaker(audio_path: str) -> Tuple[List[str], List[Any]]:\n",
        "    print(\"[Step 5] Running speaker diarization (SpeechBrain)...\")\n",
        "    segs, _ = py_audio_segmentation(audio_path)\n",
        "    speakers = list(set(segs))\n",
        "    print(\"[Step 5] Detected speakers: {}\".format(speakers))\n",
        "    return speakers, segs\n",
        "\n",
        "def run_speechbrain_verification(audio_path: str) -> float:\n",
        "    print(\"[Step 6] Running speaker verification (SpeechBrain)...\")\n",
        "    spkr_model = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", savedir=\"tmp_spkrec\")\n",
        "    try:\n",
        "        result = spkr_model.verify_files(audio_path, audio_path)\n",
        "        score = float(result['score'])\n",
        "    except Exception:\n",
        "        score = 0.0\n",
        "    print(\"[Step 6] Verification score: {:.3f}\".format(score))\n",
        "    return score\n",
        "\n",
        "def run_speechbrain_spoof(audio_path: str) -> Tuple[float, List[str]]:\n",
        "    print(\"[Step 7] Running spoof detection (SpeechBrain)...\")\n",
        "    try:\n",
        "        model = EncoderClassifier.from_hparams(source=\"speechbrain/anti-spoofing-ecapa-voxceleb\", savedir=\"tmp_spoof\")\n",
        "        output = model.classify_file(audio_path)[0]\n",
        "        score = float(output.detach().cpu().numpy()[1])\n",
        "        anomalies = [\"SpeechBrain spoof detected\"] if score > 0.5 else []\n",
        "    except Exception:\n",
        "        score, anomalies = 0.3, []\n",
        "    print(\"[Step 7] Spoof score: {:.3f}\".format(score))\n",
        "    return score, anomalies\n",
        "\n",
        "def run_language_id(audio_path: str) -> Tuple[str, float]:\n",
        "    print(\"[Step 8] Running language identification (SpeechBrain)...\")\n",
        "    try:\n",
        "        langid = LanguageIdentification.from_hparams(source=\"speechbrain/lang-id-commonlanguage_ecapa\", savedir=\"tmp_langid\")\n",
        "        result = langid.classify_file(audio_path)\n",
        "        lang = result[3][0] if result[3] else \"unknown\"\n",
        "        conf = float(result[1][0]) if result[1] else 0.0\n",
        "    except Exception:\n",
        "        lang, conf = \"unknown\", 0.0\n",
        "    print(\"[Step 8] Detected language: {} with confidence {:.2f}\".format(lang, conf))\n",
        "    return lang, conf\n",
        "\n",
        "def run_wave2vec_fake_detection(audio_path: str) -> Tuple[float, List[str]]:\n",
        "    print(\"[Step 9] Running wave2vec-based fake detection...\")\n",
        "    audio, sr = librosa.load(audio_path, sr=16000)\n",
        "    zcr = np.mean(librosa.feature.zero_crossing_rate(audio))\n",
        "    if zcr > 0.2:\n",
        "        return 0.8, [\"High ZCR detected: potential synthetic voice.\"]\n",
        "    return 0.3, []\n",
        "\n",
        "def run_replay_attack_detection(audio: np.ndarray, sr: int) -> Tuple[float, List[str]]:\n",
        "    print(\"[Step 10] Running replay attack detection...\")\n",
        "    rms = np.mean(librosa.feature.rms(y=audio))\n",
        "    if rms < 0.01:\n",
        "        return 0.7, [\"Low RMS detected: potential replay attack.\"]\n",
        "    return 0.2, []\n",
        "\n",
        "def run_augmentation_tests(audio: np.ndarray, sr: int) -> Dict[str, float]:\n",
        "    print(\"[Step 11] Running augmentation tests...\")\n",
        "    aug = Compose([AddGaussianNoise(min_amplitude=0.01, max_amplitude=0.05, p=1.0)])\n",
        "    aug_audio = aug(samples=audio, sample_rate=sr)\n",
        "    zcr_orig = np.mean(librosa.feature.zero_crossing_rate(audio))\n",
        "    zcr_aug = np.mean(librosa.feature.zero_crossing_rate(aug_audio))\n",
        "    score_diff = abs(zcr_orig - zcr_aug)\n",
        "    return {'zcr_aug_diff': score_diff}\n",
        "\n",
        "def run_enhanced_emotion_detection(audio: np.ndarray, sr: int) -> Tuple[float, List[str], str]:\n",
        "    print(\"[Step 12] Running enhanced emotion detection (SpeechBrain)...\")\n",
        "    try:\n",
        "        classifier = EncoderClassifier.from_hparams(\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", savedir=\"tmp_emotion\")\n",
        "        import torch\n",
        "        out_prob, score, index, text_lab = classifier.classify_batch(torch.tensor(audio).unsqueeze(0))\n",
        "        emotion = text_lab[0]\n",
        "        conf = float(score[0])\n",
        "        anomalies = [f\"Emotion detected: {emotion} (conf: {conf:.2f})\"]\n",
        "        desc = f\"Emotion: {emotion} (confidence: {conf:.2f})\"\n",
        "    except Exception:\n",
        "        conf, anomalies, desc = 0.1, [\"Enhanced emotion detection failed; default value used.\"], \"emotion:unknown\"\n",
        "    print(\"[Step 12] Detected emotion: {}\".format(desc))\n",
        "    return conf, anomalies, desc\n",
        "\n",
        "def run_asr_transcription(audio_path: str) -> str:\n",
        "    print(\"[Step 13] Running ASR transcription (SpeechBrain)...\")\n",
        "    try:\n",
        "        asr_model = EncoderDecoderASR.from_hparams(source=\"speechbrain/asr-crdnn-rnnlm-librispeech\", savedir=\"tmp_asr\")\n",
        "        transcript = asr_model.transcribe_file(audio_path)\n",
        "    except Exception as e:\n",
        "        transcript = f\"ASR transcription failed: {e}\"\n",
        "    print(\"[Step 13] Transcript: {}\".format(transcript[:100]))\n",
        "    return transcript"
      ],
      "metadata": {
        "id": "2vj-aP3gsXRW"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# vLLM Audio Model Integrations\n",
        "#############################################\n",
        "def run_minicpmo(question: str, audio_count: int) -> str:\n",
        "    print(\"[vLLM] Running MiniCPM-o model...\")\n",
        "    model_name = \"openbmb/MiniCPM-o-2_6\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_name,\n",
        "        trust_remote_code=True,\n",
        "        max_model_len=4096,\n",
        "        max_num_seqs=2,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    stop_tokens = ['<|im_end|>', '<|endoftext|>']\n",
        "    stop_token_ids = [tokenizer.convert_tokens_to_ids(tok) for tok in stop_tokens]\n",
        "    audio_placeholder = \"()\" * audio_count\n",
        "    messages = [{'role': 'user', 'content': f'{audio_placeholder}\\n{question}'}]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    llm = LLM(**engine_args.__dict__)\n",
        "    sampling_params = SamplingParams(temperature=0.2, max_tokens=128, stop_token_ids=stop_token_ids)\n",
        "    inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"audio\": []}}\n",
        "    outputs = llm.generate([inputs], sampling_params=sampling_params)\n",
        "    try:\n",
        "        text = outputs[0].outputs[0].text\n",
        "    except Exception:\n",
        "        text = \"[vLLM MiniCPM-o error: no output]\"\n",
        "    print(\"[vLLM] MiniCPM-o output: {}\".format(text))\n",
        "    return text\n",
        "\n",
        "def run_phi4_mm(question: str, audio_count: int) -> str:\n",
        "    print(\"[vLLM] Running Phi-4 MM model...\")\n",
        "    from huggingface_hub import snapshot_download\n",
        "    model_path = snapshot_download(\"microsoft/Phi-4-multimodal-instruct\")\n",
        "    placeholders = \"\".join([f\"<|audio_{i+1}|>\" for i in range(audio_count)])\n",
        "    prompt = f\"<|user|>{placeholders}{question}<|end|><|assistant|>\"\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_path,\n",
        "        trust_remote_code=True,\n",
        "        max_model_len=4096,\n",
        "        max_num_seqs=2,\n",
        "        enable_lora=True,\n",
        "        max_lora_rank=320,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    llm = LLM(**engine_args.__dict__)\n",
        "    sampling_params = SamplingParams(temperature=0.2, max_tokens=128)\n",
        "    inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"audio\": []}}\n",
        "    outputs = llm.generate([inputs], sampling_params=sampling_params)\n",
        "    try:\n",
        "        text = outputs[0].outputs[0].text\n",
        "    except Exception:\n",
        "        text = \"[vLLM Phi-4 MM error: no output]\"\n",
        "    print(\"[vLLM] Phi-4 MM output: {}\".format(text))\n",
        "    return text\n",
        "\n",
        "def run_qwen2_audio(question: str, audio_count: int) -> str:\n",
        "    print(\"[vLLM] Running Qwen2-Audio model...\")\n",
        "    model_name = \"Qwen/Qwen2-Audio-7B-Instruct\"\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_name,\n",
        "        max_model_len=4096,\n",
        "        max_num_seqs=5,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    audio_in_prompt = \"\".join([f\"Audio {i+1}: <|audio_bos|><|AUDIO|><|audio_eos|>\\n\" for i in range(audio_count)])\n",
        "    prompt = (\"<|im_start|>system\\nYou are an audio forensic assistant.<|im_end|>\\n\"\n",
        "              \"<|im_start|>user\\n\" + audio_in_prompt + question + \"<|im_end|>\\n\"\n",
        "              \"<|im_start|>assistant\\n\")\n",
        "    llm = LLM(**engine_args.__dict__)\n",
        "    sampling_params = SamplingParams(temperature=0.2, max_tokens=128)\n",
        "    inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"audio\": []}}\n",
        "    outputs = llm.generate([inputs], sampling_params=sampling_params)\n",
        "    try:\n",
        "        text = outputs[0].outputs[0].text\n",
        "    except Exception:\n",
        "        text = \"[vLLM Qwen2-Audio error: no output]\"\n",
        "    print(\"[vLLM] Qwen2-Audio output: {}\".format(text))\n",
        "    return text\n",
        "\n",
        "def run_ultravox(question: str, audio_count: int) -> str:\n",
        "    print(\"[vLLM] Running Ultravox model...\")\n",
        "    model_name = \"fixie-ai/ultravox-v0_5-llama-3_2-1b\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    messages = [{'role': 'user', 'content': (\"<|audio|>\\n\" * audio_count) + question}]\n",
        "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_name,\n",
        "        max_model_len=4096,\n",
        "        max_num_seqs=5,\n",
        "        trust_remote_code=True,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    llm = LLM(**engine_args.__dict__)\n",
        "    sampling_params = SamplingParams(temperature=0.2, max_tokens=128)\n",
        "    inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"audio\": []}}\n",
        "    outputs = llm.generate([inputs], sampling_params=sampling_params)\n",
        "    try:\n",
        "        text = outputs[0].outputs[0].text\n",
        "    except Exception:\n",
        "        text = \"[vLLM Ultravox error: no output]\"\n",
        "    print(\"[vLLM] Ultravox output: {}\".format(text))\n",
        "    return text\n",
        "\n",
        "def run_whisper(question: str, audio_count: int) -> str:\n",
        "    print(\"[vLLM] Running Whisper model...\")\n",
        "    if audio_count != 1:\n",
        "        return \"Whisper supports only a single audio input.\"\n",
        "    model_name = \"openai/whisper-large-v3-turbo\"\n",
        "    prompt = \"<|startoftranscript|>\"\n",
        "    engine_args = EngineArgs(\n",
        "        model=model_name,\n",
        "        max_model_len=448,\n",
        "        max_num_seqs=5,\n",
        "        limit_mm_per_prompt={\"audio\": audio_count},\n",
        "    )\n",
        "    llm = LLM(**engine_args.__dict__)\n",
        "    sampling_params = SamplingParams(temperature=0.2, max_tokens=128)\n",
        "    inputs = {\"prompt\": prompt, \"multi_modal_data\": {\"audio\": []}}\n",
        "    outputs = llm.generate([inputs], sampling_params=sampling_params)\n",
        "    try:\n",
        "        text = outputs[0].outputs[0].text\n",
        "    except Exception:\n",
        "        text = \"[vLLM Whisper error: no output]\"\n",
        "    print(\"[vLLM] Whisper output: {}\".format(text))\n",
        "    return text\n",
        "\n",
        "def get_vllm_audio_model_configs() -> Dict[str, Any]:\n",
        "    return {\n",
        "        \"minicpmo\": run_minicpmo,\n",
        "        \"phi4_mm\": run_phi4_mm,\n",
        "        \"qwen2_audio\": run_qwen2_audio,\n",
        "        \"ultravox\": run_ultravox,\n",
        "        \"whisper\": run_whisper,\n",
        "    }"
      ],
      "metadata": {
        "id": "12joAWZGscLe"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Groq LLM Integration using Real API Calls\n",
        "#############################################\n",
        "def run_groq_model1(question: str, audio_count: int) -> str:\n",
        "    print(\"[Groq] Running Groq Model 1...\")\n",
        "    url = \"https://api.groq.ai/v1/inference/model1\"\n",
        "    payload = {\"question\": question, \"audio_count\": audio_count}\n",
        "    headers = {\"Authorization\": f\"Bearer {os.environ.get('GROQ_API_KEY', '')}\"}\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json().get(\"text\", \"\")\n",
        "    else:\n",
        "        result = f\"Groq model1 error: {response.status_code}\"\n",
        "    print(\"[Groq] Groq Model 1 output: {}\".format(result))\n",
        "    return result\n",
        "\n",
        "def run_groq_model2(question: str, audio_count: int) -> str:\n",
        "    print(\"[Groq] Running Groq Model 2...\")\n",
        "    url = \"https://api.groq.ai/v1/inference/model2\"\n",
        "    payload = {\"question\": question, \"audio_count\": audio_count}\n",
        "    headers = {\"Authorization\": f\"Bearer {os.environ.get('GROQ_API_KEY', '')}\"}\n",
        "    response = requests.post(url, json=payload, headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        result = response.json().get(\"text\", \"\")\n",
        "    else:\n",
        "        result = f\"Groq model2 error: {response.status_code}\"\n",
        "    print(\"[Groq] Groq Model 2 output: {}\".format(result))\n",
        "    return result\n",
        "\n",
        "def get_groq_audio_model_configs() -> Dict[str, Any]:\n",
        "    return {\n",
        "        \"groq_model1\": run_groq_model1,\n",
        "        \"groq_model2\": run_groq_model2,\n",
        "    }\n",
        "\n",
        "def run_vllm_inference(audio_path: str, question: str) -> Dict[str, str]:\n",
        "    print(\"[Step 14] Running multimodal integration (vLLM & Groq)...\")\n",
        "    results = {}\n",
        "    for model_name, model_fn in get_vllm_audio_model_configs().items():\n",
        "        try:\n",
        "            result = model_fn(question, audio_count=1)\n",
        "            results[model_name] = result\n",
        "        except Exception as e:\n",
        "            results[model_name] = f\"vLLM error: {e}\"\n",
        "    for model_name, model_fn in get_groq_audio_model_configs().items():\n",
        "        try:\n",
        "            result = model_fn(question, audio_count=1)\n",
        "            results[model_name] = result\n",
        "        except Exception as e:\n",
        "            results[model_name] = f\"Groq error: {e}\"\n",
        "    print(\"[Step 14] Multimodal integration completed.\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "qkJICyJcsoGA"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# LangGraph Integration for Report Aggregation\n",
        "#############################################\n",
        "def langgraph_forensic_report(report_data: Dict[str, Any]) -> str:\n",
        "    print(\"[Step 15] Running LangGraph aggregation...\")\n",
        "    graph = StateGraph(\"forensic_analysis\")\n",
        "    graph.add_node(\"evidence_aggregation\", lambda data: f\"Anomalies: {data['all_anomalies']}\\nFeatures: {data['features']}\")\n",
        "    graph.add_node(\"analysis\", lambda data: langchain_llm_report(data))\n",
        "    graph.add_node(\"evidence_table\", lambda data: f\"Model Scores: {data['all_model_scores']}\")\n",
        "    def combine(data):\n",
        "        return f\"{data['evidence_aggregation']}\\n\\n{data['analysis']}\\n\\n{data['evidence_table']}\"\n",
        "    graph.add_node(\"final_report\", combine, dependencies=[\"evidence_aggregation\", \"analysis\", \"evidence_table\"])\n",
        "    graph.add_edge(\"start\", \"evidence_aggregation\")\n",
        "    graph.add_edge(\"start\", \"analysis\")\n",
        "    graph.add_edge(\"start\", \"evidence_table\")\n",
        "    graph.add_edge(\"evidence_aggregation\", \"final_report\")\n",
        "    graph.add_edge(\"analysis\", \"final_report\")\n",
        "    graph.add_edge(\"evidence_table\", \"final_report\")\n",
        "    result = graph.run(report_data)\n",
        "    final_report = result.get(END, \"LangGraph aggregation failed.\")\n",
        "    print(\"[Step 15] LangGraph aggregation completed.\")\n",
        "    return final_report"
      ],
      "metadata": {
        "id": "USnJyB2dss1b"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Concurrency Helper to run blocking functions\n",
        "#############################################\n",
        "async def async_run_in_executor(func, *args):\n",
        "    loop = asyncio.get_event_loop()\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        return await loop.run_in_executor(executor, func, *args)\n",
        "\n",
        "async def gather_agents(audio: np.ndarray, sr: int, audio_path: str) -> Dict[str, Any]:\n",
        "    print(\"[Step 16] Concurrently running forensic agent functions...\")\n",
        "    tasks = {\n",
        "        \"wave2vec\": async_run_in_executor(run_wave2vec_fake_detection, audio_path),\n",
        "        \"replay\": async_run_in_executor(run_replay_attack_detection, audio, sr),\n",
        "        \"emotion\": async_run_in_executor(run_enhanced_emotion_detection, audio, sr),\n",
        "        \"speechbrain_spoof\": async_run_in_executor(run_speechbrain_spoof, audio_path)\n",
        "    }\n",
        "    results = {}\n",
        "    for key, task in tasks.items():\n",
        "        results[key] = await task\n",
        "    print(\"[Step 16] Forensic agent functions completed.\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "ANqzZWW6sweb"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# LangChain Forensic Report Generation\n",
        "#############################################\n",
        "def langchain_llm_report(report_data: Dict[str, Any]) -> str:\n",
        "    print(\"[Step 17] Generating forensic report summary via LangChain...\")\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"verdict\", \"mean_score\", \"anomalies\", \"asr\", \"asr_lang\",\n",
        "                           \"speakers\", \"spoof\", \"features\", \"noise_quality\", \"gender_dist\", \"extra\", \"vllm_outputs\"],\n",
        "        template=(\n",
        "            \"You are a digital audio forensics expert. Provide a detailed explanation of every analysis step below.\\n\\n\"\n",
        "            \"Verdict: {verdict}\\n\"\n",
        "            \"Mean Deepfake Score: {mean_score:.2f}\\n\"\n",
        "            \"Detected Anomalies: {anomalies}\\n\"\n",
        "            \"ASR Transcript: {asr}\\n\"\n",
        "            \"ASR Language: {asr_lang}\\n\"\n",
        "            \"Speaker Identities: {speakers}\\n\"\n",
        "            \"Spoof Score: {spoof}\\n\"\n",
        "            \"Feature Summary: {features}\\n\"\n",
        "            \"Noise/Quality Score: {noise_quality}\\n\"\n",
        "            \"Gender Distribution: {gender_dist}\\n\"\n",
        "            \"Additional Analysis: {extra}\\n\"\n",
        "            \"vLLM Audio Model Outputs: {vllm_outputs}\\n\\n\"\n",
        "            \"Provide a risk assessment with actionable recommendations.\"\n",
        "        )\n",
        "    )\n",
        "    llm = OpenAI(temperature=0.2, max_tokens=700)\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    summary = chain.run(\n",
        "        verdict=report_data['verdict'],\n",
        "        mean_score=report_data['mean_score'],\n",
        "        anomalies=\", \".join(report_data['all_anomalies']),\n",
        "        asr=report_data['asr_transcript'][:400] + \"...\" if report_data['asr_transcript'] else \"N/A\",\n",
        "        asr_lang=report_data.get('asr_lang', 'unknown'),\n",
        "        speakers=\", \".join(report_data['speaker_identities']),\n",
        "        spoof=str(report_data['speaker_spoof_score']),\n",
        "        features=\"; \".join(f\"{k}: {v:.3f}\" for k, v in report_data['features'].items()),\n",
        "        noise_quality=str(report_data.get('noise_quality_score', 'N/A')),\n",
        "        gender_dist=json.dumps(report_data.get('gender_distribution', {})),\n",
        "        extra=json.dumps(report_data.get('extra_info', {})),\n",
        "        vllm_outputs=json.dumps(report_data.get('vllm_outputs', {}))\n",
        "    )\n",
        "    print(\"[Step 17] LangChain summary generated.\")\n",
        "    return summary"
      ],
      "metadata": {
        "id": "PWrUCmrks1Bw"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Aggregation & Final Report Generation\n",
        "#############################################\n",
        "def aggregate_and_report(audio_path: str, results: Dict[str, Any],\n",
        "                         feats: Dict[str, float], asr: str,\n",
        "                         asr_lang: str, speaker_identities: List[str],\n",
        "                         spk_score: float, noise_quality_score: float,\n",
        "                         gender_dist: Dict[str, float],\n",
        "                         vllm_model_outputs: Dict[str, str],\n",
        "                         extra_results: Dict[str, Any] = {}) -> ForensicReport:\n",
        "    print(\"[Step 18] Aggregating all forensic analysis results...\")\n",
        "    scores = []\n",
        "    all_anomalies = []\n",
        "    model_scores = {}\n",
        "    detailed = {}\n",
        "    for agent, (score, anomalies, detail) in results.items():\n",
        "        scores.append(score)\n",
        "        model_scores[agent] = score\n",
        "        all_anomalies.extend(anomalies)\n",
        "        detailed[agent] = {\"score\": score, \"anomalies\": anomalies, \"detail\": detail}\n",
        "    speakers, _ = run_speechbrain_speaker(audio_path)\n",
        "    spk_verif_score = run_speechbrain_verification(audio_path)\n",
        "    spoof_score, _ = results.get(\"speechbrain_spoof\", (0.0, []))\n",
        "    combined_scores = scores + [spk_verif_score]\n",
        "    mean_score = float(np.mean(combined_scores))\n",
        "    confidence = 1.0 - float(np.std(combined_scores))\n",
        "    verdict = (\"Likely FAKE (spoof/scam/deepfake detected)\" if mean_score > 0.7\n",
        "               else \"Possibly FAKE (review anomalies)\" if mean_score > 0.5\n",
        "               else \"Likely REAL\")\n",
        "    extra_info = {\n",
        "        \"speaker_diarization\": speaker_identities if speaker_identities else speakers,\n",
        "        \"augmentation_tests\": run_augmentation_tests(librosa.util.normalize(feats.get('energy')), 16000)\n",
        "    }\n",
        "    try:\n",
        "        llm_summary = langchain_llm_report({\n",
        "            \"verdict\": verdict,\n",
        "            \"mean_score\": mean_score,\n",
        "            \"all_anomalies\": list(set(all_anomalies)),\n",
        "            \"asr_transcript\": asr,\n",
        "            \"asr_lang\": asr_lang,\n",
        "            \"speaker_identities\": speaker_identities if speaker_identities else speakers,\n",
        "            \"speaker_spoof_score\": spoof_score,\n",
        "            \"features\": feats,\n",
        "            \"noise_quality_score\": noise_quality_score,\n",
        "            \"gender_distribution\": gender_dist,\n",
        "            \"extra_info\": extra_info,\n",
        "            \"vllm_outputs\": vllm_model_outputs\n",
        "        })\n",
        "        lg_summary = langgraph_forensic_report({\n",
        "            \"verdict\": verdict,\n",
        "            \"mean_score\": mean_score,\n",
        "            \"all_anomalies\": list(set(all_anomalies)),\n",
        "            \"asr_transcript\": asr,\n",
        "            \"asr_lang\": asr_lang,\n",
        "            \"speaker_identities\": speaker_identities if speaker_identities else speakers,\n",
        "            \"speaker_spoof_score\": spoof_score,\n",
        "            \"features\": feats,\n",
        "            \"noise_quality_score\": noise_quality_score,\n",
        "            \"gender_distribution\": gender_dist,\n",
        "            \"extra_info\": extra_info,\n",
        "            \"vllm_outputs\": vllm_model_outputs,\n",
        "            \"all_model_scores\": model_scores\n",
        "        })\n",
        "        natural_summary = f\"{llm_summary}\\n\\nLangGraph Analysis:\\n{lg_summary}\"\n",
        "    except Exception as e:\n",
        "        natural_summary = f\"Verdict: {verdict} (Error generating summary: {e})\"\n",
        "    detailed.update(extra_results)\n",
        "    print(\"[Step 18] Aggregation complete. Report ready.\")\n",
        "    return ForensicReport(\n",
        "        file=audio_path,\n",
        "        verdict=verdict,\n",
        "        mean_score=mean_score,\n",
        "        confidence=confidence,\n",
        "        all_model_scores=model_scores,\n",
        "        all_anomalies=list(set(all_anomalies)),\n",
        "        natural_summary=natural_summary,\n",
        "        asr_transcript=asr,\n",
        "        asr_lang=asr_lang,\n",
        "        speaker_identities=speaker_identities if speaker_identities else speakers,\n",
        "        speaker_spoof_score=spoof_score,\n",
        "        noise_quality_score=noise_quality_score,\n",
        "        gender_distribution=gender_dist,\n",
        "        detailed_results=detailed,\n",
        "        timestamp=datetime.utcnow().isoformat(),\n",
        "        extra_info=extra_info,\n",
        "        vllm_model_outputs=vllm_model_outputs\n",
        "    )"
      ],
      "metadata": {
        "id": "TUwm5fsos4pk"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Main Pipeline Execution Function (Async)\n",
        "#############################################\n",
        "async def deepfake_defensive_pipeline(audio_path: str, vllm_question: str = None) -> ForensicReport:\n",
        "    print(\"=== Deepfake Audio Forensic Analysis Pipeline Initiated ===\")\n",
        "    if vllm_question is None:\n",
        "        vllm_question = (\n",
        "            \"Analyze this audio for potential deepfake, spoofing, scam, or synthetic voice indicators. \"\n",
        "            \"Extract evidence including speaker details, replay attacks, spoof cues, emotion, ASR, \"\n",
        "            \"spectral features, and augmentation artifacts. Provide a risk score (0 to 1), a detailed evidence table, and actionable recommendations.\"\n",
        "        )\n",
        "    audio, sr = await async_run_in_executor(preprocess_audio, audio_path)\n",
        "    feats = extract_features(audio, sr)\n",
        "    feats['vad_ratio'] = extract_vad_ratio(audio, sr)\n",
        "    _, _, gender_dist = extract_gender_distribution(audio_path)\n",
        "    feats['nb_segments'] = 0  # Update if segmentation implemented.\n",
        "    langid_label, _ = run_language_id(audio_path)\n",
        "    feats['langid_label'] = langid_label\n",
        "    agent_results = await gather_agents(audio, sr, audio_path)\n",
        "    asr_transcript = run_asr_transcription(audio_path)\n",
        "    asr_lang = langid_label\n",
        "    vllm_outputs = run_vllm_inference(audio_path, vllm_question)\n",
        "    report = aggregate_and_report(\n",
        "        audio_path,\n",
        "        agent_results,\n",
        "        feats,\n",
        "        asr_transcript,\n",
        "        asr_lang,\n",
        "        speaker_identities=[],\n",
        "        spk_score=run_speechbrain_verification(audio_path),\n",
        "        noise_quality_score=nr.reduce_noise(y=audio, sr=sr).std(),\n",
        "        gender_dist=gender_dist,\n",
        "        vllm_model_outputs=vllm_outputs,\n",
        "        extra_results={}\n",
        "    )\n",
        "    print(\"=== Pipeline Completed ===\")\n",
        "    return report"
      ],
      "metadata": {
        "id": "I_EwD7jNs9oU"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Mermaid.js Flowchart for Pipeline Visualization\n",
        "#############################################\n",
        "def generate_mermaid_flowchart() -> str:\n",
        "    mermaid_code = \"\"\"\n",
        "    %% Mermaid Flowchart for Deepfake Audio Forensic Pipeline\n",
        "    graph TD\n",
        "      A[Upload Audio File] -->|File size info| B[Preprocess Audio]\n",
        "      B --> C[Extract Audio Features]\n",
        "      C --> D[Compute VAD Ratio]\n",
        "      D --> E[Estimate Gender Distribution]\n",
        "      E --> F[Speaker Diarization & Verification]\n",
        "      F --> G[ASR Transcription]\n",
        "      G --> H[vLLM & Groq Multimodal Analysis]\n",
        "      H --> I[Aggregate Forensic Evidence]\n",
        "      I --> J[Generate Forensic Report]\n",
        "    \"\"\"\n",
        "    return mermaid_code\n",
        "\n",
        "def display_mermaid_chart(mermaid_code: str):\n",
        "    html_content = f\"\"\"\n",
        "    <html>\n",
        "    <head>\n",
        "      <script type=\"module\">\n",
        "        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.js';\n",
        "        mermaid.initialize({{startOnLoad:true, theme:'default'}});\n",
        "      </script>\n",
        "    </head>\n",
        "    <body>\n",
        "      <div class=\"mermaid\">\n",
        "      {mermaid_code}\n",
        "      </div>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    display(HTML(html_content))"
      ],
      "metadata": {
        "id": "iPLeZKnOtBnS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# UI for Google Colab with Enhanced Features\n",
        "#############################################\n",
        "def display_audio_file(audio_path: str, sr: int = 16000):\n",
        "    audio, _ = librosa.load(audio_path, sr=sr)\n",
        "    plt.figure(figsize=(14, 3))\n",
        "    librosa.display.waveshow(audio, sr=sr)\n",
        "    plt.title('Audio Waveform')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    ipd.display(ipd.Audio(audio, rate=sr))\n",
        "\n",
        "def run_pipeline_ui():\n",
        "    upload_widget = widgets.FileUpload(accept=\".wav,.mp3,.flac,.m4a\", multiple=False)\n",
        "    size_label = widgets.Label(value=\"File Size: Not Uploaded\")\n",
        "    progress_bar = widgets.IntProgress(value=0, min=0, max=100, description='Upload:', bar_style='info')\n",
        "    status_label = widgets.Label(value=\"Status: Waiting for file upload...\")\n",
        "    run_button = widgets.Button(description=\"Run Deepfake Detection\", button_style='success')\n",
        "\n",
        "    # Tabs for displaying Results, Logs and Raw JSON Report\n",
        "    tab_children = [widgets.Output(), widgets.Output(), widgets.Output()]\n",
        "    tab = widgets.Tab(children=tab_children)\n",
        "    tab.set_title(0, \"Summary\")\n",
        "    tab.set_title(1, \"Logs\")\n",
        "    tab.set_title(2, \"Raw JSON\")\n",
        "\n",
        "    log_area = widgets.Output()\n",
        "\n",
        "    def append_log(message: str):\n",
        "        with log_area:\n",
        "            print(message)\n",
        "\n",
        "    def on_file_upload(change):\n",
        "        if upload_widget.value:\n",
        "            progress_bar.value = 100\n",
        "            uploaded_filename = list(upload_widget.value.keys())[0]\n",
        "            file_size = len(upload_widget.value[uploaded_filename]['content'])\n",
        "            size_label.value = f\"File Size: {file_size} bytes (Upload Complete)\"\n",
        "            status_label.value = \"Status: File uploaded. Ready to run detection.\"\n",
        "            append_log(f\"File uploaded: {uploaded_filename} ({file_size} bytes)\")\n",
        "\n",
        "    upload_widget.observe(on_file_upload, names='value')\n",
        "\n",
        "    def on_run_clicked(change):\n",
        "        with tab_children[1]:\n",
        "            clear_output()\n",
        "        with tab_children[0]:\n",
        "            clear_output()\n",
        "        with tab_children[2]:\n",
        "            clear_output()\n",
        "        with log_area:\n",
        "            clear_output()\n",
        "        if not upload_widget.value:\n",
        "            status_label.value = \"Status: Please upload an audio file.\"\n",
        "            return\n",
        "\n",
        "        status_label.value = \"Status: Saving file...\"\n",
        "        uploaded_filename = list(upload_widget.value.keys())[0]\n",
        "        content = upload_widget.value[uploaded_filename]['content']\n",
        "        local_filename = \"uploaded_audio\" + os.path.splitext(uploaded_filename)[1]\n",
        "        with open(local_filename, \"wb\") as f:\n",
        "            f.write(content)\n",
        "        append_log(f\"Audio file '{uploaded_filename}' saved as '{local_filename}'.\")\n",
        "        # Display waveform in logs tab\n",
        "        with tab_children[1]:\n",
        "            print(\"Displaying Audio Waveform:\")\n",
        "            display_audio_file(local_filename)\n",
        "        # Display pipeline flowchart in logs tab\n",
        "        with tab_children[1]:\n",
        "            print(\"\\nVisualizing Pipeline Flowchart:\")\n",
        "            mermaid_code = generate_mermaid_flowchart()\n",
        "            display_mermaid_chart(mermaid_code)\n",
        "\n",
        "        status_label.value = \"Status: Processing audio...\"\n",
        "        append_log(\"Starting Deepfake Detection Pipeline...\")\n",
        "        try:\n",
        "            try:\n",
        "                loop = asyncio.get_event_loop()\n",
        "            except RuntimeError:\n",
        "                loop = asyncio.new_event_loop()\n",
        "                asyncio.set_event_loop(loop)\n",
        "            report = loop.run_until_complete(deepfake_defensive_pipeline(local_filename))\n",
        "            status_label.value = \"Status: Processing complete.\"\n",
        "            append_log(\"Pipeline processing completed.\")\n",
        "            # Show summary in Summary tab\n",
        "            with tab_children[0]:\n",
        "                print(\"=== Forensic Report Summary ===\\n\")\n",
        "                print(report.natural_summary)\n",
        "            # Show raw JSON in Raw JSON tab\n",
        "            with tab_children[2]:\n",
        "                print(\"=== Complete JSON Report ===\\n\")\n",
        "                print(report.json(indent=2))\n",
        "        except Exception as e:\n",
        "            status_label.value = f\"Status: Error during processing: {e}\"\n",
        "            append_log(f\"Error during processing: {e}\")\n",
        "\n",
        "    run_button.on_click(on_run_clicked)\n",
        "    ui = widgets.VBox([\n",
        "        widgets.HBox([upload_widget, size_label, progress_bar]),\n",
        "        status_label,\n",
        "        run_button,\n",
        "        tab,\n",
        "        widgets.Label(\"Pipeline Logs:\"),\n",
        "        log_area\n",
        "    ])\n",
        "    display(ui)\n",
        "\n",
        "# Launch the enhanced UI when this cell is executed in Colab\n",
        "run_pipeline_ui()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206,
          "referenced_widgets": [
            "41ddeb180c9d4cb59918348017471193",
            "943dfc2d890741f0a931cf518c5a1667",
            "d90cf447a1d44ab6ab0de05a324bff0a",
            "be3b01019da749a6a780f68cf6197a00",
            "f73e510193a34c9991b6bd1b8facd744",
            "6fd4e970927c4f4f862a3fcae2810bc9",
            "310ae85a5b124e97a3b6a6caf72e3936",
            "a170a0ebb5e5467ca62022928c163378",
            "195480593d664d01aaf07ad1a944d8c5",
            "0b421cb2609847caadf69a018cf6e63c",
            "46e3b6c0ed694647a2f7b81c891cb534",
            "96b87e31f365423bbaf5154dd0d66efd",
            "a3ba8e7f37b240929347f6cfc2fde589",
            "a867a68869f0423191959ab02684c1f2",
            "636f9d14597847c08bb039659a610adc",
            "d335bf11605f45ae964d97d912072e1c",
            "33b2dcf026fb4856b914e077878c633a",
            "132ff040874c4def9e4a5bd56b607fc2",
            "9184c45b0b78450da1beb3e6bb198452",
            "b76639c08e4b4833a065b217c86d78d0",
            "2da31718c7e2488fa2d404df22afc248",
            "6558621b719642b6a1df24c0cfaa254b",
            "7016580fba0741c4a6e3eeb4f6f1c6e8",
            "97cb97624496420e945501176fc78ada",
            "5445c20eba2d44919e33affe3de82e63",
            "4a56e9406f134758ae422c08bce5dde4",
            "c87e7d90e95e4594b7ff5878b87d26aa",
            "65a7b0e4c8ca41e9b3b3218543dcf112",
            "dd92ef3ec72649e29b302082c303b380",
            "896728ac5a8245a4a65519ecff94cef3",
            "ccca3934c6d744c787f2fe531f81b589",
            "fe2056b38d044274a070d1358cb9a743"
          ]
        },
        "id": "lKz-bUz0tGLM",
        "outputId": "639ec9aa-df0c-4672-a072-68c4ee686e7d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HBox(children=(FileUpload(value={}, accept='.wav,.mp3,.flac,.m4a', description='Upload'), Label…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41ddeb180c9d4cb59918348017471193"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KtcxZk0RtMvq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}