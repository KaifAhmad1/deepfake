{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOU51cgdFOzfL4tTR01z1RM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/deepfake/blob/main/Deepfake_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X1MVlzWHePOl"
      },
      "outputs": [],
      "source": [
        "%pip install -q torch opencv-python librosa numpy face-recognition\n",
        "%pip install -q vllm transformers mediapipe scipy pillow tqdm pydantic moviepy langchain_community langgraph dtw-python\n",
        "%pip install -q ipywidgets nest_asyncio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import librosa\n",
        "import asyncio\n",
        "import json\n",
        "import re\n",
        "import gc\n",
        "from datetime import datetime\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import Dict, List, Any, Tuple, Optional\n",
        "\n",
        "import mediapipe as mp\n",
        "from pydantic import BaseModel, Field, PrivateAttr\n",
        "\n",
        "# Imports for LLMs and chain operations\n",
        "from langchain_community.llms import VLLM, VLLMOpenAI\n",
        "from langchain_core.language_models.llms import LLM\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Additional imports for image and video quality metrics\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "# For lip-sync DTW computation\n",
        "from dtw import dtw\n",
        "\n",
        "# For face recognition\n",
        "import face_recognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6-iAOCtenf9",
        "outputId": "ceab1580-5dbf-4584-adcf-8236f84b01f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing the dtw module. When using in academic works please cite:\n",
            "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
            "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set CUDA environment variables and clear GPU memory\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "_Wd4s6osg-3t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common parameters for model initialization\n",
        "COMMON_PARAMS = {\n",
        "    \"task\": \"generate\",\n",
        "    \"max_model_len\": 4096,\n",
        "    \"dtype\": \"half\",\n",
        "    \"gpu_memory_utilization\": 0.85,\n",
        "    \"cpu_offload_gb\": 8,\n",
        "    \"enforce_eager\": True,\n",
        "    \"trust_remote_code\": True\n",
        "}\n",
        "\n",
        "def init_vllm_model(name: str, model_id: str, **overrides):\n",
        "    params = {**COMMON_PARAMS, **overrides}\n",
        "    print(f\"[DEBUG] Initializing model '{name}' with id '{model_id}' with params: {params}\")\n",
        "    return {\"name\": name, \"model_id\": model_id, \"params\": params}\n",
        "\n",
        "def init_groq_model(name: str, model_id: str):\n",
        "    api_key = os.environ.get(\"GROQ_API_KEY\", \"your_groq_api_key\")\n",
        "    print(f\"[DEBUG] Loading model '{name}' with id '{model_id}' using API key.\")\n",
        "    return {\"name\": name, \"model_id\": model_id, \"api_key\": api_key}\n",
        "\n",
        "# A simple Groq LLM class that simulates the Groq native SDK response.\n",
        "class GroqLLM:\n",
        "    def __init__(self, model_data):\n",
        "        self.model_data = model_data\n",
        "    def call_as_llm(self, prompt: str) -> str:\n",
        "        print(f\"[DEBUG] GroqLLM called with prompt: {prompt}\")\n",
        "        # Simulated response; replace with actual Groq SDK call if needed.\n",
        "        return \"Score: 0.75\\nAnomalies: []\"\n",
        "\n",
        "# GroqLLMWrapper adapts GroqLLM to the LangChain LLM interface.\n",
        "class GroqLLMWrapper(LLM):\n",
        "    _groq_llm: GroqLLM = PrivateAttr()\n",
        "\n",
        "    def __init__(self, groq_llm: GroqLLM, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self._groq_llm = groq_llm\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"groq_llm\"\n",
        "\n",
        "    def _call(self, prompt: str, stop=None) -> str:\n",
        "        return self._groq_llm.call_as_llm(prompt)\n",
        "\n",
        "print(\"[DEBUG] Initializing models...\")\n",
        "models = {\n",
        "    \"video\": [\n",
        "        init_vllm_model(\"llava_next_video\", \"llava-hf/LLaVA-NeXT-Video-7B-hf\", tensor_parallel_size=2, max_tokens=1024),\n",
        "        init_vllm_model(\"videomae\", \"MCG-NJU/videomae-large-static\", tensor_parallel_size=2),\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\"))),\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\")))\n",
        "    ],\n",
        "    \"audio\": [\n",
        "        init_vllm_model(\"wav2vec2\", \"facebook/wav2vec2-large-robust-ft-swbd-300h\", tensor_parallel_size=1),\n",
        "        init_vllm_model(\"whisper\", \"openai/whisper-large-v3\", tensor_parallel_size=2),\n",
        "        init_groq_model(\"groq_audio_model\", \"whisper-large-v3-turbo\"),\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\"))),\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\")))\n",
        "    ],\n",
        "    \"image\": [\n",
        "        init_vllm_model(\"llava_image\", \"llava-hf/llava-onevision-qwen2-7b-ov-hf\", tensor_parallel_size=2),\n",
        "        init_vllm_model(\"clip\", \"openai/clip-vit-large-patch14\", tensor_parallel_size=1),\n",
        "        init_groq_model(\"groq_vision_model\", \"llama-3.2-90b-vision-preview\"),\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\"))),\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\")))\n",
        "    ],\n",
        "    \"text\": [\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_text_model\", \"llama-3.3-70b-versatile\"))),\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_scout\", \"meta-llama/llama-4-scout-17b-16e-instruct\"))),\n",
        "        GroqLLMWrapper(GroqLLM(init_groq_model(\"groq_llama_maverick\", \"meta-llama/llama-4-maverick-17b-128e-instruct\")))\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "opWQ8x6uhWpr",
        "outputId": "dcef628a-8437-47de-f13a-bac5a06c5330",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Initializing models...\n",
            "[DEBUG] Initializing model 'llava_next_video' with id 'llava-hf/LLaVA-NeXT-Video-7B-hf' with params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2, 'max_tokens': 1024}\n",
            "[DEBUG] Initializing model 'videomae' with id 'MCG-NJU/videomae-large-static' with params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2}\n",
            "[DEBUG] Loading model 'groq_llama_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "[DEBUG] Loading model 'groq_llama_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n",
            "[DEBUG] Initializing model 'wav2vec2' with id 'facebook/wav2vec2-large-robust-ft-swbd-300h' with params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 1}\n",
            "[DEBUG] Initializing model 'whisper' with id 'openai/whisper-large-v3' with params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2}\n",
            "[DEBUG] Loading model 'groq_audio_model' with id 'whisper-large-v3-turbo' using API key.\n",
            "[DEBUG] Loading model 'groq_llama_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "[DEBUG] Loading model 'groq_llama_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n",
            "[DEBUG] Initializing model 'llava_image' with id 'llava-hf/llava-onevision-qwen2-7b-ov-hf' with params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 2}\n",
            "[DEBUG] Initializing model 'clip' with id 'openai/clip-vit-large-patch14' with params: {'task': 'generate', 'max_model_len': 4096, 'dtype': 'half', 'gpu_memory_utilization': 0.85, 'cpu_offload_gb': 8, 'enforce_eager': True, 'trust_remote_code': True, 'tensor_parallel_size': 1}\n",
            "[DEBUG] Loading model 'groq_vision_model' with id 'llama-3.2-90b-vision-preview' using API key.\n",
            "[DEBUG] Loading model 'groq_llama_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "[DEBUG] Loading model 'groq_llama_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n",
            "[DEBUG] Loading model 'groq_text_model' with id 'llama-3.3-70b-versatile' using API key.\n",
            "[DEBUG] Loading model 'groq_llama_scout' with id 'meta-llama/llama-4-scout-17b-16e-instruct' using API key.\n",
            "[DEBUG] Loading model 'groq_llama_maverick' with id 'meta-llama/llama-4-maverick-17b-128e-instruct' using API key.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KLBa7NkFhdZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}