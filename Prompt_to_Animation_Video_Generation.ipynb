{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOUKK1fWoXv2Lcw32HnXEe/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bcce1c05d29c477db9dc7391f7ae5675": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55c9fd00c5a04244802c132c8a107575",
              "IPY_MODEL_315247a1faec44629d1bb2af8d0043d5",
              "IPY_MODEL_041e9bbe6b5441cc88327555842795ab"
            ],
            "layout": "IPY_MODEL_6dc2427f55c5411787b1278176226fb1"
          }
        },
        "55c9fd00c5a04244802c132c8a107575": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c99adfc3a194f769d2888f7dd9c58a8",
            "placeholder": "​",
            "style": "IPY_MODEL_820599ea5f04443eb1b58534459ca0ae",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "315247a1faec44629d1bb2af8d0043d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b75f099db45498e88143ae99965a572",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_86e4f95ed6a24c3180495c53c12d339f",
            "value": 4
          }
        },
        "041e9bbe6b5441cc88327555842795ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daa824b871f742a5ab63e9ba0ba2ae73",
            "placeholder": "​",
            "style": "IPY_MODEL_38a4996ea35d4ec1b317dd84b0fbb141",
            "value": " 4/4 [00:00&lt;00:00,  3.99it/s]"
          }
        },
        "6dc2427f55c5411787b1278176226fb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c99adfc3a194f769d2888f7dd9c58a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "820599ea5f04443eb1b58534459ca0ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b75f099db45498e88143ae99965a572": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86e4f95ed6a24c3180495c53c12d339f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "daa824b871f742a5ab63e9ba0ba2ae73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38a4996ea35d4ec1b317dd84b0fbb141": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/deepfake/blob/main/Prompt_to_Animation_Video_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DM-MELvYKa3U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36cd5c22-94f6-4682-90ad-faa06ee66235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement bistandbytes (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for bistandbytes\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU diffusers transformers accelerate torch\n",
        "!pip install -qU opencv-python moviepy\n",
        "!pip install -U bistandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from diffusers import (\n",
        "    DiffusionPipeline,\n",
        "    LTXPipeline,\n",
        "    BitsAndBytesConfig,\n",
        "    HunyuanVideoTransformer3DModel,\n",
        "    HunyuanVideoPipeline,\n",
        ")\n",
        "from diffusers.utils import export_to_video\n",
        "from diffusers.hooks import apply_layerwise_casting\n",
        "from transformers import LlamaModel"
      ],
      "metadata": {
        "id": "3NFumQqUcSlu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_video_generic(model_id, prompt, output_path, num_frames, width, height, num_inference_steps, fps):\n",
        "    try:\n",
        "        print(f\"Loading pipeline for {model_id} ...\")\n",
        "        pipe = DiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "        pipe = pipe.to(\"cuda\")\n",
        "        # Enable CPU offloading to conserve GPU memory if needed.\n",
        "        pipe.enable_model_cpu_offload()\n",
        "        print(f\"Generating video for prompt:\\n{prompt}\")\n",
        "        result = pipe(prompt=prompt, num_frames=num_frames, width=width, height=height, num_inference_steps=num_inference_steps)\n",
        "        video_frames = result.frames[0]\n",
        "        export_to_video(video_frames, output_path, fps=fps)\n",
        "        print(f\"Video generated and saved at {output_path}\")\n",
        "    except Exception as ex:\n",
        "        print(f\"Error generating video with {model_id}: {ex}\")\n",
        "\n",
        "def generate_video_ltx(prompt, negative_prompt, output_path):\n",
        "    print(\"Using LTX-Video pipeline...\")\n",
        "    try:\n",
        "        pipe = LTXPipeline.from_pretrained(\"Lightricks/LTX-Video\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
        "        print(f\"Generating video for prompt:\\n{prompt}\\nwith negative prompt:\\n{negative_prompt}\")\n",
        "        result = pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            width=704,\n",
        "            height=480,\n",
        "            num_frames=161,\n",
        "            num_inference_steps=50,\n",
        "        )\n",
        "        video_frames = result.frames[0]\n",
        "        export_to_video(video_frames, output_path, fps=24)\n",
        "        print(f\"LTX-Video generated video saved at {output_path}\")\n",
        "    except Exception as ex:\n",
        "        print(f\"Error in LTX-Video generation: {ex}\")\n",
        "\n",
        "def generate_video_hunyuan(prompt, output_path):\n",
        "    print(\"Using HunyuanVideo pipeline with advanced optimizations...\")\n",
        "    try:\n",
        "        model_id = \"hunyuanvideo-community/HunyuanVideo\"\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "        text_encoder = LlamaModel.from_pretrained(model_id, subfolder=\"text_encoder\", torch_dtype=torch.float16)\n",
        "        apply_layerwise_casting(text_encoder, storage_dtype=torch.float8_e4m3fn, compute_dtype=torch.float16)\n",
        "        transformer = HunyuanVideoTransformer3DModel.from_pretrained(\n",
        "            model_id,\n",
        "            subfolder=\"transformer\",\n",
        "            quantization_config=quantization_config,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "        )\n",
        "        pipe = HunyuanVideoPipeline.from_pretrained(\n",
        "            model_id,\n",
        "            transformer=transformer,\n",
        "            text_encoder=text_encoder,\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "        # Enable memory optimizations.\n",
        "        pipe.vae.enable_tiling()\n",
        "        pipe.enable_model_cpu_offload()\n",
        "\n",
        "        print(f\"Generating video for prompt:\\n{prompt}\")\n",
        "        result = pipe(\n",
        "            prompt=prompt,\n",
        "            height=320,\n",
        "            width=512,\n",
        "            num_frames=61,\n",
        "            num_inference_steps=30,\n",
        "        )\n",
        "        video_frames = result.frames[0]\n",
        "        export_to_video(video_frames, output_path, fps=15)\n",
        "        print(f\"HunyuanVideo generated video saved at {output_path}\")\n",
        "    except Exception as ex:\n",
        "        print(f\"Error in HunyuanVideo generation: {ex}\")"
      ],
      "metadata": {
        "id": "Gy1gJqqBoQh1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    parser = argparse.ArgumentParser(\"Product Awareness Video Generation with Diffusers\")\n",
        "    parser.add_argument(\n",
        "        \"--model\",\n",
        "        choices=[\"ltx\", \"hunyuan\", \"cogvideo\", \"mochi\", \"allegro\"],\n",
        "        default=\"hunyuan\",\n",
        "        help=\"Select the video generation model: ltx, hunyuan, cogvideo, mochi, or allegro\"\n",
        "    )\n",
        "    # Use parse_known_args() to ignore any additional arguments (e.g. those provided by Jupyter)\n",
        "    args, unknown = parser.parse_known_args()\n",
        "\n",
        "    output_dir = os.path.join(os.getcwd(), \"video_outputs\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    if args.model == \"ltx\":\n",
        "        prompt = (\n",
        "            \"A high-definition product showcase in a futuristic urban setting with glowing neon accents. \"\n",
        "            \"The scene highlights a sleek, innovative gadget on display in a modern showroom. \"\n",
        "            \"Intricate reflections, dynamic camera movements, and cinematic lighting emphasize the product's premium design and cutting-edge features.\"\n",
        "        )\n",
        "        negative_prompt = \"low quality, motion blur, grainy, poor color grading\"\n",
        "        output_path = os.path.join(output_dir, \"product_output_ltx.mp4\")\n",
        "        generate_video_ltx(prompt, negative_prompt, output_path)\n",
        "\n",
        "    elif args.model == \"hunyuan\":\n",
        "        prompt = (\n",
        "            \"A visually striking and ultra-realistic product advertisement. \"\n",
        "            \"The video features a state-of-the-art smartwatch displayed against a minimalist, high-tech background. \"\n",
        "            \"Emphasize smooth transitions, detailed textures, and vibrant lighting to highlight the product's advanced technology and elegant design.\"\n",
        "        )\n",
        "        output_path = os.path.join(output_dir, \"product_output_hunyuan.mp4\")\n",
        "        generate_video_hunyuan(prompt, output_path)\n",
        "\n",
        "    elif args.model == \"cogvideo\":\n",
        "        prompt = (\n",
        "            \"A futuristic and dynamic montage showcasing a cutting-edge smart home device. \"\n",
        "            \"The scene interweaves architectural elements, smart interfaces, and urban vibes, creating an immersive portrayal of a tech-enhanced lifestyle.\"\n",
        "        )\n",
        "        output_path = os.path.join(output_dir, \"product_output_cogvideo.mp4\")\n",
        "        generate_video_generic(\n",
        "            model_id=\"THUDM/CogVideoX-1.5B\",\n",
        "            prompt=prompt,\n",
        "            output_path=output_path,\n",
        "            num_frames=121,\n",
        "            width=768,\n",
        "            height=512,\n",
        "            num_inference_steps=50,\n",
        "            fps=24,\n",
        "        )\n",
        "\n",
        "    elif args.model == \"mochi\":\n",
        "        prompt = (\n",
        "            \"An animated, playful product spot that brings a new line of eco-friendly sneakers to life. \"\n",
        "            \"Vibrant, cartoonish characters and whimsical backgrounds illustrate the brand's commitment to sustainability and style in a fun and creative way.\"\n",
        "        )\n",
        "        output_path = os.path.join(output_dir, \"product_output_mochi.mp4\")\n",
        "        generate_video_generic(\n",
        "            model_id=\"Genmo/Mochi-1\",\n",
        "            prompt=prompt,\n",
        "            output_path=output_path,\n",
        "            num_frames=16,\n",
        "            width=512,\n",
        "            height=512,\n",
        "            num_inference_steps=50,\n",
        "            fps=24,\n",
        "        )\n",
        "\n",
        "    elif args.model == \"allegro\":\n",
        "        prompt = (\n",
        "            \"An abstract and artistic interpretation of innovation in product design. \"\n",
        "            \"Fluid brush strokes and vibrant, morphing shapes form a visually arresting representation of a new line of tech gadgets, \"\n",
        "            \"capturing both the energy and elegance of modern design.\"\n",
        "        )\n",
        "        output_path = os.path.join(output_dir, \"product_output_allegro.mp4\")\n",
        "        generate_video_generic(\n",
        "            model_id=\"RhymesAI/Allegro\",\n",
        "            prompt=prompt,\n",
        "            output_path=output_path,\n",
        "            num_frames=16,\n",
        "            width=512,\n",
        "            height=512,\n",
        "            num_inference_steps=50,\n",
        "            fps=24,\n",
        "        )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "bcce1c05d29c477db9dc7391f7ae5675",
            "55c9fd00c5a04244802c132c8a107575",
            "315247a1faec44629d1bb2af8d0043d5",
            "041e9bbe6b5441cc88327555842795ab",
            "6dc2427f55c5411787b1278176226fb1",
            "6c99adfc3a194f769d2888f7dd9c58a8",
            "820599ea5f04443eb1b58534459ca0ae",
            "6b75f099db45498e88143ae99965a572",
            "86e4f95ed6a24c3180495c53c12d339f",
            "daa824b871f742a5ab63e9ba0ba2ae73",
            "38a4996ea35d4ec1b317dd84b0fbb141"
          ]
        },
        "id": "ytSXyYoGoWa1",
        "outputId": "a4584b18-2037-444e-f464-9cb77a37bf35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using HunyuanVideo pipeline with advanced optimizations...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bcce1c05d29c477db9dc7391f7ae5675"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_KVqIU20oa4r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}