{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyONDSUCi91dsG7RBc8HdKQk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaifAhmad1/deepfake/blob/main/Product_Marketing_lmage_Generation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Product Marketing AI System\n",
        "\n",
        "\n",
        "## Overview\n",
        "This system helps create high-quality marketing images automatically. It takes in photos and optional audio or video, then processes, refines, and enhances them to produce beautiful marketing visuals for many different industries.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- **Easy Input:** Upload main and supplementary images, plus optional multimedia for extra context.\n",
        "- **Smart Processing:** The system automatically cuts out key parts, improves image details, and boosts overall clarity.\n",
        "- **Creative Prompts:** Custom prompts are generated to guide the image creation process, making it tailored to your needs.\n",
        "- **Fast Generation:** Uses multiple AI models working together to generate and improve images quickly.\n",
        "- **Quality Check:** Compares final images to the originals and provides simple quality feedback.\n",
        "- **Simple Reports:** Automatically produces a brief report with the final prompt and quality scores.\n",
        "\n",
        "## Benefits\n",
        "- Saves time by automating the creation of professional marketing images.\n",
        "- Provides consistent and attractive visuals optimized for your business.\n",
        "- Easy to use with straightforward input and clear feedback.\n",
        "\n",
        "Enjoy a seamless experience in making your marketing visuals stand out!"
      ],
      "metadata": {
        "id": "LqMK4cWzkXhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q opencv-python numpy matplotlib pillow torch diffusers scikit-image tenacity langchain-google-genai openai groq accelerate pipecat-ai aiohttp python-dotenv"
      ],
      "metadata": {
        "id": "9_GeB9UykYUH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import base64\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageFilter, ImageEnhance\n",
        "import io\n",
        "import sys\n",
        "import torch\n",
        "from skimage.metrics import structural_similarity as ssim, peak_signal_noise_ratio as psnr\n",
        "from tenacity import retry, stop_after_attempt, wait_exponential, RetryError\n",
        "import time\n",
        "import warnings\n",
        "import random\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import traceback\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Langchain/LLM Imports\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.messages import HumanMessage\n",
        "from groq import Groq, GroqError\n",
        "from openai import OpenAI, OpenAIError\n",
        "\n",
        "# Pipecat Imports\n",
        "try:\n",
        "    from pipecat.services.moondream.vision import MoondreamService\n",
        "    from pipecat.services.google.image import GoogleImageGenService\n",
        "    from pipecat.frames.frames import VisionImageRawFrame, TextFrame, URLImageRawFrame, ErrorFrame\n",
        "    PIPECAT_AVAILABLE = True\n",
        "    print(\"Pipecat AI library loaded successfully.\")\n",
        "except ImportError as e:\n",
        "    print(f\"Warning: Pipecat AI library not found or failed to import ({e}). Moondream and Google Imagen (via Pipecat) will be unavailable.\")\n",
        "    PIPECAT_AVAILABLE = False\n",
        "    MoondreamService, GoogleImageGenService = None, None\n",
        "    VisionImageRawFrame, TextFrame, URLImageRawFrame, ErrorFrame = None, None, None, None\n",
        "\n",
        "# --- Global Settings and API Keys ---\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='torchvision')\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# Load API keys from .env file if it exists, otherwise use environment variables or defaults\n",
        "load_dotenv()\n",
        "\n",
        "# Fetch keys - Prioritize .env, then environment variables, then placeholder\n",
        "# *** IMPORTANT: REPLACE PLACEHOLDERS OR SET ENVIRONMENT VARIABLES / .env FILE ***\n",
        "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", \"YOUR_GROQ_API_KEY_HERE\")\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"YOUR_GOOGLE_API_KEY_HERE\")\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"YOUR_OPENAI_API_KEY_HERE\")\n",
        "\n",
        "# --- Directories ---\n",
        "OUTPUT_DIR = \"outputs\"\n",
        "UPLOAD_DIR = \"uploads\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(UPLOAD_DIR, exist_ok=True)\n",
        "\n",
        "# --- Constants ---\n",
        "DEFAULT_FAST_LLM_MODEL = \"llama3-8b-8192\" # For strategy definition, parsing\n",
        "DEFAULT_CAPABLE_LLM_MODEL_GROQ = \"llama3-70b-8192\"\n",
        "DEFAULT_CAPABLE_LLM_MODEL_GEMINI = \"gemini-1.5-pro-latest\"\n",
        "DEFAULT_ANALYZER_MODEL_GEMINI = \"gemini-1.5-flash-latest\"\n",
        "DEFAULT_IMAGE_GEN_MODEL_GEMINI = \"imagen-3.0-generate-002\" # Using Google Imagen via Pipecat preferably\n",
        "DEFAULT_IMAGE_GEN_MODEL_OPENAI = \"dall-e-3\"\n",
        "\n",
        "# --- Environment Detection ---\n",
        "try:\n",
        "    from google.colab import files\n",
        "    ENV = \"colab\"\n",
        "    print(\"Running in Colab environment.\")\n",
        "except ImportError:\n",
        "    try:\n",
        "        if 'IPython' in sys.modules and 'google.colab' not in sys.modules:\n",
        "             # Check if running in Kaggle or similar Jupyter environments\n",
        "             if os.path.exists(\"/kaggle/input\"):\n",
        "                  ENV = \"kaggle\"\n",
        "                  print(\"Running in Kaggle environment.\")\n",
        "             else:\n",
        "                  ENV = \"jupyter\"\n",
        "                  print(\"Running in Jupyter environment.\")\n",
        "        else:\n",
        "             raise ImportError(\"Not in Jupyter or Colab\")\n",
        "    except ImportError:\n",
        "        ENV = \"standalone\"\n",
        "        print(\"Running in standalone environment.\")\n",
        "\n",
        "# --- Device Setup ---\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "try: # Intel XPU\n",
        "    import intel_extension_for_pytorch as ipex\n",
        "    if torch.xpu.is_available(): device = \"xpu\"\n",
        "except ImportError: pass\n",
        "if device == \"cpu\" and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available(): device = \"mps\" # Apple MPS\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "if device == \"cuda\": torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buHcXvnxkb28",
        "outputId": "825b35d7-a496-4396-aeb6-78a812154c6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Pipecat AI library not found or failed to import (No module named 'pipecat.services.moondream.vision'; 'pipecat.services.moondream' is not a package). Moondream and Google Imagen (via Pipecat) will be unavailable.\n",
            "Running in Colab environment.\n",
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Model Initialization ---\n",
        "def initialize_models():\n",
        "    \"\"\"Initializes all AI models and clients, checking API keys.\"\"\"\n",
        "    print(\"\\n--- Initializing AI Models ---\")\n",
        "    model_status = {\n",
        "        \"groq_client\": None, \"groq_fast_llm\": DEFAULT_FAST_LLM_MODEL, \"groq_capable_llm\": DEFAULT_CAPABLE_LLM_MODEL_GROQ,\n",
        "        \"gemini_analyzer\": None, \"gemini_capable_llm\": None,\n",
        "        \"gemini_direct_image_gen\": None, \"gemini_direct_image_gen_model\": DEFAULT_IMAGE_GEN_MODEL_GEMINI,\n",
        "        \"openai_client\": None, \"openai_image_gen_model\": DEFAULT_IMAGE_GEN_MODEL_OPENAI,\n",
        "        \"moondream_service\": None,\n",
        "        \"google_image_gen_service\": None,\n",
        "        \"parsing_llm_client\": None,\n",
        "        \"strategy_llm_client\": None,\n",
        "        \"prompt_llm_client\": None,\n",
        "        \"feedback_llm_client\": None,\n",
        "        \"available\": {\"groq\": False, \"gemini\": False, \"openai\": False, \"moondream\": False, \"google_imagen\": False}\n",
        "    }\n",
        "    api_key_warnings = []\n",
        "\n",
        "    # Validate API Keys more strictly\n",
        "    if not GROQ_API_KEY or \"YOUR_\" in GROQ_API_KEY or len(GROQ_API_KEY) < 50:\n",
        "        api_key_warnings.append(\"Groq API Key missing/invalid.\")\n",
        "    else: model_status[\"available\"][\"groq\"] = True\n",
        "    if not GOOGLE_API_KEY or \"YOUR_\" in GOOGLE_API_KEY or len(GOOGLE_API_KEY) < 30:\n",
        "        api_key_warnings.append(\"Google API Key missing/invalid (needed for Gemini & Google Imagen).\")\n",
        "    else: model_status[\"available\"][\"gemini\"] = True; model_status[\"available\"][\"google_imagen\"] = True\n",
        "    if not OPENAI_API_KEY or \"YOUR_\" in OPENAI_API_KEY or len(OPENAI_API_KEY) < 50:\n",
        "        api_key_warnings.append(\"OpenAI API Key missing/invalid.\")\n",
        "    else: model_status[\"available\"][\"openai\"] = True\n",
        "\n",
        "    if api_key_warnings:\n",
        "        print(\"\\n--- ⚠️ API KEY WARNINGS ---\")\n",
        "        for warning in api_key_warnings: print(f\"- {warning}\")\n",
        "        print(\"--- Functionality will be limited. Please set keys in .env file or environment variables. ---\\n\")\n",
        "\n",
        "    # Initialize Groq (if key valid)\n",
        "    if model_status[\"available\"][\"groq\"]:\n",
        "        try:\n",
        "            model_status[\"groq_client\"] = Groq(api_key=GROQ_API_KEY, timeout=120.0)\n",
        "            print(f\"✅ Groq client initialized.\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error initializing Groq: {e}. Disabling Groq.\")\n",
        "            model_status[\"available\"][\"groq\"] = False; model_status[\"groq_client\"] = None\n",
        "\n",
        "    # Initialize Google Gemini (if key valid)\n",
        "    if model_status[\"available\"][\"gemini\"]:\n",
        "        try:\n",
        "            # Analyzer (Flash)\n",
        "            model_status[\"gemini_analyzer\"] = ChatGoogleGenerativeAI(\n",
        "                model=DEFAULT_ANALYZER_MODEL_GEMINI, temperature=0.3, max_retries=2, google_api_key=GOOGLE_API_KEY, request_timeout=120)\n",
        "            # Capable LLM (Pro)\n",
        "            model_status[\"gemini_capable_llm\"] = ChatGoogleGenerativeAI(\n",
        "                model=DEFAULT_CAPABLE_LLM_MODEL_GEMINI, temperature=0.6, max_retries=2, google_api_key=GOOGLE_API_KEY, request_timeout=180)\n",
        "            print(f\"✅ Gemini models initialized (Analyzer: {DEFAULT_ANALYZER_MODEL_GEMINI}, Capable: {DEFAULT_CAPABLE_LLM_MODEL_GEMINI}).\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error initializing Gemini models: {e}. Disabling Gemini.\")\n",
        "            model_status[\"available\"][\"gemini\"] = False\n",
        "            model_status[\"gemini_analyzer\"], model_status[\"gemini_capable_llm\"], model_status[\"gemini_direct_image_gen\"] = None, None, None\n",
        "\n",
        "    # Initialize OpenAI (if key valid)\n",
        "    if model_status[\"available\"][\"openai\"]:\n",
        "        try:\n",
        "            model_status[\"openai_client\"] = OpenAI(api_key=OPENAI_API_KEY, timeout=120.0)\n",
        "            print(f\"✅ OpenAI client initialized (Model: {DEFAULT_IMAGE_GEN_MODEL_OPENAI}).\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error initializing OpenAI: {e}. Disabling OpenAI.\")\n",
        "            model_status[\"available\"][\"openai\"] = False; model_status[\"openai_client\"] = None\n",
        "\n",
        "    # Initialize Pipecat Services (if library available)\n",
        "    if PIPECAT_AVAILABLE:\n",
        "        # Moondream (Local Vision Analysis)\n",
        "        try:\n",
        "            force_cpu = (device == \"cpu\")\n",
        "            model_status[\"moondream_service\"] = MoondreamService(use_cpu=force_cpu)\n",
        "            print(f\"✅ Pipecat MoondreamService initialized (will use {device if not force_cpu else 'CPU'}). Model download may occur on first use.\")\n",
        "            model_status[\"available\"][\"moondream\"] = True\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error initializing Pipecat MoondreamService: {e}. Moondream unavailable.\")\n",
        "            model_status[\"moondream_service\"] = None\n",
        "\n",
        "        # Google Imagen (via Pipecat - Preferred Image Gen if available)\n",
        "        if model_status[\"available\"][\"google_imagen\"]:\n",
        "            try:\n",
        "                model_status[\"google_image_gen_service\"] = GoogleImageGenService(api_key=GOOGLE_API_KEY)\n",
        "                print(\"✅ Pipecat GoogleImageGenService initialized.\")\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error initializing Pipecat GoogleImageGenService: {e}. Google Imagen (Pipecat) unavailable.\")\n",
        "                model_status[\"google_image_gen_service\"] = None\n",
        "                model_status[\"available\"][\"google_imagen\"] = False\n",
        "        else:\n",
        "             print(\"Skipping Pipecat Google Imagen (Google API key invalid or unavailable).\")\n",
        "    else:\n",
        "        print(\"Pipecat AI library not installed/imported. Moondream and Google Imagen (Pipecat) skipped.\")\n",
        "        model_status[\"available\"][\"moondream\"] = False; model_status[\"available\"][\"google_imagen\"] = False\n",
        "\n",
        "    # --- Assign LLM Clients for Specific Tasks (Prioritization) ---\n",
        "    # Priority: Groq (Fastest/Cheapest) -> Gemini Flash -> Gemini Pro\n",
        "\n",
        "    # Parsing LLM (Needs to be fast)\n",
        "    if model_status[\"available\"][\"groq\"]: model_status[\"parsing_llm_client\"] = model_status[\"groq_client\"]\n",
        "    elif model_status[\"available\"][\"gemini\"]: model_status[\"parsing_llm_client\"] = model_status[\"gemini_analyzer\"]\n",
        "    if model_status[\"parsing_llm_client\"]: print(f\"Selected Parsing LLM: {'Groq' if isinstance(model_status['parsing_llm_client'], Groq) else 'Gemini Flash'}\")\n",
        "    else: print(\"⚠️ No parsing LLM available - Moondream analysis will be disabled.\"); model_status[\"available\"][\"moondream\"] = False\n",
        "\n",
        "    # Strategy LLM (Needs to be fast and decent)\n",
        "    if model_status[\"available\"][\"groq\"]: model_status[\"strategy_llm_client\"] = model_status[\"groq_client\"]\n",
        "    elif model_status[\"available\"][\"gemini\"]: model_status[\"strategy_llm_client\"] = model_status[\"gemini_analyzer\"]\n",
        "    if model_status[\"strategy_llm_client\"]: print(f\"Selected Strategy LLM: {'Groq' if isinstance(model_status['strategy_llm_client'], Groq) else 'Gemini Flash'}\")\n",
        "    else: print(\"⚠️ No strategy LLM available - Prompting might be less adaptive.\")\n",
        "\n",
        "    # Main Prompt LLM (Needs to be capable)\n",
        "    if model_status[\"available\"][\"gemini\"] and model_status[\"gemini_capable_llm\"]: model_status[\"prompt_llm_client\"] = model_status[\"gemini_capable_llm\"]\n",
        "    elif model_status[\"available\"][\"groq\"]: model_status[\"prompt_llm_client\"] = model_status[\"groq_client\"]\n",
        "    if model_status[\"prompt_llm_client\"]: print(f\"Selected Main Prompt LLM: {'Gemini Pro' if isinstance(model_status['prompt_llm_client'], ChatGoogleGenerativeAI) and 'pro' in model_status['prompt_llm_client'].model else 'Groq 70b'}\")\n",
        "    else: print(\"⚠️ CRITICAL: No capable LLM available for main prompt generation.\")\n",
        "\n",
        "    # Feedback LLM (Needs to be capable, preferably vision-aware if possible)\n",
        "    if model_status[\"available\"][\"gemini\"] and model_status[\"gemini_capable_llm\"]: model_status[\"feedback_llm_client\"] = model_status[\"gemini_capable_llm\"]\n",
        "    elif model_status[\"available\"][\"groq\"]: model_status[\"feedback_llm_client\"] = model_status[\"groq_client\"]\n",
        "    elif model_status[\"available\"][\"gemini\"]: model_status[\"feedback_llm_client\"] = model_status[\"gemini_analyzer\"]\n",
        "    if model_status[\"feedback_llm_client\"]: print(f\"Selected Feedback LLM: {'Gemini Pro' if isinstance(model_status['feedback_llm_client'], ChatGoogleGenerativeAI) and 'pro' in model_status['feedback_llm_client'].model else ('Groq 70b' if isinstance(model_status['feedback_llm_client'], Groq) else 'Gemini Flash')}\")\n",
        "    else: print(\"⚠️ No LLM available for AI feedback/evaluation.\")\n",
        "\n",
        "    print(\"--- Model Initialization Complete ---\")\n",
        "    # Check overall readiness\n",
        "    can_analyze = any([model_status[\"available\"][\"moondream\"], model_status[\"available\"][\"gemini\"]])\n",
        "    can_gen_prompt = model_status[\"prompt_llm_client\"] is not None\n",
        "    can_gen_image = any([model_status[\"available\"][\"openai\"], model_status[\"available\"][\"google_imagen\"]])\n",
        "\n",
        "    if not (can_analyze and can_gen_prompt and can_gen_image):\n",
        "         print(\"\\n--- ⚠️ WARNING: Core functionality missing! ---\")\n",
        "         if not can_analyze: print(\"- Cannot analyze images.\")\n",
        "         if not can_gen_prompt: print(\"- Cannot generate detailed prompts.\")\n",
        "         if not can_gen_image: print(\"- Cannot generate images.\")\n",
        "         print(\"--- Please check API keys and model initialization logs. Exiting. ---\")\n",
        "         sys.exit(1)\n",
        "\n",
        "    return model_status\n",
        "\n",
        "models = initialize_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "fGTOZ1Y5kmuD",
        "outputId": "ed2379fc-7336-4bf1-ae00-7db4bd84e388"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Initializing AI Models ---\n",
            "\n",
            "--- ⚠️ API KEY WARNINGS ---\n",
            "- Groq API Key missing/invalid.\n",
            "- Google API Key missing/invalid (needed for Gemini & Google Imagen).\n",
            "- OpenAI API Key missing/invalid.\n",
            "--- Functionality will be limited. Please set keys in .env file or environment variables. ---\n",
            "\n",
            "Pipecat AI library not installed/imported. Moondream and Google Imagen (Pipecat) skipped.\n",
            "⚠️ No parsing LLM available - Moondream analysis will be disabled.\n",
            "⚠️ No strategy LLM available - Prompting might be less adaptive.\n",
            "⚠️ CRITICAL: No capable LLM available for main prompt generation.\n",
            "⚠️ No LLM available for AI feedback/evaluation.\n",
            "--- Model Initialization Complete ---\n",
            "\n",
            "--- ⚠️ WARNING: Core functionality missing! ---\n",
            "- Cannot analyze images.\n",
            "- Cannot generate detailed prompts.\n",
            "- Cannot generate images.\n",
            "--- Please check API keys and model initialization logs. Exiting. ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "1",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LmzCPkZ2y6p3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}